<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

========================================================
autosize: true
incremental: true
width: 1920
height: 1080

<div style="text-align:center; width:100%; float:left;">
<h1>A fast, single-iteration ensemble Kalman smoother for sequential data assimilation</h1>
<h3>Colin Grudzien<sup>1,2</sup>, Marc Bocquet<sup>3</sup></h3>
<div style="float:left; width:100%; text-align:center; padding-top:20px;">
<div style="float:left; width:30%; text-align:center;  padding-top:40px">
<img style="width:100%", src="UNR_logo_H.png" alt="University of Nevada, Reno."/>
</div>
<div style="float:left; width:40%; text-align:center; padding-top:20px">
<img style="width:100%", src="CW3E-Logo.png" alt="Center for Western Weather and Water Extremes."/>
</div>
<div style="float:left; width:30%; text-align:center;  padding-top:40px">
<img style="width:100%", src="cerea_logo.png" alt="CEREA."/>
</div>
</div>
<div style="float:left; width:100%; padding-top:40px;">
<ol style="font-size:30px;text-align: center;list-style-position: inside;">
  <li>Center for Western Weather and Water Extremes (CW3E), Scripps Institution of Oceanography, University of California San Diego, San Diego, CA, USA</li>
  <li>Department of Mathematics and Statistics, University of Nevada, Reno, Reno, NV, USA</li>
  <li>CEREA, A joint laboratory of &Eacute;cole des Ponts Paris Tech and EDF R&D, Universit&eacute; Paris-Est, Champs-sur-Marne, France.</li>
</ol>
</div>
</div>


========================================================
## Data assimilation

<div style="float:left; width:100%">
<ul>
  <li><b>Data assimilation (DA)</b> refers to methodology for combining data from:</li>
  <ul>
    <li><b  style="color:#1b9e77">physics-based, numerical models</b>; and</li>
    <li><b style="color:#7570b3">real-world observations</b></li>
  </ul>
  <li>to produce an <b style="color:#d95f02">improved estimate for the state</b> of a time-evolving, random process and related parameters.</li>
  <li>Suppose that the <b style="color:#1b9e77">dynamical physical states</b> can be written in a vector, ${\color{#1b9e77} {\pmb{x}_k \in \mathbb{R}^{N_x}} }$, where $k$ corresponds to some time $t_k$.</li>
  <li>Formally, the <strong>time-evolution</strong> of these states is represented by the <b style="color:#1b9e77">nonlinear map $\mathcal{M}$</b>,
  $$\begin{align}\\
  \pmb{x}_k = \mathcal{M}_{k} \left(\pmb{x}_{k-1}, \boldsymbol{\lambda}\right) + \boldsymbol{\eta}_k
  \end{align}$$
  where</li>
  <ul>
    <li>$\pmb{x}_{k-1}$ is the <b>vector of physical states</b> at an <strong>earlier time</strong> $t_{k-1}$;</li>
    <li>$\boldsymbol{\lambda}$ is a vector of <b>uncertain physical parameters</b> on which the <strong>time evolution depends</strong>;</li>
    <li>$\boldsymbol{\eta}_k$ is an <b>additive, stochastic noise</b> term, representing <strong>errors in our model for the physical process</strong>.</li>
  </ul>
  <li>We wish to <b style="color:#d95f02">estimate the random vector</b> ${\color{#d95f02} {\pmb{x}_k } }$ with a prior distribution on $\left(\pmb{x}_{k-1}, \boldsymbol{\lambda}\right)$ and knowledge of $\mathcal{M}_{k}$ and knowledge of how $\boldsymbol{\eta}_k$ is distributed.</li>
    <li>We <b>restrict to the case</b> that $\boldsymbol{\lambda}$ is a known constant, and the <strong>forecast model is perfect</strong>,
  $$\begin{align}
  \pmb{x}_k = \mathcal{M}_{k} \left(\pmb{x}_{k-1}\right),
  \end{align}$$
  <strong>for simplicity</strong>.</li>
  <li>Extending our methodology to erroneous models as above is ongoing work.</li>
</ul>
</div>

========================================================
### Data Assimilation

  
<ul>
  <li>At time $t_{k-1}$, we make a <b style="color:#1b9e77">forecast for the distribution of</b> ${\color{#1b9e77} {\pmb{x}_k} }$ with our prior knowledge, including the physics-based model.</li>  
  <li>After some time, we are given <b style="color:color:#7570b3">real-world observations</b> ${\color{#7570b3} {\pmb{y}_k\in\mathbb{R}^{N_y}} }$ related to the physical states by,
  $${\color{#7570b3} { \pmb{y}_k = \mathcal{H}_k \left(\pmb{x}_k\right) + \boldsymbol{\epsilon}_k } }$$
  where</li>
  <ul>
    <li>${\color{#7570b3} {\mathcal{H}_k:\mathbb{R}^{N_x} \rightarrow \mathbb{R}^{N_y} } }$ is a nonlinear map relating the <b style="color:#d95f02">states we wish to estimate</b> ${\color{#d95f02} {\pmb{x}_k} }$ to the <b style="color:#7570b3">values that are observed</b> ${\color{#7570b3} { \pmb{y}_k} }$.</li>
    <li>$\boldsymbol{\epsilon}_k$ is an <b>additive, stochastic noise</b> term representing <strong>errors in the measurements</strong> or in their representation.</li>
  </ul>
  <li>At time $t_k$ we have a <b style="color:#1b9e77">forecast distribution</b> for the states $\pmb{x}_k$  <strong>generated by our prior</strong> on $\pmb{x}_{k-1}$ and our state model $\mathcal{M}$.</li>
  <li>We also have an <b style="color:#7570b3">observation</b> ${\color{#7570b3} {\pmb{y}_k} }$ with uncertainty.</li>
  <li>The basic goal of Bayesian DA to find a <b style="color:#d95f02">posterior distribution for</b> ${\color{#d95f02} {\pmb{x}_k}}$ <b style="color:#7570b3">conditioned on</b> ${\color{#7570b3} {\pmb{y}_k} }$, or some statistics of this distribution.</li>
  <li>For <b>operational weather forecasting</b>, this furthermore should be performed <strong>sequentially and recursively in time</strong>.</li>
</ul> 

========================================================

### Bayesian MAP estimates
<ul>
  <li>Under the <strong>linear-Gaussian assumption</strong>, the optimal filtering problem is equivalent to the <b>Bayesian MAP cost function</b>
$$\begin{align}
{\color{#d95f02} {\mathcal{J}(\pmb{x}_{L})} } &= {\color{#1b9e77} {\frac{1}{2}\parallel  \overline{\pmb{x}}_{L}^\mathrm{fore} -\pmb{x}_{L}\parallel_{\mathbf{B}_{L}^\mathrm{fore}}^2} } + {\color{#7570b3} {\frac{1}{2}\parallel\pmb{y}_L - \mathbf{H}_L \pmb{x}_L\parallel_{\mathbf{R}_L}^2} },
\end{align}$$
where the above weighted norms can be understood as:
<ul>
  <li>$\parallel \circ \parallel_{\mathbf{B}_L^\mathrm{fore}}$ weighting relative to the  <b style="color:#1b9e77">forecast covariance (spread)</b>; and 
  <li>$\parallel \circ \parallel_{\mathbf{R}_L}$ weighting relative to the <b style="color:#7570b3">observation error covariance (imprecision)</b>.</li>
</ul>
<li>The <b style="color:#d95f02">MAP state interpolates</b> the <b style="color:#1b9e77">forecast mean</b> and the <b style="color:#7570b3">observational data</b> <strong>relative to the uncertainty in each piece of data</strong>.</li>
<li>To render the cost function into <b>right-transform analysis</b>, write the matrix factor
$$\begin{align}
\mathbf{B}_{L}^\mathrm{fore} : = \boldsymbol{\Sigma}_{L}^\mathrm{fore} \left(\boldsymbol{\Sigma}_{L}^\mathrm{fore}\right)^\top.
\end{align}$$</li>
<li>The analysis can be written in terms of <strong>optimizing weights $\pmb{w}$</strong> where
$$\begin{align}
\pmb{x}_L := \overline{\pmb{x}}_L^\mathrm{fore} + \boldsymbol{\Sigma}_L^\mathrm{fore}\pmb{w};
\end{align}$$</li>
<li>the equation written in terms of the weights is given as
$$\begin{align}
{\color{#d95f02} {\mathcal{J}(\pmb{w}) } } = {\color{#1b9e77} {\frac{1}{2} \parallel \pmb{w}\parallel^2} } + {\color{#7570b3} {\frac{1}{2} \parallel \pmb{y}_L - \mathbf{H}_L \overline{\pmb{x}}_L^\mathrm{fore} - \mathbf{H}_L \boldsymbol{\Sigma}_L^\mathrm{fore} \pmb{w} \parallel_{\mathbf{R}_L}^2 } }.
\end{align}$$</li>
</ul>

========================================================

### Bayesian MAP estimates

<div style="float:left; width:100%">
<ul>
  <li>Make the following definitions:
  
  $$\begin{align}
  \overline{\pmb{y}}_L = \mathbf{H}_L \overline{\pmb{x}}_L^\mathrm{fore}, & &
  \overline{\pmb{\delta}}_L &= \mathbf{R}^{-\frac{1}{2}}_L \left(\pmb{y}_L - \overline{\pmb{y}}_L\right), & &
  \boldsymbol{\Gamma}_L  =\mathbf{R}_L^{-\frac{1}{2}}\mathbf{H}_L \boldsymbol{\Sigma}_L^\mathrm{fore}.
  \end{align}$$
</li>
<ul>
  <li>The vector $\overline{\pmb{y}}_L$ is the <b>forecast mean</b> <strong>mapped to observation space</strong>.</li>
  <li> The vector $\overline{\pmb{\delta}}_L$ is the <b>innovation vector</b>, <strong>weighted by the observation uncertainty</strong>.</li>
</ul>
<li> Then, the MAP cost function is further reduced to
  $$\begin{align}
  {\color{#d95f02} {\mathcal{J}(\pmb{w}) } } = {\color{#1b9e77} {\frac{1}{2} \parallel \pmb{w}\parallel^2}} +  {\color{#7570b3} {\frac{1}{2} \parallel \overline{\pmb{\delta}}_L  - \boldsymbol{\Gamma}_L \pmb{w} \parallel^2 } }
  \end{align}$$
</li>
<li> We find the critical value as
  
  $$\begin{align}
  \overline{\pmb{w}} = \pmb{0} - {\boldsymbol{\Xi}_\mathcal{J}}^{-1} \nabla_{\pmb{w}} \mathcal{J}|_{\pmb{w}=\pmb{0}}.
  \end{align}$$
  where $\boldsymbol{\Xi}_\mathcal{J}:= \nabla^2_{\pmb{w}}\mathcal{J}$ is the <b>Hessian of the cost function</b>, corresponding to a single iteration of <b>Newton's descent algorithm</b>.</li>
  <li>The forecast mean is updated as,
  $$\begin{align}
  \overline{\pmb{x}}_L^\mathrm{filt}:= \overline{\pmb{x}}_{L}^\mathrm{fore} + \boldsymbol{\Sigma}_{L}^\mathrm{fore} \overline{\pmb{w}}.
  \end{align}$$</li>
  <li>Defining a <strong>right-transform matrix</strong>, $\mathbf{T}:= \boldsymbol{\Xi}^{-\frac{1}{2}}_{\mathcal{J}}$,  the update for the covariance is given as
  $$\begin{align}
  \mathbf{B}^\mathrm{filt}_L = \left(\boldsymbol{\Sigma}_L^\mathrm{fore} \mathbf{T} \right)\left(\boldsymbol{\Sigma}_L^\mathrm{fore} \mathbf{T} \right)^\top.
  \end{align}$$</li>
  <li>This derivation sketches the <b>square root Kalman filter</b>,<sup>1</sup> written for the <strong>perfect, linear-Gaussian model</strong>.</li>
<div style="width:100%;float:left;font-size:30px;">
<b>1.</b> Tippett, M. K., Anderson, J. L., Bishop, C. H., Hamill, T. M., & Whitaker, J. S. (2003). <i>Ensemble square root filters</i>. Monthly Weather Review, 131(7), 1485-1490.
</div>

========================================================
## ETKF

<ul>
  <li>Under the <b>perfect model assumption</b>, the forecast distribution is parametrized by
  $$\begin{align}
  \overline{\pmb{x}}_{L+1}^\mathrm{fore}:= \mathbf{M}_{L+1} \overline{\pmb{x}}_{L}^{\mathrm{filt}} & & \boldsymbol{\Sigma}_{L+1}^\mathrm{fore} := \mathbf{M}_{L+1}\left(\boldsymbol{\Sigma}_{L}^\mathrm{filt}\right)
  \end{align}$$
  giving a <strong>complete recursion in time, within the matrix factor</strong>.</li>
  <li>Using <b>ensemble-based, empirical estimates</b>,
  $$\begin{align}
  & & \hat{\pmb{x}}_L^\mathrm{fore} &= \mathbf{E}_L^\mathrm{fore} \pmb{1} / N_e ; & 
  \hat{\pmb{\delta}}_L &= \mathbf{R}_k^{-\frac{1}{2}}\left(\pmb{y}_L - \mathbf{H}_L \hat{\pmb{x}}_L\right)\\
  &&\mathbf{X}_L^\mathrm{fore} &= \mathbf{E}_L^\mathrm{fore} - \hat{\pmb{x}}^\mathrm{fore}_L \pmb{1}^\top ; & 
  \mathbf{P}^\mathrm{fore}_L &= \mathbf{X}_L^\mathrm{fore} \left(\mathbf{X}_L^\mathrm{fore}\right)^\top / \left(N_e - 1\right);\\
  & &\mathbf{S}_L &:=\mathbf{R}_L^{-\frac{1}{2}}\mathbf{H}_L \mathbf{X}_L^\mathrm{fore};
  \end{align}$$</li>
  <li> the <strong>ensemble-based cost function</strong> is written as
  $$\begin{alignat}{2}
  & & {\color{#d95f02} {\widetilde{\mathcal{J}} (\pmb{w})} } &= {\color{#1b9e77} {\frac{1}{2} \parallel \hat{\pmb{x}}_L^\mathrm{fore} - \mathbf{X}^\mathrm{fore}_L \pmb{w}- \hat{\pmb{x}}^\mathrm{fore}_L \parallel_{\mathbf{P}^\mathrm{fore}_L}^2} } + {\color{#7570b3} {\frac{1}{2} \parallel \pmb{y}_L - \mathbf{H}_L\hat{\pmb{x}}^\mathrm{fore}_L - \mathbf{H}_L \mathbf{X}^\mathrm{fore}_L \pmb{w} \parallel_{\mathbf{R}_L}^2 } }\\
  \Leftrightarrow& & {\color{#d95f02} {\widetilde{\mathcal{J}}(\pmb{w})} } &= {\color{#1b9e77} {\frac{1}{2}(N_e - 1) \parallel\pmb{w}\parallel^2} } + {\color{#7570b3} {\frac{1}{2}\parallel \hat{\pmb{\delta}}_L - \mathbf{S}_L \pmb{w}\parallel^2 } }
  \end{alignat}$$
  <b>optimization over a weight vector $\pmb{w}$ in $N_e$</b> <strong>rather than in the state dimension $N_x$</strong>.</li>
  <li>This a key reduction that <b>makes Monte Carlo methods feasible</b> for the large size of geophysical models.</li>
  <ul>
    <li>Techniques such as <strong>covariance localization<sup>2</sup> and hybridization<sup>3</sup> are used in practice to overcome the curse of dimensionality</strong>.</li>
  </ul>
</ul>
<div style="width:100%;float:left;font-size:30px;">
<b>2.</b> Sakov, P., & Bertino, L. (2011). <i>Relation between two common localisation methods for the EnKF</i>. Computational Geosciences, 15(2), 225-237.<br>
<b>3.</b> Penny, S. G. (2017). <i>Mathematical foundations of hybrid data assimilation from a synchronization perspective</i>. Chaos: An Interdisciplinary Journal of Nonlinear Science, 27(12), 126801.<br>
</div>


========================================================

### The ETKF
<ul>
  <li>Additionally introducing <strong>local analysis</strong>, this sketches the <b>local ensemble transform Kalman filter (LETKF)</b>.<sup>4</sup></li>
  <li>In this formalism, an <b>ensemble right-transform</b> $\boldsymbol{\Psi}_k$ exists (locally) such that for any $t_k$,

 $$\begin{align}
 \mathbf{E}^\mathrm{filt}_k = \mathbf{E}^\mathrm{fore}_k \boldsymbol{\Psi}_k
 \end{align}$$
  where in the above we would say that (approximately)
  $$\begin{align}
  \mathbf{E}^\mathrm{filt}_k &\sim p(\pmb{x}_k \vert \pmb{y}_{k:1}) \\
  \mathbf{E}^\mathrm{fore}_k =\mathbf{M}_k \mathbf{E}_{k-1}^\mathrm{filt}&\sim p(\pmb{x}_k \vert \pmb{y}_{k-1:1})
  \end{align}$$</li>

  <li>We will associate $\mathbf{E}^\mathrm{filt}_L \equiv \mathbf{E}^\mathrm{smth}_{L|L}$;</li>
  <ul>
    <li>under the <strong>linear-Gaussian model</strong>, we furthermore have that (approximately)

  $$\begin{align}
  \mathbf{E}^\mathrm{smth}_{k |L} = \mathbf{E}^\mathrm{smth}_{k|L-1}\boldsymbol{\Psi}_{L} & &
  \mathbf{E}^\mathrm{smth}_{k|L} \sim p(\pmb{x}_k \vert \pmb{y}_{L:1}).
  \end{align}$$</li>
  </ul>
  <li> A <b style="color:#d95f02">retrospective smoothing analysis</b> can be performed on all past states stored in memory <strong>using the latest right-transform update from the filtering step</strong>.</li>
  <li>This form of <b style="color:#d95f02">retrospective analysis</b> is the basis of the <b>ensemble Kalman smoother (EnKS)</b>.<sup>5</sup></li>
</ul>

<div style="width:100%;float:left;font-size:30px">
<b>4.</b> Hunt, B. R., Kostelich, E. J., & Szunyogh, I. (2007). <i>Efficient data assimilation for spatiotemporal chaos: A local ensemble transform Kalman filter</i>. Physica D: Nonlinear Phenomena, 230(1-2), 112-126.<br>
<b>5.</b> Evensen, G., & Van Leeuwen, P. J. (2000). <i>An ensemble Kalman smoother for nonlinear dynamics. Monthly Weather Review, 128(6), 1852-1867.</i>
</div>

========================================================

### The EnKS

<div style="float:left; width:100%">
<ul>
  <li>The EnKS takes advantage of the simple form of the <b style="color:#d95f02">retrospective, right-transform analysis</b> by including an <strong>additional, inner loop of the filtering cycle</strong>.</li>
</ul>
</div>
<div style="float:left; width:100%; text-align:center;" class="fragment">
<img style="width:55%", src="enks_scheme.png" alt="Diagram of the filter observation-analysis-forecast cycle."/>
</div>
<div style="float:left; width:100%">
<ul>
    <li><b>Time is the horizontal axis</b> where right moves forward in time.</li>
    <li>At each time, we produce the standard <b style="color:#d95f02">filtering estimate</b> by computing $\boldsymbol{\Psi}_L$ from the cost function, and updating the <b style="color:#1b9e77">forecast</b> 
    $$\mathbf{E}^\mathrm{filt}_L = \mathbf{E}_L^\mathrm{fore} \boldsymbol{\Psi}_L.$$</li> 
    <li>The <b style="color:#7570b3">information of incoming observations</b> is <strong>passed backward in time using the right-transform</strong> to condition the ensemble at past times:
    $$\begin{align}
    \mathbf{E}^\mathrm{smth}_{k|L} = \mathbf{E}^\mathrm{smth}_{k|L-1} \boldsymbol{\Psi}_L.
    \end{align}$$
</ul>
</div>

========================================================

## The 4D cost function
<div style="float:left; width:100%;">
<ul>
  <li><b>However</b>, re-initializing the DA cycle with the <b style="color:#d95f02">smoothed conditional ensemble estimate</b> $\mathbf{E}^\mathrm{smth}_{0|L}$ can <strong>dramatically improve the performance of the subsequent forecast and filtering statistics</strong>.</li>
  <ul>
    <li>Denote the <b style="color:#1b9e77">composition of the model forecast</b> as,
  $$\begin{align}
  \\
  \mathcal{M}_{l:k} : = \mathcal{M}_l \circ \cdots \circ \mathcal{M}_{k} & & \mathbf{M}_{l:k} := \mathbf{M}_{l}\cdots \mathbf{M}_{k}
  \end{align}$$</li>
  </ul>
  <li>This <b>exploits the miss-match</b> in nonlinear dynamics between
  $$\begin{align}
  \\
  \mathcal{M}_{L:1}\left(\mathbf{E}_{0|L}^\mathrm{smth}\right):= \mathcal{M}_L \circ \cdots \circ \mathcal{M}_1\left(\mathbf{E}_{0|L}^\mathrm{smth}\right) \neq \mathbf{E}_{L}^\mathrm{filt}.
  \end{align}$$
  </li>
  <li>The effectiveness of the <b>linear-Gaussian approximation</b> strongly depends on the <strong>length of the forecast window</strong> $\Delta t$;</li>
  <ul>
    <li>for small $\Delta t$, the <strong>densities are well-approximated with Gaussians</strong>, yet there are <b>deformations induced due to nonlinearity</b>.</li>
  </ul>
  <li> This has been exploited to a great extent by utilizing the <b>4D cost function</b>;<sup>6,7</sup></li>
  <ul>
    <li>the filtering MAP cost function is <strong>extended over multiple observations simultaneously, and in terms of a lagged state directly</strong>.</li>
  </ul>
</ul>
</div>

<div style="width:100%;float:left;font-size:30px;">
<b>6.</b> Hunt, B. R., et al., (2004). <i>Four-dimensional
ensemble Kalman filtering</i>, Tellus A, 56, 273–277<br>
<b>7.</b> Bannister, R. N. (2017). <i>A review of operational methods of variational and ensemble‐variational data assimilation</i>. Quarterly Journal of the Royal Meteorological Society, 143(703), 607-633.
</div>


========================================================

### The 4D cost function

<div style="float:left; width:100%;">
<ul>
  <li> Suppose now we want to write $p(\pmb{x}_{L:1}\vert \pmb{y}_{L:1})$, the <b style="color:#d95f02">joint smoothing posterior</b> as a <strong>recursive cycle</strong>.</li>
</ul>
</div>
<div style="float:left; width:100%; text-align:center;" class="fragment">
<img style="width:60%", src="cycling.png" alt="Diagram of the filter observation-analysis-forecast cycle."/>
</div>
<div style="float:left; width:100%">
<ul>
  <li> Using a Bayesian analysis we can write 
  $$\begin{align}
  {\color{#d95f02} { p(\pmb{x}_{L:1} \vert \pmb{y}_{L:1}) } }
  &\propto  \int \mathrm{d}\pmb{x}_0 \underbrace{ {\color{#d95f02} { p(\pmb{x}_0 \vert \pmb{y}_{L-S:-S}) } } }_{(1)} \underbrace{ {\color{#7570b3} { \left[ \prod_{k=L-S+1}^L   p(\pmb{y}_k \vert \pmb{x}_k) \right] } }}_{(2)} \underbrace{{\color{#1b9e77} {  \left[\prod_{k=1}^L p(\pmb{x}_k|\pmb{x}_{k-1}) \right]  } }}_{(3)}
  \end{align}$$
  where</li>
  <ol>
    <li> is the marginal for $\pmb{x}_0$ of the last <b style="color:#d95f02">joint smoothing smoothing posterior</b> $p(\pmb{x}_{L-S:-S}\vert\pmb{y}_{L-S:-S})$;</li>
    <li> is the <b style="color:#7570b3">joint likelihood of the incoming observations</b> to the current DAW, given the background forecast;</li>
    <li> is the <b style="color:#1b9e77">free-forecast with the perfect model</b> $\mathcal{M}_k$.
  </ol>
  </ul>
</div>

========================================================

### The 4D cost function
<div style="float:left; width:100%">
<ul>
  <li> In the <b>linear-Gaussian case</b>, the solution can again be found by a <strong>single iteration of Newton's descent</strong>.</li>
 <li>  However, when the <b>state and observation model are nonlinear</b>, using $\mathcal{H}_k$ and $\mathcal{M}_{k:1}$ in the cost function, this <strong>cost function is solved iteratively to find a local minimum</strong>.</li>
  <li> The difficulty arises in that the gradient $\nabla_{\pmb{w}}$ actually requires <b style="color:#1b9e77">differentiating the equations of motion</b> in $\mathcal{H}_k\circ\mathcal{M}_{k:1}$.</li>
  <li>One alternative to constructing the tangent-linear and adjoint models is to perform a hybrid,  analysis as based on the ETKF.</li>
  <li>This approach is at the basis of the <b>iterative ensemble Kalman filter / smoother (IEnKF/S)</b>.<sup>8</sup></li>
  <li>This technique seeks to perform an ensemble analysis like the square root ETKF by <strong>defining the ensemble estimates and the weight vector directly in the ensemble span</strong>
  $$\begin{alignat}{2}
  & & {\color{#d95f02} {\widetilde{\mathcal{J}} (\pmb{w})} } &= {\color{#d95f02} {\frac{1}{2} \parallel \hat{\pmb{x}}_{0|L-S}^\mathrm{smth} - \mathbf{X}^\mathrm{smth}_{0|L-S} \pmb{w}- \hat{\pmb{x}}^\mathrm{smth}_{0|L-S} \parallel_{\mathbf{P}^\mathrm{smth}_{0|L-S}}^2} } + {\color{#7570b3} {\sum_{k=L-S+1}^L \frac{1}{2} \parallel \pmb{y}_k - \mathcal{H}_k\circ {\color{#1b9e77} { \mathcal{M}_{k:1}\left( {\color{#d95f02} { \hat{\pmb{x}}^\mathrm{smth}_{0|L-S} +  \mathbf{X}^\mathrm{smth}_{0|L-S} \pmb{w} } } \right)}}\parallel_{\mathbf{R}_k}^2 } }\\
 \Leftrightarrow & & {\color{#d95f02} {\widetilde{\mathcal{J}} (\pmb{w})} } &= {\color{#d95f02} {(N_e - 1) \frac{1}{2} \parallel \pmb{w}\parallel^2} } + {\color{#7570b3} {\sum_{k=L-S+1}^L \frac{1}{2} \parallel \pmb{y}_k - \mathcal{H}_k\circ {\color{#1b9e77} { \mathcal{M}_{k:1}\left( {\color{#d95f02} { \hat{\pmb{x}}^\mathrm{smth}_{0|L-S} +  \mathbf{X}^\mathrm{smth}_{0|L-S} \pmb{w} } } \right)}}\parallel_{\mathbf{R}_k}^2 } }.
  \end{alignat}$$</li>
  <li>The scheme produces an <b>iterative estimate</b> using a <strong>Gauss-Newton</strong>-<sup>9</sup> or, e.g., <strong>Levenberg-Marquardt</strong>-based<sup>12</sup> optimization.</li>
</ul>

<div style="width:100%;float:left;font-size:30px">
<b>8.</b> Bocquet, M., & Sakov, P. (2014).<i> An iterative ensemble Kalman smoother</i>. Quarterly Journal of the Royal Meteorological Society, 140(682), 1521-1535.<br>
<b>9.</b> Bocquet, M., & Sakov, P. (2012). <i>Combining inflation-free and iterative ensemble Kalman filters for strongly nonlinear systems</i>. Nonlinear Processes in Geophysics, 19(3), 383-399.<br>
</div>


========================================================

## The single iteration ensemble transform Kalman smoother (SIEnKS)

<div style="float:left; width:100%">
<ul>
  <li>Forecast accuracy can improve with <b>iterations in the 4D estimate</b>, at the <b style="color:#1b9e77">cost of the state ensemble forecast</b> ${\color{#1b9e77} { \mathcal{M}_{L:1} } }$.</li>
  <li>In short-range forecasting the <b>linear-Gaussian approximation</b> of the evolution of the densities is usually an <strong>adequate approximation</strong>;</li>
  <ul>
    <li><b style="color:#1b9e77">the cost of iterating over the nonlinear dynamics</b> <strong>may not be justified by the improvement in the forecast statistics</strong>.</li>
  </ul>
  <li>However, the <b>iterative optimization over a nonlinear observation operator</b> $\mathcal{H}_k$ or <b>hyper-parameters</b> in the filtering step of the classical EnKS can be run <strong>without the additional cost of model forecasts</strong>.</li>
  <ul>
    <li>This can be performed similarly to the IEnKS with the <b>maximum likelihood ensemble filter (MELF)</b> analysis.<sup>10</sup></li>
  </ul>
  <li> Subsequently, the <b style="color:#d95f02">retrospective, right-transform analysis</b> can be applied to <b style="color:#d95f02">condition the initial ensemble</b>
 
 $$\begin{align}
 \mathbf{E}^\mathrm{smth}_{0|L} = \mathbf{E}_{0:L-1}^\mathrm{smth} \boldsymbol{\Psi}_L
 \end{align}$$</li>
  <li>One can <b>initialize the next DA cycle in terms of the retrospective analysis</b>, and gain the benefit of the improved initial estimate like 4D schemes.</li>
  <li>This scheme, <strong>currently in open review</strong>, is the <b>single-iteration ensemble Kalman smoother (SIEnKS)</b>.<sup>11</sup></li>
</ul>
</div>
<div style="width:100%;float:left;font-size:30px">
<b>10.</b> Zupanski, M., et al. (2008). <i>The Maximum Likelihood Ensemble Filter as a non-differentiable minimization algorithm</i>. Quarterly Journal of the Royal Meteorological Society, 134, 1039–1050.<br>
<b>11.</b> Grudzien, C. and Bocquet, M. (2021). <i>A fast, single-iteration ensemble Kalman smoother for sequential data assimilation</i>. GMD Discussions, https://doi.org/10.5194/gmd-2021-306. In revision.
</div>

========================================================

<div style="float:left; width:100%">
<h3>The single iteration ensemble Kalman smoother (SIEnKS)</h3>
<ul>
  <li>Compared to the classical EnKS, this <strong>adds an outer loop</strong> to the <b style="color:#d95f02">filtering cycle</b> to produce the <b style="color:#d95f02">posterior analysis</b>.</li>
</ul>
</div>
<div style="float:left; width:100%; text-align:center;" class="fragment">
<img style="width:65%", src="single_iteration_diagram.png" alt="Diagram of the filter observation-analysis-forecast cycle."/>
</div>
<div style="float:left; width:100%">
<ul>
  <li>The <b style="color:#7570b3">information flows</b> from the filtered state back in time from the <b style="color:#d95f02">retrospective analysis</b>.</li>
  <li>This re-analyzed state becomes the <b style="color:#1b9e77">initialization for the next cycle</b> over the shifted DAW, carrying this information forward.</li>
  <li>The iterative cost function is only solved in the <b>filtering estimate</b> for the <b style="color:#7570b3">new observations entering the DAW</b>.</li>
  <ul>
    <li>Combined with the retrospective analysis, this comes <b style="color:#1b9e77">without the cost of iterating the model forecast over the DAW</b>.</li>
  </ul>
  <li>For <b>short-range forecasting</b>, this is shown to be an <strong>accurate and highly efficient approach to sequential smoothing</strong>.</li>
</ul>
</div>

========================================================
### The single iteration ensemble Kalman smoother (SIEnKS)

<div style=float:left; width:100%">
<ul>
  <li>Our <b>key result</b> is an <strong>efficient multiple data assimilation (MDA)</strong><sup>12</sup>  scheme within the EnKS cycle.</li>
  <ul>
    <li>MDA is a technique based on statistical tempering<sup>13</sup> designed to relax the nonlinearity of the Bayesian MAP estimation.</li>
  </ul>
  <li>In a <b>single data assimilation (SDA) smoother</b>, each observation is only assimilated once so that <strong>new observations are only distantly connected to the initial conditions</strong> of the simulation;</li>
  <ul>
    <li>this can introduce many local minima to the 4D cost function, strongly affecting the optimization.<sup>14</sup></li>
  </ul>
  <li>MDA is designed to <b>artificially inflate observation errors</b> and <strong>weakly assimilate the same observation over multiple DAWs</strong>.</li>
  <ul>
    <li>This weakens the effects of local minima, and slowly brings the estimator close to a more optimal solution.<sup>15</sup></li>
  </ul>
  <li>However, the <b>SIEnKS treats MDA</b> <strong>differently than 4D-EnVAR estimators</strong> by using a classic EnKS cycle to weakly assimilate the observations over multiple passes.</li>
  <ul>
    <li>The filter step in this analysis is used as a <b>boundary condition</b> for the <strong>interpolation of the posterior over the lag window</strong>.</li>
  </ul>
   <li>This MDA scheme is demonstrated to be <b>more accurate, stable and cost-effective</b> than several EnKF-based 4D-EnVAR schemes <strong>in short-range forecasts</strong>.</li>
   <ul>
    <li>Note, the <b>SIEnKS is not as robust as 4D estimates</b> in <strong>highly nonlinear forecast dynamics</strong></li>
   </ul>
</ul>
</div>
<div style="width:100%;float:left;font-size:30px">
<b>12.</b> Emerick, A. A., & Reynolds, A. C. (2013). <i>Ensemble smoother with multiple data assimilation</i>. Computers & Geosciences, 55, 3-15.<br>
<b>13.</b> Neal, R. M. (1996). <i>Sampling from multimodal distributions using tempered transitions</i>. Statistics and computing, 6(4), 353-366.<br>
<b>14.</b> Fillion, A., Bocquet, M., and Gratton, S. (2018). <i>Quasi-static ensemble variational data assimilation: a theoretical and numerical study with the iterative ensemble Kalman smoother</i>, Nonlinear Processes in Geophysics, 25, 315–334.<br>
<b>15.</b> Evensen, G. (2018). <i>Analysis of iterative ensemble smoothers for solving inverse problems</i>, Computational Geosciences, 22, 885–908
</div>

========================================================
### The single iteration ensemble Kalman smoother (SIEnKS)

<div style="float:left; width:100%; text-align:center;">
<img style="width:80%;", src="sda_heat_plot.png" alt="Ensemble statistics" class="fragment"/>
<div style="position:absolute; top:58px; left:0px; width:100%">
<img style="width:80%;", src="mda_heat_plot.png" alt="Ensemble statistics" class="fragment"/>
</div>
</div>


========================================================
### The single iteration ensemble Kalman smoother (SIEnKS)

<div style="float:left;  width:100%; text-align:center;">
<img style="width:80%;", src="sda_line_plot.png" alt="Ensemble statistics" class="fragment"/>
<div style="position:absolute; top:58px; left:0px; width:100%">
<img style="width:80%;", src="mda_line_plot.png" alt="Ensemble statistics" class="fragment"/>
</div>
</div>
<div style="float:left;  width:100%">
<div style="float:left; width:100%">
<ul>
  <li>The <b>data boundary condition</b> <strong>improves the forecast statistics</strong>, controlling the accumulated forecast error over lagged states unlike traditional 4D-EnVAR approaches.</li>
  <li>Similarly, the <b>interpolation of the posterior</b> estimate remains <strong>more stable over the DAW</strong>, when the forecast error dynamics are not highly nonlinear.</li>
</ul>
</div>

========================================================
### The single iteration ensemble Kalman smoother (SIEnKS)

<ul>
  <li>A <strong>wide variety of test cases</strong> for short-range forecast systems are presented in our manuscript.<sup>16</sup></li>
  <ul>
    <li>We demonstrate improved accuracy at lower leading order cost of the SIEnKS with highly nonlinear observation operators, hyper-parameter optimization and other relevant tests.</li>
  </ul>
  <li>The <b>two qualifications</b> are that:
  <ol>
    <li>these <b>theoretical results</b> are based on the <strong>perfect model assumption for simplicity</strong> in this initial analysis; and</li>
    <li>we have <strong>not yet introduced localization or covariance hybridization</strong> in this initial study for simplicity.</li>
  </ol>
  <li>However, localization / hybridization are likely easy extensions based on the forecast regime that we target.</li>
  <li>Initial results in model error support the case that the SIEnKS can be modified for this regime.</li> 
  <li>Our mathematical results are <strong>supported by extensive numerical demonstration</strong>, with the <b>Julia package DataAssimilationBenchmarks.jl.</b><sup>17</sup></li>
  <li>A wider survey of methods for Bayesian DA in the geosciences is available in my condensed lecture notes.<sup>18</sup>
</ul>

<div style="width:100%;float:left;font-size:30px">
<b>16.</b> Grudzien, C. and Bocquet, M. (2021). <i>A fast, single-iteration ensemble Kalman smoother for sequential data assimilation</i>. GMD Discussions, https://doi.org/10.5194/gmd-2021-306. In revision.<br>
<b>17.</b> Grudzien, C., Sandhu, S. (2022). <i>DataAssimilationBenchmarks.jl: a data assimilation research framework</i>. The Journal of Open Source Software.  <a href="https://joss.theoj.org/papers/478dcc0b1608d2a4d8c930edebb58736"><img style="height:45px" src="https://joss.theoj.org/papers/478dcc0b1608d2a4d8c930edebb58736/status.svg"></a>.<br>
<b>18.</b> Grudzien, C. and Bocquet, M. (2022). <i>A Tutorial on Bayesian Data Assimilation</i>. Applications of Data Assimilation and Inverse Problems in the Earth Sciences. Cambridge University Press. Accepted.<br>
</div>

