<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>


Matrix algebra in R
========================================================
date: 10/19/2020
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

<ul>
  <li>The following topics will be covered in this lecture:</li>
  <ul>
    <li>Basic vector properties</li>
    <li>Basic matrix properties and invariants</li>
    <li>Basic matrix operations and linear equations</li>
    <li>Matrix spectrum and norms</li>
  </ul>
</ul>

========================================================
<h2>An introduction to matrices</h2>

* Matrix algebra is a fundamental concept for applying and understanding statistical methods in more than one variable.
  
  * This is at the basis of <strong>formulating multivariate distributions and random vectors, as well as their analysis</strong>.

* We will start by recalling a few basic ideas about vectors and their properties such as the <b>inner product and the norm</b>.

* Then we will introduce the basic characteristics of matrices and their operations and their implementation in R. 

* Thereafter, other operations, such as the <strong>inverse and the solution to linear equations will be introduced</strong>.

* Finally, we will introduce some basic properties about <b>norms and matrix spectrum</b>, with an emphasis on certain classes of matrices.

========================================================
### Basic properties of vectors

* We have had a lot of  practice now using vectors generally, such as with the slice operator


```r
1:10
```

```
 [1]  1  2  3  4  5  6  7  8  9 10
```

* We should introduce some basic mathematical properties about vectors and their analysis.

*  Suppose we have two vectors

  $$\begin{align}
  \mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix}\in\mathbb{R}^{3 \times 1} & & \mathbf{b} = \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix}\in \mathbb{R}^{3\times 1}
  \end{align}$$

* We can perform basic mathematical operations on these element-wise as follows

  $$\begin{align}
  \mathbf{a} + \mathbf{b} = \begin{pmatrix} a_1 + b_1 \\ a_2 + b_2 \\ a_3 + b_3 \end{pmatrix} & & \mathbf{a}*\mathbf{b} = \begin{pmatrix} a_1 * b_1 \\ a_2 * b_2 \\ a_3 * b_3 \end{pmatrix}
  \end{align}$$

* Both of these operations generalize to vectors of arbitrary length.  

  * However, the <strong>above multiplication rule is rarely used in practice</strong> as it is not as meaningful as the scalar-valued product considered next.

========================================================
### Vector inner product

* Notice that the two previously defined vectors $\mathbf{a}$ and $\mathbf{b}$ were defined as column vectors
  
  $$\begin{align}
  \mathbf{a} = \begin{pmatrix} a_1 \\ a_2 \\ a_3 \end{pmatrix}\in\mathbb{R}^{3 \times 1} & & \mathbf{b} = \begin{pmatrix} b_1 \\ b_2 \\ b_3 \end{pmatrix}\in \mathbb{R}^{3\times 1}
  \end{align}$$

* The <b>transpose of $\mathbf{a}$</b> is <strong>defined as the row vector</strong>,
  
  $$\begin{align}
  \mathbf{a}^\mathrm{T} = \begin{pmatrix} a_1 & a_2 & a_3 \end{pmatrix} \in \mathbb{R}^{1 \times 3}
  \end{align}$$

* The <b>standard vector inner product</b> is defined for the vectors $\mathbf{a}$ and $\mathbf{b}$ as follows 

  $$\begin{align}
  \mathbf{a}^\mathrm{T} \mathbf{b} = a_1 * b_1 + a_2 * b_2 + a_3 * b_3
  \end{align}$$
  
* That is, we <b style="color:#d95f02">take each row element from $\mathbf{a}^\mathrm{T}$</b> and <b style="color:#1b9e77">multiply it by each column element of $\mathbf{b}$</b> and <b>take the sum of these products</b>.

* This generalizes to vectors of arbitrary length $n$ as,

  $$\begin{align}
  \mathbf{a}^\mathrm{T} \mathbf{b} = \sum_{i=1}^n a_i * b_i 
  \end{align}$$

* This rule also defines the general form of matrix multiplication that we will consider shortly.

========================================================
### Vector inner product example

* Let's consider a quick example of the vector product of two vectors.

  * We will write this manually in the form of the equation we derived earlier, first in terms of the element-wise product
  

```r
a <- 1:3
b <- 4:6
a * b
```

```
[1]  4 10 18
```

* taking the sum, we obtain the inner product


```r
sum(a*b)
```

```
[1] 32
```

```r
t(a)%*%b
```

```
     [,1]
[1,]   32
```



========================================================
### Vector inner product continued

* Mathematically, the standard inner product can be described as follows,
  
  $$\begin{align}
  \mathbf{a}^\mathrm{T}\mathbf{b} = \parallel \mathbf{a} \parallel * \parallel \mathbf{b} \parallel \cos\left(\theta\right),
  \end{align}$$
  
* where,

  1. $\parallel \mathbf{a}\parallel$ refers to the Euclidean length of the vector, defined as $\parallel \mathbf{a}\parallel^2 =\mathbf{a}^\mathrm{T}\mathbf{a}$; and
  
  2. $\theta$ is the angle formed by the two vectors $\mathbf{a}$ and $\mathbf{b}$ at the origin $\boldsymbol{0}$.

* There are other ways to define the length of a vector that do not use the inner product as above, but we will be more interested in these ideas in the case of matrices.

* The above standard inner product is also a special case of <b>general matrix multiplication</b>.


========================================================
### Basic matrix properties

* We have developed a basic use of matrices in R already, often encoding data into a matrix or a dataframe format,


```r
require("faraway")
gala_mat <- as.matrix(gala)
gala_mat
```

```
             Species Endemics    Area Elevation Nearest Scruz Adjacent
Baltra            58       23   25.09       346     0.6   0.6     1.84
Bartolome         31       21    1.24       109     0.6  26.3   572.33
Caldwell           3        3    0.21       114     2.8  58.7     0.78
Champion          25        9    0.10        46     1.9  47.4     0.18
Coamano            2        1    0.05        77     1.9   1.9   903.82
Daphne.Major      18       11    0.34       119     8.0   8.0     1.84
Daphne.Minor      24        0    0.08        93     6.0  12.0     0.34
Darwin            10        7    2.33       168    34.1 290.2     2.85
Eden               8        4    0.03        71     0.4   0.4    17.95
Enderby            2        2    0.18       112     2.6  50.2     0.10
Espanola          97       26   58.27       198     1.1  88.3     0.57
Fernandina        93       35  634.49      1494     4.3  95.3  4669.32
Gardner1          58       17    0.57        49     1.1  93.1    58.27
Gardner2           5        4    0.78       227     4.6  62.2     0.21
Genovesa          40       19   17.35        76    47.4  92.2   129.49
Isabela          347       89 4669.32      1707     0.7  28.1   634.49
Marchena          51       23  129.49       343    29.1  85.9    59.56
Onslow             2        2    0.01        25     3.3  45.9     0.10
Pinta            104       37   59.56       777    29.1 119.6   129.49
Pinzon           108       33   17.95       458    10.7  10.7     0.03
Las.Plazas        12        9    0.23        94     0.5   0.6    25.09
Rabida            70       30    4.89       367     4.4  24.4   572.33
SanCristobal     280       65  551.62       716    45.2  66.6     0.57
SanSalvador      237       81  572.33       906     0.2  19.8     4.89
SantaCruz        444       95  903.82       864     0.6   0.0     0.52
SantaFe           62       28   24.08       259    16.5  16.5     0.52
SantaMaria       285       73  170.92       640     2.6  49.2     0.10
Seymour           44       16    1.84       147     0.6   9.6    25.09
Tortuga           16        8    1.24       186     6.8  50.9    17.95
Wolf              21       12    2.85       253    34.1 254.7     2.33
```

========================================================
### Basic matrix properties


* There are several special matrices that are frequently encountered in practical and theoretical work. 

* Diagonal matrices are special matrices where all off-diagonal elements are equal to 0; 

  * i.e., the matrix $\mathbf{A}\in\mathbb{R}^{n\times p}$ is a diagonal matrix if $a_{ij} = 0$ for all $i\neq j$.

* The function `diag()` extracts the main diagonal of a matrix in R,


```r
dim(gala_mat)
```

```
[1] 30  7
```

```r
diag(gala_mat)
```

```
[1] 58.00 21.00  0.21 46.00  1.90  8.00  0.34
```

```r
for (i in 1:7) {print(gala_mat[i,i])}
```

```
[1] 58
[1] 21
[1] 0.21
[1] 46
[1] 1.9
[1] 8
[1] 0.34
```

========================================================
### Basic matrix properties

* We can also use the `diag()` function to produce a diagonal matrix,


```r
diag(3)
```

```
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1
```

```r
diag(2,3)
```

```
     [,1] [,2] [,3]
[1,]    2    0    0
[2,]    0    2    0
[3,]    0    0    2
```

```r
diag(1:3)
```

```
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    2    0
[3,]    0    0    3
```

========================================================
### Basic matrix properties

* Diagonal matrices have the benefit that their operation is like that of regular scalar algebra.

  * Particularly, operations can be considered element-wise on the diagonal
  

```r
diag(1:3) - diag(2,3)
```

```
     [,1] [,2] [,3]
[1,]   -1    0    0
[2,]    0    0    0
[3,]    0    0    1
```

```r
diag(1:3) * diag(2,3)
```

```
     [,1] [,2] [,3]
[1,]    2    0    0
[2,]    0    4    0
[3,]    0    0    6
```

```r
diag(1:3) %*% diag(2,3)
```

```
     [,1] [,2] [,3]
[1,]    2    0    0
[2,]    0    4    0
[3,]    0    0    6
```


========================================================
### Basic matrix properties

* Note that in the R language <b style="color:#d95f02">`*` corresponds to element-wise multiplication</b>.

* Define the two matrices,

  $$\begin{align}
  A = \begin{pmatrix} 
  a_1 & a_2 & a_3 \\
  a_4 & a_5 & a_6 \\
  a_7 & a_8 & a_9
  \end{pmatrix} & &   
  B = \begin{pmatrix} 
  b_1 & b_2 & b_3 \\
  b_4 & b_5 & b_6 \\
  b_7 & b_8 & b_9
  \end{pmatrix} 
  \end{align}$$
  
* Their element-wise product is then,
  
  $$\begin{align}
    A * B = \begin{pmatrix}
  a_1 * b_1 & a_2 * b_2 & a_3 * b_3 \\
  a_4 * b_4 & a_5 * b_5 & a_6 * b_6 \\
  a_7 * b_7 & a_8 * b_8 & a_9 * b_9
  \end{pmatrix}
  \end{align}$$
  
* Note, that this is not the same product as the <b style="color:#1b9e77">matrix product defined by `%*%`</b> in general.

* The above element-wise product is like the element-wise vector product;

  * this will only be occasionally considered, unlike the matrix product, defined in terms of the vector inner product, which underpins all linear algebra.


========================================================
### Matrix / vector multiplication

* As a simple case that leads to general matrix multiplication, let us consider <b style="color:#1b9e77">matrix / vector multiplication</b>.

* Let's suppose that 

  $$\begin{align}
  \mathbf{N} \in \mathbb{R}^{N \times p} & & \mathbf{x} \in \mathbf{R}^{p \times 1}
  \end{align}$$
  
  
* We will suppose that we can write the following 
  
  $$\mathbf{N} = \begin{pmatrix} \mathbf{n}_1^\mathrm{T} \\ \vdots \\ \mathbf{n}_N^\mathrm{T} \end{pmatrix} $$
  where each $\mathbf{n}_i^\mathrm{T} \in \mathbb{R}^{1 \times p}$ is a row of the matrix $\mathbf{N}$.
  
* Then, the recall the vector multiplication,
  $$\mathbf{n}_i^\mathrm{T} \mathbf{x} = \sum_{j=1}^p n_{i,j} * x_{j} $$
  
* The product of the matrix and the vector is given as,
  $$\mathbf{N}\mathbf{x} = \begin{pmatrix} \mathbf{n}^\mathrm{T}_1 \mathbf{x} \\ \vdots \\ \mathbf{n}_N^\mathrm{T} \mathbf{x} \end{pmatrix}$$

========================================================
### Matrix multiplication

* This type of multiplication is commonly known as <b>row-versus-column multiplication</b>.

* Particularly, for a simple matrix and vector pair,

  $$\begin{align}
  \mathbf{A} = \begin{pmatrix} 1 & 2 \\ 3 & 4\end{pmatrix} & & \mathbf{x} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}
  \end{align}$$
  
* we recover the product

  $$\begin{align}
  \mathbf{A}\mathbf{x} = \begin{pmatrix} 1 * 1 + 2 * 1 \\ 3 * 1 + 4 * 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 7 \end{pmatrix}
  \end{align}$$

========================================================
### Matrix multiplication

<ul>
  <li>For two general matrices,

  $$\begin{align}
  \mathbf{N} \in \mathbb{R}^{N \times p} & & \mathbf{M} \in \mathbb{R}^{p \times M}
  \end{align}$$</li>

  <li> Let us write these matrices in terms of sub-vectors as,
  
  $$\begin{align}
  \mathbf{N} = \begin{pmatrix} \mathbf{n}_1^\mathrm{T} \\ \vdots \\ \mathbf{n}_N^\mathrm{T} \end{pmatrix} & & \mathbf{M} = \begin{pmatrix} \mathbf{m}_1 & \cdots & \mathbf{m}_M\end{pmatrix} 
  \end{align}$$
  
  where </li>
  <ol>
    <li> $\mathbf{n}_i^\mathrm{T} \in \mathbb{R}^{1 \times p}$ is the $i$-th row vector in the matrix $\mathbf{N} \in \mathbb{R}^{N\times p}$; and</li>
    <li> $\mathbf{m}_i \in \mathbb{R}^{p\times 1}$ is the $i$-th column vector in the matrix $\mathbf{M}\in \mathbb{R}^{p \times M}$</li>
  </ol>
  <li> We have their product defined as,
  $$\begin{align}
  \mathbf{N} \mathbf{M}  = 
  \begin{pmatrix}
  \mathbf{n}_1^\mathrm{T} \mathbf{m_1} & \mathbf{n}_1^\mathrm{T} \mathbf{m}_2 & \cdots & \mathbf{n}_1^\mathrm{T} \mathbf{m}_M \\
  \vdots & \ddots & \cdots & \vdots \\
  \mathbf{n}_N^\mathrm{T} \mathbf{m}_1 & \mathbf{n}_N^\mathrm{T} \mathbf{m}_2 & \cdots & \mathbf{n}_N^\mathrm{T} \mathbf{m}_M
  \end{pmatrix} \in \mathbb{R}^{N\times M}
  \end{align}$$</li>
  <li> Notice that for the product to make sense, <strong>the dimensionality has to match between the $p$ columns in $\mathbf{N}$ and the $p$ rows in $\mathbf{M}$</strong>.</li>
  <ul>
    <li>This <b>inner dimension of $p$ is eliminated in the product</b> of the matrices to form the final dimensionality of $N \times M$.</li>
  </ul>
</ul>


========================================================
### More notes on diagonal matrices

* Notice that given our previous definitions, this means that <strong>matrix multiplication and element-wise multiplication are equivalent for diagonal matrices</strong>.

  * Particularly, <b>all the operations reduce to elements on the diagonal</b>, as all other operations cancel.
  
*  <b>Not every matrix can be reduced to a diagonal matrix</b>, but <strong>many can be reduced to almost-diagonal or other useful forms</strong> under various coordinate transformations.

* Matrices can be considered a <b style="color:#1b9e77">coordinate representation of a general, linear transformation</b>;
  
  * the <b style="color:#d95f02">choice of coordinates will affect how the linear transformation is represented as a matrix</b>.
  
* A property of the linear transformation that does not depend on the choice of coordinates is called an <b>invariant of the matrix under coordinate transformation</b>.

========================================================
### Basic invariants of matrices

* The most basic invariant we can consider is the <b>trace of a matrix</b>.

* For an arbitrary matrix of size $n\times p$, $\mathbf{A}_{ij} = a_{ij}$ we define this as,
  
  $$\mathrm{tr}\left(\mathbf{A}\right) = \sum_{i=1}^{\mathrm{min}(n,p)} a_{ii}$$
  
  i.e., <strong>the sum of all diagonal elements</strong>.
  
* <b>Q:</b> suppose we randomly generate a matrix as follows:


```r
set.seed(0)
my_matrix <- matrix(rnorm(16), nrow=4, ncol=4)
my_matrix
```

```
           [,1]       [,2]         [,3]       [,4]
[1,]  1.2629543  0.4146414 -0.005767173 -1.1476570
[2,] -0.3262334 -1.5399500  2.404653389 -0.2894616
[3,]  1.3297993 -0.9285670  0.763593461 -0.2992151
[4,]  1.2724293 -0.2947204 -0.799009249 -0.4115108
```

* How can we find the trace of this matrix in R?


========================================================
### Basic invariants of matrices


* <b>A:</b> we can use the `diag` and the `sum` function to obtain


```r
sum(diag(my_matrix))
```

```
[1] 0.07508687
```

* Trace is relatively easy to understand how to compute, but the meaning of trace can take a while to appreciate.

  * We will see one particularly useful form of this in the Frobenius norm of a matrix later on.
  
* <b>Determinants</b> on the other hand are <strong>difficult to understand how to compute, but give a very basic tool for understanding matrices</strong>.

* Only in the special case of a $2\times 2$ matrix is a determinant easy to compute,

  $$\begin{align}
  \mathbf{A} = \begin{pmatrix} a_1 & a_2 \\ a_3 & a_4 \end{pmatrix} & & \mathrm{det}\left(\mathbf{A}\right) = a_1*a_4 - a_2*a_3
  \end{align}$$

* We will suppress the general calculation of determinants and instead focus on one of the most useful properties for matrix analysis.

  * We will briefly discuss why this is the case later as we introduce eigenvalues and matrix spectra.
  
========================================================
### Properties of the determinant

* The determinant in practice is often used to check the invertibility of a matrix $\mathbf{A}$.

* If $\mathbf{A}$ is a square matrix, i.e., $\mathbf{A} \in \mathbb{R}^{n\times n}$, then its <b>inverse is defined</b>,

  $$\mathbf{A}^{-1} \mathbf{A} = \mathbf{A} \mathbf{A}^{-1} = \mathbf{I}_n $$
 
  <strong>if it actually even exists</strong>.
  
* The following useful property can be used to determine if a matrix is invertible.

<blockquote>
The matrix $\mathbf{A}$ is invertible if and only if $\mathrm{det}\left(\mathbf{A}\right) \neq 0$. I.e., the inverse $\mathbf{A}^{-1}$ will only exist when $\mathrm{det}\left(\mathbf{A}\right) \neq 0$ and if $\mathrm{det}\left(\mathbf{A}\right) = 0$ there is no such inverse as discussed above.
</blockquote>

* We note that the <strong>determinant and the inverse of a matrix $\mathbf{A}$ can only be computed</strong> in the case when $\mathbf{A}$ is <b>square in its dimensions</b>.

========================================================
### An example of the determinant

* <b>Q:</b> suppose we randomly generate a matrix as follows:


```r
set.seed(0)
my_matrix <- matrix(rnorm(16), nrow=4, ncol=4)
my_matrix
```

```
           [,1]       [,2]         [,3]       [,4]
[1,]  1.2629543  0.4146414 -0.005767173 -1.1476570
[2,] -0.3262334 -1.5399500  2.404653389 -0.2894616
[3,]  1.3297993 -0.9285670  0.763593461 -0.2992151
[4,]  1.2724293 -0.2947204 -0.799009249 -0.4115108
```

* Then suppose we compute the determinant as,


```r
det(my_matrix)
```

```
[1] -2.429628
```

* can we say the inverse exists?

* <b>A:</b> the determinant is non-zero so an inverse exists.

========================================================
### An example of the determinant continued

* Now suppose that we define the following matrix,


```r
A <- matrix(1:16, nrow=4, ncol=4)
A
```

```
     [,1] [,2] [,3] [,4]
[1,]    1    5    9   13
[2,]    2    6   10   14
[3,]    3    7   11   15
[4,]    4    8   12   16
```

```r
det(A)
```

```
[1] 0
```

* This clearly does not have an inverse, but the question as to why may not be obvious.

========================================================
### An example of the determinant continued


* Consider,


```r
(A[,2] - A[,1]) 
```

```
[1] 4 4 4 4
```

```r
(A[,3] - A[,4])
```

```
[1] -4 -4 -4 -4
```

```r
(A[,2] - A[,1]) + (A[,3] - A[,4])
```

```
[1] 0 0 0 0
```

* This says that there is a direct linear dependence between the columns of the matrix, and a zero vector can be written as a combination of the columns.

* The maximal number of linearly independent columns can be computed as follows:


```r
qr(A)$rank
```

```
[1] 2
```

========================================================
### An example of the determinant continued

* Then consider,


```r
qr(my_matrix)$rank
```

```
[1] 4
```

* Notice that the size of `my_matrix` and `A` is $4\times 4$, so that we can say that `my_matrix` has an entire set of linearly independent columns.

* The <b>determinant function detects then when there is a linear dependence</b> between the columns, and <strong>gives a zero when there is a dependence</strong>.

* <b style="color:#d95f02">Only square matrices with linearly independent columns (non-zero determinants) have inverses</b>.



========================================================
### Solving a linear system of equations

* An inverse in the R language can be found from a more general problem called a <b>linear inverse problem</b>:
  
  $$\mathbf{A} \mathbf{x} = \mathbf{b}$$
  
* In the above, we assume that the <b style="color:#1b9e77">vector $\mathbf{x}$ is of interest but unknown</b>.
  
  * On the other hand, we assume that <b style="color:#d95f02">$\mathbf{A}$ is a known relationship between $\mathbf{x}$ and the observed vector $\mathbf{b}$</b>.
  
* If there are <strong>no dependencies in the relationship defined by the columns of $\mathbf{A}$</strong>, then there is a <b>unique relationship</b> that transfers the unobserved $\mathbf{x}$ to the observed variables $\mathbf{b}$.

* Being able to invert this relationship, we find
  
  $$\mathbf{x} = \mathbf{A}^{-1} \mathbf{b}.$$
  

* The way to implement such a procedure in R is with the `solve()` function.

* If a matrix $\mathbf{A}$ is supplied alone, this computes the inverse $\mathbf{A}^{-1}$ if it exists.

  * If a vector $\mathbf{b}$ is supplied additionally, then this solves for $\mathbf{x}$ as above.
  
========================================================
### Solving a linear system of equations example

* We will use the `solve()` function with `my_matrix` to demonstrate:


```r
solve(my_matrix) %*% my_matrix
```

```
              [,1]          [,2]          [,3]          [,4]
[1,]  1.000000e+00 -4.013442e-17  2.629072e-16 -5.308467e-17
[2,] -3.066731e-16  1.000000e+00  9.306799e-17 -3.742162e-17
[3,]  1.982988e-17  1.600473e-16  1.000000e+00 -5.321621e-17
[4,] -5.155545e-17  3.160103e-16 -4.878093e-16  1.000000e+00
```

```r
my_matrix %*% solve(my_matrix)
```

```
              [,1]          [,2]          [,3]          [,4]
[1,]  1.000000e+00  8.708007e-17  8.300185e-17 -2.376320e-16
[2,] -7.102400e-17  1.000000e+00 -1.065612e-16  3.907720e-16
[3,] -1.536517e-18  7.933046e-17  1.000000e+00  3.875857e-17
[4,] -1.063274e-16 -5.806924e-18  3.450065e-16  1.000000e+00
```

* Notice that the off diagonal elements are not exactly but approximately zero.

========================================================
### Solving a linear system of equations example


* We can also solve the linear inverse problem,


```r
my_matrix %*% solve(my_matrix, 1:4)
```

```
     [,1]
[1,]    1
[2,]    2
[3,]    3
[4,]    4
```
 
* This is representing the equation
 $$\mathbf{A} \mathbf{x} = \mathbf{A}  \left(\mathbf{A}^{-1}\mathbf{b}\right) = \mathbf{b}.$$



========================================================
## Matrix spectrum and norms

* Notice the dimensionality in the previous linear inverse problem,

  $$\begin{align}
  \mathbf{A}\in \mathbb{R}^{n\times n} & & \mathbf{x}\in\mathbb{R}^{n \times 1 } & & \mathbf{b} \in \mathbb{R}^{n\times 1}
  \end{align}$$

* That is to say, $\mathbf{A}$ takes $\mathbf{x}$ from where it lies in $\mathbb{R}^n$ to another vector in $\mathbb{R}^n$.

* Generally, when we consider a square matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$, the transformation it represents is from the space $\mathbb{R}^n$ back to itself.

* A special notion exists for such transformations, when the transformation only scales the existing values.

* Consider the diagonal matrix,


```r
diag(1:3)
```

```
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    2    0
[3,]    0    0    3
```

```r
diag(1:3) %*% c(1, 0, 0)
```

```
     [,1]
[1,]    1
[2,]    0
[3,]    0
```

========================================================
### Matrix spectrum and norms continued

* Now we consider


```r
diag(1:3) %*% c(0, 1, 0)
```

```
     [,1]
[1,]    0
[2,]    2
[3,]    0
```

```r
diag(1:3) %*% c(0, 0, 1)
```

```
     [,1]
[1,]    0
[2,]    0
[3,]    3
```

* In each case, the matrix `diag(1:3)` had the property that it sent the vector back to a re-scaled copy of itself.

* This is because each of the vectors were distinct eigenvectors for the matrix.

========================================================
### Eigenvalues and eigenvectors

* If a nonzero vector $\mathbf{x}$ has the property that,
  
  $$\mathbf{A}\mathbf{x} =\lambda \mathbf{x}$$

  then $\mathbf{x}$ is said to be an eigenvector of $\mathbf{A}$ associated to the eigenvalue $\lambda$.
  
* Diagonal matrices are ones that have an entire coordinate system composed of eigenvectors.

  * Certain non-diagonal matrices can be transformed into diagonal matrices by finding such a coordinate system.
  
*  Notice now that,
 
  $$\begin{align}
  & \mathbf{A}\mathbf{x} =\lambda \mathbf{x} \\
  \Leftrightarrow & \mathbf{A}\mathbf{x} - \lambda \mathbf{x} = 0 \\
  \Leftrightarrow & \left(\mathbf{A}  - \lambda \mathbf{I}_n\right) \mathbf{x} = 0 \\
  \end{align}$$
  
* This means that $\mathbf{x}$ is an eigenvector of the matrix $\left(\mathbf{A}  - \lambda \mathbf{I}_n\right)$ associated to the zero eigenvalue.

  * Particularly, this means that there is a linear dependence in $\left(\mathbf{A}  - \lambda \mathbf{I}_n\right)$ and 
  
  $$\mathrm{det} \left(\mathbf{A}  - \lambda \mathbf{I}_n\right) = 0.$$
  
* The above fact is a basic property that allows us to solve for eigenvalues of the matrix $\mathbf{A}$.

========================================================
### Eigenvalues and eigenvectors

* In the R language, eigenvalues and eigenvectors can be computed as follows:


```r
eigen(my_matrix)
```

```
eigen() decomposition
$values
[1] -0.312062+1.564758i -0.312062-1.564758i  1.387185+0.000000i
[4] -0.687975+0.000000i

$vectors
                      [,1]                  [,2]           [,3]         [,4]
[1,] -0.3449514+0.0145144i -0.3449514-0.0145144i -0.62691105+0i 0.2303783+0i
[2,]  0.5832284+0.0000000i  0.5832284+0.0000000i -0.44282830+0i 0.6601861+0i
[3,]  0.2216425+0.4402345i  0.2216425-0.4402345i -0.63480339+0i 0.3408087+0i
[4,] -0.2440101+0.4880265i -0.2440101-0.4880265i -0.08893981+0i 0.6284342+0i
```

* Notice that some of the eigenvalue are actually complex numbers, and the eigenvectors are complex vectors.

  * This is due to the fact that such a coordinate system to transform a matrix into a representation in eigenspaces may only exist in complex coordinates.
  
========================================================
### Eigenvalues and eigenvectors

* Instead consider,


```r
eigen(diag(1:3))
```

```
eigen() decomposition
$values
[1] 3 2 1

$vectors
     [,1] [,2] [,3]
[1,]    0    0    1
[2,]    0    1    0
[3,]    1    0    0
```

```r
eigen(A)
```

```
eigen() decomposition
$values
[1]  3.620937e+01 -2.209373e+00 -7.652201e-16  7.166935e-16

$vectors
          [,1]        [,2]       [,3]        [,4]
[1,] 0.4140028  0.82289268 -0.5477226 -0.06798489
[2,] 0.4688206  0.42193991  0.7302967  0.49573777
[3,] 0.5236384  0.02098714  0.1825742 -0.78752087
[4,] 0.5784562 -0.37996563 -0.3651484  0.35976799
```

========================================================
### Eigenvalues and eigenvectors

* Taking a product of `A` with one of its zero eigenvectors, we get


```r
A %*% eigen(A)$vectors[,3]
```

```
              [,1]
[1,] -8.881784e-16
[2,]  4.440892e-16
[3,]  1.110223e-15
[4,]  2.553513e-15
```

```r
A %*% eigen(A)$vectors[,4]
```

```
     [,1]
[1,]    0
[2,]    0
[3,]    0
[4,]    0
```

* Specifically, this gives a dependence relationship as,


```r
A[,1] *eigen(A)$vectors[1,3] + A[,2] *eigen(A)$vectors[2,3] + A[,3] *eigen(A)$vectors[3,3] + A[,4] *eigen(A)$vectors[4,3]
```

```
[1] -8.881784e-16  0.000000e+00  8.881784e-16  2.664535e-15
```


========================================================
### Eigenvalues and eigenvectors

* Recall now that the eigenvalues of `diag(1:3)` are 1, 2 and 3.

* <b>Q:</b> can you tell how these eigenvalues relate to the value


```r
det(diag(1:3))
```

```
[1] 6
```

* <b>A:</b> In fact, the determinant of the matrix is equal to the product of the eigenvalues.

* From the above, we recover a general equivalence:

<blockquote>
The matrix $\mathbf{A}$ has an inverse $\Leftrightarrow$ $\mathrm{det}\left(\mathbf{A}\right) \neq 0$ $\Leftrightarrow$ $\mathbf{A}$ has no linear dependence between its columns $\Leftrightarrow$ the matrix $\mathbf{A}$ has no zero eigenvalues $\Leftrightarrow$ the linear inverse problem $\mathbf{A}\mathbf{x} = \mathbf{b}$ has a unique solution for $\mathbf{x}$. 
</blockquote>

* All of the above statements are equivalent and are useful to equivalently in different scenarios.

========================================================
### Eigenvalues and eigenvectors

*  This shows how the determinant is related to the eigenvalues and the spectrum of the matrix $\mathbf{A}$.

* The trace is also related to the eigenvalues as follows:

  * Let the collection of all eigenvalues of $\mathbf{A}$ be defined as $\{\lambda_i\}_{i=1}^k$ for some $k\leq n$, then
  $$\mathrm{tr}\left(\mathbf{A}\right) = \sum_{i=1}^k \lambda_i$$
  
*  We will use this fact shortly when we discuss the Frobenius norm.

* At the moment, we will introduce a basic case of the spectral theorem, that is useful for understanding the idea of a matrix norm.

========================================================
### A special case of the spectral theorem

* Suppose that we have a matrix $\mathbf{A} \in \mathbb{R}^{n\times p}$ where we assume that $p \leq n$.

* We will define a square product of this matrix  with itself so that the dimensionality makes sense, and so that it is in the smallest dimension $p\times p$ as,
 
 $$\mathbf{A}^\mathrm{T} \mathbf{A} \in \mathbb{R}^{p\times p}.$$
 
* The spectral theorem guarantees that this matrix can be transformed into a diagonal matrix in an appropriate real-valued coordinate change, and the diagonal will have only non-negative values on the diagonal.

*  Particularly, the $p$ non-negative values on the diagonal are the eigenvalues $\{\lambda_i\}_{i=1}^p$ of $\mathbf{A}^\mathrm{T}\mathbf{A}$ (or the singular values squared of $\mathbf{A}$).

* The reason that this can be decomposed into such a coordinate system is because of the symmetry of the product under transpose:

  $$\left(\mathbf{A}^\mathrm{T}\mathbf{A}\right)^\mathrm{T} = \mathbf{A}^\mathrm{T} \left(\mathbf{A}^\mathrm{T}\right)^\mathrm{T} =\mathbf{A}^\mathrm{T} \mathbf{A}$$

* The reason that the eigenvalues must be non-negative is because this acts like the square of a scalar, but in terms of the scalar eigenvalues.

========================================================
### The matrix norm

* There are many ways we can describe the "length" of the matrix, and all give different features more prominence, but are algebraicly equivalent up to rescaling.

* One particularly useful type of norm is known as the Frobenius norm of a matrix, and arises naturally due to the previous decomposition.

* We note that $\mathbf{A}^\mathrm{T} \mathbf{A}$ has p non-negative eigenvalues and that therefore,

  $$\mathrm{tr}\left(\mathbf{A}^\mathrm{T}\mathbf{A}\right) = \sum_{i=1}^p \lambda_i.$$
  can be computed directly by solving for the eigenvalues.
  
* Particularly, this gives a kind of weighted measure of the expansion and contraction under the map $\mathbf{A}^\mathrm{T}\mathbf{A}$.

* We define the Frobenius norm as an actual mathematical matrix "distance" as,

  $$\parallel \mathbf{A}\parallel_F = \sqrt{\mathrm{tr}\left(\mathbf{A}^\mathrm{T}\mathbf{A}\right)}$$

========================================================
### The Frobenius norm

* The Frobenius norm,

  $$\parallel \mathbf{A}\parallel_F = \sqrt{\mathrm{tr}\left(\mathbf{A}^\mathrm{T}\mathbf{A}\right)}$$
   
  is a particularly useful distance to understand as it relates to the singular value decomposition / principal component analysis.

  * This is also the choice of norm that arises from the inner product of matrices defined in terms of,
  
  $$\mathrm{tr}\left(\mathbf{B}^\mathrm{T} \mathbf{A}\right)$$
  for $\mathbf{A},\mathbf{B}\in\mathbb{R}^{n\times p}$.
  
* This has a very similar interpretation then in terms of the Euclidean norm for a vector, but extended to matrices.

  
========================================================
### The matrix norm

* To compute a matrix norm in R, this is performed with the `norm` function.

* There are several different choices, but in terms of making some choice, you should understand what is special about that choice of norm.

* We can also use the default choice that R provides to produce a size of the matrix in terms of the maximum size of one of its columns, treated as
  
  $$\parallel \mathbf{A} \parallel_1 = \max_{j=1, \cdots, p} \sum_{i=1}^n \vert a_{i,j}\vert$$

========================================================
### The matrix norm


* For a generic matrix, we can see the differences in the length estimation as follows:


```r
A <- matrix(c(1,2), nrow=2, ncol=2, byrow=TRUE)
A
```

```
     [,1] [,2]
[1,]    1    2
[2,]    1    2
```

```r
norm(A) # default norm
```

```
[1] 4
```

```r
norm(A, type="f") # frobenius norm
```

```
[1] 3.162278
```


========================================================
### The matrix norm

* Note the eigenvalue decomposition as,


```r
eigen(t(A) %*% A)
```

```
eigen() decomposition
$values
[1] 10  0

$vectors
          [,1]       [,2]
[1,] 0.4472136 -0.8944272
[2,] 0.8944272  0.4472136
```

* and therefore we find $\parallel \mathbf{A}\parallel_F = \sqrt{\mathrm{tr\left(\mathbf{A}^\mathrm{T} \mathbf{A}\right)}}= \sqrt{10 + 0} =$


```r
sqrt(10)
```

```
[1] 3.162278
```

```r
norm(A, type="f")
```

```
[1] 3.162278
```

========================================================
## A summary of main ideas

<ul>
  <li>The main ideas to take away from this  introduction to matrix algebra are the following:</li>
  <ol>
    <li>Matrix multiplication is a general case of vector <b>multiplication of rows versus columns</b>.  This is fundamentally different from the element-wise multiplication which does not contain the same mathematical / geometric meaning of the inner product or matrix product.  Paricularly, 
      $$\begin{align}
  \mathbf{a}^\mathrm{T}\mathbf{b} = \parallel \mathbf{a} \parallel * \parallel \mathbf{b} \parallel \cos\left(\theta\right),
  \end{align}$$
  so that this product is a scalar value that depends on the lengths and angle between the vectors.
    </li>
    <li>The <b>general matrix product</b> is defined as,
  $$\begin{align}
  \mathbf{N} \mathbf{M}  = 
  \begin{pmatrix}
  \mathbf{n}_1^\mathrm{T} \mathbf{m_1} & \mathbf{n}_1^\mathrm{T} \mathbf{m}_2 & \cdots & \mathbf{n}_1^\mathrm{T} \mathbf{m}_M \\
  \vdots & \ddots & \cdots & \vdots \\
  \mathbf{n}_N^\mathrm{T} \mathbf{m}_1 & \mathbf{n}_N^\mathrm{T} \mathbf{m}_2 & \cdots & \mathbf{n}_N^\mathrm{T} \mathbf{m}_M
  \end{pmatrix} \in \mathbb{R}^{N\times M}
  \end{align}$$</li>
    <li>As an algebra system, <strong>matrices only have inverses under certain conditions</strong>:
    <blockquote>
The matrix $\mathbf{A}$ has an inverse $\Leftrightarrow$ $\mathrm{det}\left(\mathbf{A}\right) \neq 0$ $\Leftrightarrow$ $\mathbf{A}$ has no linear dependence between its columns $\Leftrightarrow$ the matrix $\mathbf{A}$ has no zero eigenvalues $\Leftrightarrow$ the linear inverse problem $\mathbf{A}\mathbf{x} = \mathbf{b}$ has a unique solution for $\mathbf{x}$. 
</blockquote></li>
  <li><b>Eigenvalues and eigenvectors describe special coordinate systems</b> in which the effect of the matrix $\mathbf{A}$ can be described as <strong>diagonal or close-to-diagonal, but such coordinate systems may be complex valued</strong>.</li>
  <li>These also provide a useful way to describe the length of the matrix in terms of the Frobenius norm, but multiple choices of norm exist with different interpretations of length.</li>
  </ol>
  <li>In the activities and quiz, we will be focused on the implications of the above points in terms of analyzing, e.g., the dependence between vectors and the existence of unique solutions to linear inverse problems.</li>
</ul>

