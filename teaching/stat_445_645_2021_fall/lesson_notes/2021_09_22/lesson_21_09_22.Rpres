<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>


Continuous random variables and univariate distributions
========================================================
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

<ul>
  <li>The following topics will be covered in this lecture:</li>
  <ul>
    <li>A review of the basics of continuous distributions</li>
    <li>The uniform distribution</li>
    <li>The normal distribution</li>
  </ul>
</ul>



========================================================
## A review of continuous random variables


* Unlike <b style="color:#1b9e77">discrete random variables</b>, <b style="color:#d95f02">continuous random variables</b> can take on an <b style="color:#d95f02">uncountably infinite
number of possible values</b>.

  * This is to say that if $X$ is a <b style="color:#d95f02">continuous random variable</b>, there is <b style="color:#d95f02">no possible index set $\mathcal{I}\subset \mathbb{Z}$</b> which can enumerate the possible values $X$ can attain.
  
  * For <b style="color:#1b9e77">discrete random variables</b>, we could perform this with a possibly <b style="color:#1b9e77">infinite index set</b>, $\{x_j\}_{j=1}^\infty$
  
  * This has to do with how the <b style="color:#d95f02">infinity of the continuum $\mathbb{R}$</b> is actually <strong>larger than</strong> the <b style="color:#1b9e77">infinity of the counting numbers, $\aleph_0$</b>;
  
  * in the <b style="color:#d95f02">continuum</b> you can <b style="color:#d95f02">arbitrarily sub-divide the units of measurement</b>.

* These <strong>random variables are characterized</strong> by a <b>distribution function</b> and a <b>density function</b>.

* Let <strong.$X$ be a continuous (rv)</strong>, then the mapping 
 $$F_X:\mathbb{R} \rightarrow [0,1]$$
 defined by $F_X (x) = P(X \leq x)$, is called the <b>cumulative distribution function (cdf)</b> of the rv $X$.

* A mapping  $f_X: \mathbb{R} \rightarrow \mathbb{R}^+$ is called the <b>probability density function (pdf)</b> of an rv $X$ if $f_X(x) = \frac{\mathrm{d} F_X}{\mathrm{d}x}$ exists for all $x\in \mathbb{R}$; and 

* and the <b>density is integrable</b>, i.e., 
 $$\int_{-\infty}^\infty f_X (x) \mathrm{d}x$$ 
 <strong>exists and takes on the value one</strong>.


========================================================
### A review of continuous random variables

* <b>Q:</b> we defined,
  $$\begin{align}
  f_X(x) = \frac{\mathrm{d} F_X}{\mathrm{d}x}  & & \text{ and }& &
  \int_{a}^b \frac{\mathrm{d}f}{\mathrm{d}x} \mathrm{d}x = f(b) - f(a)
  \end{align}$$
  how can you use the <b>definition above</b> and the <b>fundamental theorem of calculus</b> to <strong>find another form for the CDF</strong>?

  * <b>A:</b> Notice that $\frac{\mathrm{d} F_X}{\mathrm{d}x}$ means that the <b>CDF</b> can be <strong>written in terms of the anti-derivative of the density</strong>.
  
  * If <b>$s$</b> and <b>$t$</b> are <strong>arbitrary values</strong>, the <b>definite integral</b> is written as
  
  $$\begin{align}
  \int_{s}^t f_X(x) \mathrm{d}x &= \int_{s}^t \frac{\mathrm{d} F_X}{\mathrm{d}x} \mathrm{d}x\\
  &= F_X(t) - F_X(s) \\
  & = P(X \leq t) - P(X \leq s) = P(s &lt; X \leq t)
  \end{align}$$
  
  * If we take a limit as $s \rightarrow \infty$ we thus recover that
  
  $$\begin{align}
  \lim_{s\rightarrow - \infty} \int_{s}^t f_X(x) \mathrm{d} x & = \lim_{s \rightarrow -\infty} P(s &lt; X \leq t) \\
  & = P(X\leq t) = F_X(t)
  \end{align}$$

========================================================
### Properties of continuous distributions

<ul>
  <li>Last week we discussed how the elementary properties of the <b>probability distribution</b> of a <b style="color:#1b9e77">discrete rv</b> can be described by an <strong>expectation</strong> and a <strong>variance</strong>. </li>
  <li> With respect to this, the only difference with <b style="color:#d95f02">continuous rvs</b> is in the use of <b style="color:#d95f02">integrals</b>, rather than <b style="color:#1b9e77">sums</b>, over the possible values of the rv.</li>
  <li> Let $X$ be a <b style="color:#d95f02">continuous rv</b> with a <b>density function $f_X(x)$</b> -- then the <strong>expectation of $X$</strong> is defined as
  $$ \mathbb{E}\left[X\right] = \int_{-\infty}^{+\infty} xf_X(x)\mathrm{d}x = \mu_X$$
  where $f_X$ is the density function described before.</li>
  <li>  Note that the same interpretation of the expected value from discrete rvs applies here:</li>
  <ol>
    <li>We see <b>$\mathbb{E}\left[X\right]=\mu_X$</b> as representing the <strong>"center of mass" for the "density" curve $f_X$</strong>.</li>
    <li>We see <b>$\mathbb{E}\left[X\right]=\mu_X$</b> as representing the <strong>mean</strong> that we would obtain if we could take <strong>infinitely many independently replicated measurements of $X$</strong>, and took the average of these measurements over all possible scenarios.</li>
  </ol>
  <li><strong>If the expectation of $X$ exists</strong>, the <b>variance</b> is defined as 
    $$\begin{align}
    \mathrm{var} \left(X\right)& = \mathbb{E}\left[\left(X − \mu_X \right)^2\right] \\
    &=\int_{-\infty}^\infty \left(x - \mu_X\right)^2 f_X(x)\mathrm{d}x = \sigma_X^2
    \end{align}$$</li>
</ul>

* Once again, this is a <b>measure of dispersion</b> by <strong>averaging the deviation</strong> of each case from the mean in the square sense, weighted by the probability density. 


========================================================
### Properties of continuous distributions

<ul>
  <li>While the <b style="color:#d95f02">variance</b> is a <b style="color:#d95f02">more "fundamental" theoretical quantity</b> for various reasons, in practice <b style="color:#1b9e77">we are usually concerned with</b> the <b style="color:#1b9e77">standard deviation</b> of the random variable $X$,
  $$\mathrm{std}(X)=\sqrt{\mathrm{var}\left(X\right)} = \sigma_X.$$</li>
  <li>This is due to the fact that the <b style="color:#d95f02">variance $\sigma^2_X$</b> has the <b style="color:#d95f02">units of $X^2$</b> by the definition of the product.</li>
  <ul>
    <li>For example, if the <b>units of $X$ are $\mathrm{cm}$</b>, then <strong>$\sigma_X^2$ will be in $\mathrm{cm}^2$</strong>.</li> 
  </ul>
  <li>Taking a square root on the variance gives us the <b style="color:#1b9e77">standard deviation $\sigma_X$</b>  in the <b style="color:#1b9e77">units of $X$ itself</b>.</li>

========================================================
### Quantiles / percentiles

* While together the <b style="color:#d95f02">mean $\mu_X$</b> and the <b style="color:#1b9e77">standard deviation $\sigma_X$</b> give a picture of the <b style="color:#d95f02">center</b> and <b style="color:#1b9e77">dispersion</b> of a probability distribution, we can analyze this in a different way.

* For example, while the mean is the notion of the "center of mass", we may also be interested in <strong>where the upper and lower $50\%$ of values are separated</strong> as a different notion of <b>"center"</b>.

  * The value that separates this upper and lower half <strong>does not need to equal the center of mass in general</strong>, and it is known commonly as the <b>median</b>.
 
 
* More generally, for any <b>univariate cumulative distribution function $F$</b>, and for <b>$0 < p < 1$</b>, we can identify <strong>$p$ as a percent of the data that lies under the graph of a density curve</strong>.

  * We might be interested in where the <b>lower $p$ area</b> is <strong>separated from</strong> the <b>upper $1-p$ area</b>.

* The quantity
  $$\begin{align}
  F^{-1}(p)=\inf \left\{x \vert F(x) \geq p \right\}
  \end{align}$$
  is called the <b>theoretical $p$-th quantile or percentile of $F$</b>.
  
* The "$\inf$" in the above refers to the smallest possible quantity in the set on the right-hand-side.
  
* We will usually refer to the <b>$p$-th quantile as $\xi_p$</b>.
  
* $F^{-1}$ is called the <b>quantile function</b>.

  * Particularly, $\xi_{-\frac{1}{2}}$ is known as the <b>theoretical median</b> of a distribution.

========================================================
### Skewness and kurtosis

<div style="float:left; width:75%">
<img style="width:100%", src="skewness.png" alt="Diagram of kurtosis for different distributions."/>
<p style="text-align:center;">
Courtesy of   <a href="https://commons.wikimedia.org/wiki/File:Relationship_between_mean_and_median_under_different_skewness.png" title="via Wikimedia Commons">Diva Jain</a> / <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA</a>
</p>
</div>
<div style="float:left; width:25%">
<ul>
  <li> Other useful characteristics of a distribution are its <b style="color:#d95f02">skewness</b> and <b style="color:#1b9e77">excess kurtosis</b>.</li>
  <li>The <b style="color:#d95f02">skewness</b> of a probability distribution is defined as the <strong>extent to which it deviates from symmetry</strong>.</li> 
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>A distribution has <b>negative skewness</b> if the <strong>left tail is longer than the right tail of the distribution</strong>;</li>
  <ul>
    <li>i.e., there are more values on the right side of the mean than on the left side of the mean.</li>
  </ul>
  <li>Respectively, <b>positive skewness</b> refers to the <strong>right tail being longer than the left tail</strong>.</li>
  <li>For the rv $X$, we define the <b style="color:#d95f02">skewness</b> to be,
  $$\mathbb{E}\left[ \left( X - \mu_X\right)^3 \right] / \sigma_X^3.$$</li>
  <li>This can be understood as a <strong>kind of average, third order signed deviation of the random variable from the mean</strong>, relative to the dispersion cubed.</li>
</ul>
</div>

========================================================
### Skewness and kurtosis

<div style="float:left; width:50%">
<img style="width:100%", src="kurtosis.png" alt="Diagram of kurtosis for different distributions."/>
<p style="text-align:center;">
Courtesy of <a href="https://commons.wikimedia.org/wiki/File:Kurtosis_no_text.png" title="via Wikimedia Commons">Joxemai</a> / <a href="https://creativecommons.org/licenses/by-sa/3.0">CC BY-SA</a>
</p>
</div>
<div style="float:left; width:50%">
<ul>
  <li>The <b style="color:#1b9e77">kurtosis</b> on the other hand is a <strong>measure of the peakedness of a probability distribution</strong>.</li>
  <li>The <b style="color:#1b9e77">excess kurtosis</b> is used to compare the kurtosis of a pdf <strong>with the kurtosis of the normal distribution</strong>, which equals $3$. </li>
    <li>The formula for the <b style="color:#1b9e77">excess kurtosis</b> is given as follows:
  $$\begin{align}
  K(X) = \frac{\mathbb{E}\left[\left(X - \mu_X\right)^4\right]}{\sigma_X^4} - 3
  \end{align}$$
  where the <b style="color:#1b9e77">excess kurtosis</b> gives a <strong>signed, fourth order average of the deviation from the mean</strong>, relative to the dispersion to the quartic.</li>
  <li>Distributions with negative or positive excess kurtosis are called <b>platykurtic</b> distributions and <b>leptokurtic</b> distributions, respectively.</li>
  <li>A distribution that displays normal Kurtosis is described as <b>mesokurtic</b>.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li><b>Q:</b> given the picture on the left, which of the distributions correspond to positive or negative excess kurtosis and which correpond to normal kurtosis?</li>
  <li><b>A:</b> in the figure above, <b>A</b> represents a distribution with positive excess kurtosis, <b style="color:red;">B</b> represents normal kurtosis, <b style="color:green">C</b> represents negative excess kurtosis, while <b style="color:blue">D</b> is an extreme case of non-peakedness, the uniform distribution.</li>
</ul>
</div>

========================================================
## The uniform distribution


* The <b>uniform distribution $U(a, b)$</b> is defined such that all <strong>intervals of the same length on the distribution’s support are equally probable</strong>. 

* Suppose $a=0$ and $b=1$, we will use the `dunif` function to plot the probability density function to plot the density function similarly to earlier examples:

```{r fig.width=24, fig.height=5.5}
par(cex = 2.0, mar = c(5, 4, 4, 2) + 0.3)
f = dunif(x=seq(-1,2,by=0.01), min=0, max=1)
plot(x=seq(-1,2,by=0.01), f, type = "s", main = "Uniform distribution on [0,1]", xlab = "x", ylab = "Prob.")
```

* The support is defined by the <b>two parameters</b>, $a$ and $b$, which are its <strong>minimum and maximum values</strong>. 

========================================================
### The uniform distribution

```{r fig.width=24, fig.height=5.5, echo=FALSE}
par(cex = 2.0, mar = c(5, 4, 4, 2) + 0.3)
f = dunif(x=seq(-1,2,by=0.01), min=0, max=1)
plot(x=seq(-1,2,by=0.01), f, type = "s", main = "Uniform distribution on [0,1]", xlab = "x", ylab = "Prob.")
```

* Notice given the above shape, and the description of the probability as the area under the curve.

* <b>Q:</b> the uniform distribution gives zero probability to any interval outside of $[a,b]$, and if the total area must equal to one -- what must the height of the uniform distribution be equal to?

* <b>A:</b> we can use the basic property of the area of a rectangle, the width (equal to $b-a$) times the height (the density function $f_X(x)$) must multiply to one.

* Therefore, for an arbitrary uniform distribution over $[a,b]$ the density curve will be given by,
  
  $$\begin{align}
  f(x,a,b) = 
  \begin{cases}
  \frac{1}{b-a} & x \in [a,b]\\
  0 & x\notin [a,b]
  \end{cases}
  \end{align}$$


========================================================
### The uniform distribution


* <b>Q:</b> now that we have found the density curve for the uniform distribution 
 
  $$\begin{align}
  f(x,a,b) = 
  \begin{cases}
  \frac{1}{b-a} & x \in [a,b]\\
  0 & x\notin [a,b]
  \end{cases}
  \end{align}$$

  can you find the expected value of an arbitrary uniformly distributed random variable $U \sim U(a,b)$?
  
* <b>A:</b> consider that,
  
  $$\begin{align}
  \mathbb{E}\left[ U\right] &=\int_{a}^{b} x \frac{1}{b-a} \mathrm{d}x \\ 
  &= \frac{x^2}{2(b-a)}\Big{\vert}_{a}^b  = \frac{b^2 - a^2}{2(a-b)} = \frac{(b-a)(b+a)}{2(b-a)} = \frac{b+a}{2}
  \end{align}$$
  
* That says, the <b>expected value (or center of mass)</b> <strong>lies exactly in the midpoint of the interval</strong>.

  * Likewise, it is easy to see that the median will align with the center of mass here by the symmetry about the midpoint.

* We can similarly show that $\mathrm{var}=\frac{(b-a)^2}{12}$ and by symmetry, we know that the skewness is zero by default.
  

========================================================
### The uniform distribution


<ul>
  <li>An extremely important property about the <b>uniform distribution</b> for simulation purposes has to do with the notion again of <strong>quantiles</strong>.</li>
  <li> Let $F_X$ be the cdf of an arbitrary rv $X$; then $X$ can be <b>converted to a uniform distribution</b> via the <strong>probability integral transform</strong>.</li>
  <li> Notice firstly, that if $F_X(X)$ is read as a composition of the function,<br><br>
  $$\begin{align}
  X : \Omega \rightarrow \mathbb{R} \\
  F_X: \mathbb{R} \rightarrow [0,1]\\ \\
  \end{align}$$
  we can see $F_X(X)$ as a <strong>random variable taking values in the interval $[0,1]$</strong>.</li>
  <li>It is a general property that $X$ has the CDF $F_X$, then <b>$F_X(X) \sim U(0,1)$</b>, where<br><br>
    $$\begin{align}
    F_X(X) = F_X(X=t) = \int_{\infty}^t f_X(x) \mathrm{d}x\\\\
    \end{align}$$
    and <strong>the attained value $t$ of $X$ depends on the random outcome $\omega\in\Omega$</strong>.</li>
    </li>
  <li>On the other hand, suppose that <b>$U \sim U(0,1)$ is a uniform random variable on the unit interval</b>,</li>
  <ul>
    <li>then $F_X^{-1}(U)$ has a CDF of $F_X$ and we say that <strong>$X$ and $F_X^{-1}(U)$ have the same distribution</strong>.</li>
  </ul>
  <li>Practically speaking, this means that if we can <b>simulate the uniformly distributed variable $U\sim [0,1]$</b>, we can <strong>compose this with an arbitrary CDF to generate a different random variable</strong>.
</ul>




========================================================
### The uniform distribution
   
* In R the <storng>generic functions</strong> for the <b>uniform distribution</b> are the following:

  * `dunif(x, min, max)` is the <b>probability density</b> function of the uniform.
  * `punif(q, min, max)` is the <b>cumulative density</b> funciton of the uniform.
  * `qunif(p, min, max)` is the <b>quantile</b> function of the uniform.
  * `runif(n, min, max)` randomly generates a <b>sample</b> of size n from the uniform

* Note that `dunif` also contains the argument `log` which allows for computation of the log density, useful in the likelihood estimation.


========================================================
## The normal distribution

<ul>
  <li>The <b>normal distribution</b> is considered the <strong>most prominent distribution in statistics</strong>.</li>
  <li>It is a continuous probability distribution that has a <strong>bell-shaped probability density</strong>
function, also known as the <b>Gaussian function</b>.</li>
  <li>The <b>normal distribution</b> arises from the <strong>central limit theorem (CLT)</strong>,</li>
  <ul>
    <li>under weak conditions, the <strong>sum of a large number of rvs drawn from the same distribution is distributed approximately normally</strong> <b>irrespective of the form of the original distribution</b>.</li>
  </ul>
  <li>This gives mathematical justification to why we see normally distributed data quite often in practice; as was noted by Henri Poincare</li>
  <ul>
    <li><blockquote style="background-color:white">
"Everybody believes in the exponential law of errors [i.e., the
normal / Gaussian distribution]: the experimenters, because they think it
can be proved by mathematics; and the mathematicians,
because they believe it has been established by observation."
— Poincare, Henri “Calcul Des Probabilités.”
</blockquote></li>
  </ul>
  <li>In addition to the ubiquity of the normal
distribution, it can be easily manipulated analytically in equations,</li>
  <ul>
    <li> this enables one to derive a large number of results in explicit form.</li>
  </ul>
  <li>Due to these two aspects, the normal distribution is used extensively in theory and practice.</li>
</ul>

========================================================
## The normal distribution

<ul>
  <li>Formally, we will describe the <b>Gaussian pdf</b> as,
  $$\begin{align}
  \phi\left(x,\mu,\sigma^2\right) = \left( 2 \pi \sigma^2 \right)^{-2} \exp\left\{-\left(x - \mu\right)^2/ \left(2 \sigma^2\right)\right\},
  \end{align}$$

* In R, this is encoded as `dnorm(x=value, mean=mu, sd=1)` and we can picture the <strong>standard normal or Gaussian density</strong> below:

```{r fig.width=24, fig.height=5.5, echo=FALSE}
par(cex = 2.0, mar = c(5, 4, 4, 2) + 0.3)
f = dnorm(x=seq(-5,5,by=0.01), mean=0, sd=1)
plot(x=seq(-5,5,by=0.01), f, type = "s", main = "Standard normal distribution curve", xlab = "x", ylab = "Prob.")
```

========================================================
## The normal distribution

* In order to work with this distribution in R, there is a list of standard implemented functions: 

  * `dnorm(x, mean, sd)` for the pdf (if argument log = TRUE then log density); 
  * `pnorm(q, mean, sd)` for the cdf; 
  * `qnorm(p, mean, sd)` for the quantile function; and 
  * `rnorm(n, mean, sd)` for generating random normally distributed samples. 
  
* Their parameters are:

  * `x`, a vector of quantiles,
  * `p`, a vector of probabilities, and 
  * `n`, the number of observations. 

* If the mean and standard deviation are not specified, are set to the standard normal values by default.

* It will become clear in the next lecture the central place the normal distribution occupies, by the number of other distributions that are closely related or derived from the normal.


========================================================
## The normal distribution

* Another useful property of the <b>family of normal distributions</b> is that it is <strong>closed under linear transformations</strong>. 

* Thus a linear combination of two independent normal rvs, 
  
  $$\begin{align}
  X_1\sim N(\mu_1 , \sigma_1^2 ) & & 
  X_2 \sim N(\mu_2, \sigma_2^2 ),
  \end{align}$$
  is also normally distributed:
   
  * i.e.,
  
  $$aX_1 + bX_2 + c \sim N\left(a \mu_1 + b\mu_2 + c, a^2 \sigma_1^2 + b^2 \sigma_2^2\right)$$
  
* This is actually a <strong>general property</strong> of the family of <b>stable distributions</b> which is discussed in greater detail in the recommended reading.