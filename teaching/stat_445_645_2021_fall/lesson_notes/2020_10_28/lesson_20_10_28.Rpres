<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>


Unconstrained optimization
========================================================
date: 10/28/2020
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

<ul>
  <li>The following topics will be covered in this lecture:</li>
  <ul>
    <li>Concepts in optimization</li>
    <li>Local versus global optimum</li>
    <li>Convexity</li>
    <li>Gradient descent versus Newton descent</li>
    <li>BFGS</li>
    <li>Least-squares</li>
    <li>Gauss-Newton</li>
</ul>
</ul>

========================================================
## Concepts in optimization

* The maximization and minimization of functions, or <b>optimization problems</b>, contain two components:
  1. an <b style="color:#d95f02">objective function $f(\mathbf{x})$</b>; and
  2. <b style="color:#1b9e77">constraints $g(\mathbf{x})$</b>.
  
* E.g., we may wish to <b style="color:#d95f02">optimize factory output $f(x)$</b> as a function of hours $x$ in a week, with a measure of our <b style="color:#1b9e77">active machine-hours $g(x)$ not exceeding a pre-specified limitation $g(x)\leq C$</b>.
  
* Optimization problems can thus be classified into two categories:
  
  * If there are <b style="color:#1b9e77">constraints $g(\mathbf{x})$</b> affiliated with the objective function $f(\mathbf{x})$, then it is a <b style="color:#1b9e77">constrained optimization problem</b>, <strong>otherwise, it is a unconstrained optimization problem</strong>.
  
* We will focus on <strong>unconstrained optimization as often arises in MLE</strong>; this is formulated as the following problem,

  $$\begin{align}
  f: \mathbb{R}^n &\rightarrow \mathbb{R}\\
     \mathbf{x} &\rightarrow  f(\mathbf{x})\\
  f(\mathbf{x}^\ast) &= \mathrm{max}_{\mathbf{x} \in \mathcal{D}} f
  \end{align}$$


* We note that the above problem is equivalent to a minimization problem by a subsitution of $\tilde{f} = -f$, i.e.,

  $$\begin{align}
  \tilde{f}: \mathbb{R}^n &\rightarrow \mathbb{R}\\
     \mathbf{x} &\rightarrow  -f(\mathbf{x})\\
  f(\mathbf{x}^\ast) &= \mathrm{max}_{\mathbf{x} \in \mathcal{D}} \tilde{f} = \mathrm{min}_{\mathbf{x}\in \mathcal{D}} f
  \end{align}$$

========================================================
### Concepts in optimization

<div style="float:left; width:60%">
<ul>
  <li> Because these problems are equivalent, we <strong>focus on the minimization of functions</strong> at the moment as they are traditionally phrased in optimization literature.</li>
  <li>The same techniques will apply for maximization by a simple change of variables.</li>
  <li> We will need to identify a few key concepts: <b style="color:#1b9e77">global</b> and <b style="color:#d95f02">local</b> minimizers.</li>
  <li> Suppose we are trying to minimize an objective function $f$.</li>
  <li>We would ideally find a <b style="color:#1b9e77">global minimizer of $f$</b>, a point where the function attains its least value over all possible values under consideration:
<blockquote>
A point $\mathbf{x}^\ast$ is a global minimizer if $f(\mathbf{x}^\ast) \leq f(\mathbf{x})$ for all other possible $\mathbf{x}$ in the domain of consideration $D\subset \mathbb{R}^n$.
</blockquote></li>
  <li> A <b style="color:#1b9e77">global minimizer can be difficult to find</b>, because our <b style="color:#d95f02">knowledge of $f$ is usually only local</b>;</li>
</ul>
</div>
<div style="float:left; width:40%" class="fragment">
<img style="width:100%", src="difficult_minimization.png" alt="A difficult global minimization."/><p style="text-align:center"> Courtesy of: J. Nocedal and S. Wright. <i>Numerical optimization</i>. Springer Science & Business Media, 2006.</p>
</div>
<div style="float:left; width:100%">
<ul>
  <ul>
    <li>i.e., we only can approximate the behavior of the function $f$ within <b style="color:#d95f02">small perturbations $\boldsymbol{\delta}_{x}$ of values $\mathbf{x}$</b> where we already know $f(\mathbf{x})$.</li>
  </ul>
  <li>Since our algorithm hopefully does not need to compute $f$ over many points, we usually do not have a good picture of the overall shape of $f$,</li>
  <ul>
    <li>generally, we can never be sure that the function does not take a sharp dip in some region that has not been sampled by the algorithm.</li> 
  </ul>
</ul>
</div>

========================================================
### Concepts in optimization


* <b style="color:#d95f02">Most algorithms are able to find only a local minimizer</b>, which is a point that achieves the smallest value of $f$ in
its neighborhood, i.e.,

<blockquote>
Let $\mathcal{N}\subset \mathcal{D} \subset\mathbb{R}^n$ be a neighborhood of the point $\mathbf{x}^\ast$ in the domain of consideration.  We say $\mathbf{x}^\ast$ is a local minimizer in the neighborhood of $\mathcal{N}$ if
$$\begin{align}
f(\mathbf{x}^\ast) \leq f(\mathbf{x}) & & \text{ for each other }\mathbf{x}\in \mathcal{N}
\end{align}$$
</blockquote>

* For finding a local minimizer, the main tools will be derived directly from the <strong>second order approximation of the objective function $f$</strong>, defined by

  $$\begin{align}
  f(\mathbf{x}_1) \approx f(\mathbf{x}_0) + \left(\nabla f(\mathbf{x}_0)\right)^\mathrm{T} \boldsymbol{\delta}_{x_1}+\frac{1}{2} \boldsymbol{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\mathbf{x}_0) \boldsymbol{\delta}_{x_1}
  \end{align}$$
  
* We will consider how this is related to the notion of <b>convexity</b> as follows.

========================================================

### Convexity

<div style="float:left; width:30%">
<img style="width:100%", src="convex_super_graph.png" alt="Image of the upper region enclosed by a convex function."/><p style="text-align:center"> Courtesy of: Oleg Alexandrov. Public domain, via <a href="https://commons.wikimedia.org/wiki/File:Convex_supergraph.svg" target="blank">Wikimedia Commons</a>.</p>
</div>
<div style="float:left; width:70%">
 
<ul> 
  <li>Throughout mathematics, the notion of convexity is a powerful tool, often used in optimization.</li>
  <li>Particularly, a <b>function is convex</b> if and only if <strong>the region above its graph is a convex set</strong>.</li>
  <ul>
    <li>The <b>convexity of the full epigraph</b> set means that the function attains a <b style="color:#1b9e77">global minimum over its entire domain</b>.</li>
  </ul>
  <li>In <b style="color:#d95f02">non-convex functions</b>, we can have regions that are also <b style="color:#d95f02">locally convex</b> in the graph of the function.</li>
  <li>For such regions, we can find <b style="color:#d95f02">local minimizers</b> as defined in the last slide.</li>
  <li>In a single variable, this is phrased in terms of the <b>second derivative test</b>, i.e.,
  <blockquote>
  For the function of one variable $f(x)$ we say that $x^\ast$ is a local minimizer if $f'(x^\ast)=0$ and $f''(x^\ast)&gt; 0$.
  </blockquote>
  <li>There is a direct analogy for a function of multiple variables, but this need to be rephrased slightly.</li>
  </ul>
</div>

========================================================

### The second derivative test with the Hessian

<div style="float:left; width:40%">
<ul>
  <li>For a real-valued function of multiple variables,
  $$f:\mathbb{R}^n \rightarrow \mathbb{R},$$
  we will instead phrase the <strong>second derivative test in terms of the Hessian of $f$</strong>,
  $$\begin{align}
  \mathbf{H}_{f} = 
  \begin{pmatrix}
  \partial_{x_1}^2 f & \cdots & \partial_{x_1}\partial_{x_n}f \\
  \vdots &\ddots & \vdots \\
  \partial_{x_n}\partial_{x_1} f  & \cdots & \partial_{x_n}^2 f
  \end{pmatrix}
  \end{align}$$
  </li>
  <li>Particularly, the spectral theorem says the Hessian has an eigen decomposition such that by a change of coordinates, $\mathbf{H}_f$ will be diagonal.</li>
</ul>
</div>
<div style="float:right; width:55%" class="fragment">
<img style="width:100%", src="Paraboloids.png" alt="Different critical points for different Hessian spectrum."/><p style="text-align:center"> Courtesy of: <a href="https://commons.wikimedia.org/wiki/File:Parabol-el-zy-hy-s.svg">Ag2gaeh</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons.</p>
</div>
<div style="float:left; width:100%">
<ul>
  <li>The different combinations of <b>eigenvalues of the Hessian</b> will determine the <strong>local curvature of the graph of $f$</strong> with examples in two dimensions pictured above:</li>
  <ol>
    <li><b>Left:</b> we see a convex equation for the simple paraboloid -- around this critical point, the eigenvalues of $\mathbf{H}_f$ will all be strictly positive.</li>
    <li><b>Middle:</b> we see an equation with no global minimum, but infinitely local minima -- all points $(x,y)$ with $x=0$ are critical points, but this is convex in only in the $x$ direction and the $y$ direction will correspond to a zero eigenvalue of $\mathbf{H}_f$.</li>
    <li><b>Right:</b> there is a critical saddle point at $(0,0)$ but this is not even a local mimizer -- here the Hessian $\mathbf{H}_f$ will have one positive and one negative eigenvalue.</li>
  </ol>
  <li>These examples extend into higher dimensions, and generally we say that the function is locally convex at $\mathbf{x}^\ast$ when the Hessian has only positive eigenvalues at $\mathbf{x}^\ast$.</li>
</ul>
</div>

========================================================

### The second derivative test with the Hessian

* We can understand the second derivative test with the Hessian using our <b>second order Taylor approximation</b> as follows.

* Let $\mathbf{x}^\ast$ be a critical point and $\mathbf{x}_1 = \mathbf{x}^\ast + \boldsymbol{\delta}_{x_1}$ be a perturbation of this in the neighborhood $\mathcal{N}$.

* Suppose that the Hessian has only positive eigenvalues at $\mathbf{x}^\ast$, then our approximation at second order gives
  $$\begin{align}
  f(\mathbf{x}_1) &\approx f(\mathbf{x}^\ast) + \left(\nabla f(\mathbf{x}^\ast)\right)^\mathrm{T} \boldsymbol{\delta}_{x_1}+ \frac{1}{2} \boldsymbol{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\mathbf{x}^\ast) \boldsymbol{\delta}_{x_1} \\
  &= f(\mathbf{x}^\ast) + \frac{1}{2} \boldsymbol{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\mathbf{x}^\ast) \boldsymbol{\delta}_{x_1}
  \end{align}$$
  
* Provided $\mathcal{N}$ is a small enough neighborhood, the $\mathcal{o}\left(\parallel \boldsymbol{\delta}_{x_1}\parallel^3\right)$ will remain very small.

* However, $\boldsymbol{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\mathbf{x}^\ast) \boldsymbol{\delta}_{x_1}$ must be positive by the positive eigenvalues of the Hessian.

* This says for a radius sufficiently small, $\parallel \boldsymbol{\delta}_{x_1}\parallel$, and any perturbation of the point $\mathbf{x}^\ast$ defined as $\mathbf{x}_1 = \mathbf{x}^\ast + \boldsymbol{\delta}_{x_1}$, we have
  $$f(\mathbf{x}_1) \geq f(\mathbf{x}^\ast).$$

* Therefore, we can identify a local minimizer whenever the gradient is zero and the Hessian has positive eigenvalues, due to the local convexity.


========================================================

## Gradient descent vs Newton descent

<div style="float:left; width:55%">
<ul>
  <li> We noted before that the <b>gradient $\nabla f$</b> is the <strong>direction and the velocity of the greatest rate of increase of the function $f$</strong>.</li>
  <li> There are some good reasons to consider thus following the direction $-\nabla f$ to find a local minimizer.
  <li>However, it is possible that we will overshoot the local minimum if we take too long of a step along this direction.</li>
  <li>We also may not know a good choice for what length of step to use, for example consider the following figure.</li>
</ul>
</div>
<div style="float:right; width:40%" class="fragment">
<img style="width:100%", src="gradient_descent_edit.png" alt="Gradient descent."/><p style="text-align:center"> Courtesy of: J. Nocedal and S. Wright. <i>Numerical optimization</i>. Springer Science & Business Media, 2006.</p>
</div>
<div sytle="float:left; width:55%">
<ul>
  <li>The contours represent fixed values for the function $f$; i.e.,</li> 
  <li>if a contour is defined $\mathcal{C}$, then for all $\mathbf{c}_0 \in \mathcal{C}$, 
  $$f(\mathbf{c}_0 ) = C$$ 
  for a fixed value $C$.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>For $t &gt; 0$ we define a perturbation along the gradient this vector by $\mathbf{x}_k - t \nabla f$. </li>
  <li>For most of this direction,
  $$f(\mathbf{x}_k - t \nabla f) \leq f(\mathbf{x}_k)$$
  as this moves to the inner contours around $\mathbf{x}^\ast$.</li>
  <li>However, for $t$ large enough, this is not true and we do not know by default what size $t$ is appropriate, or if $f$ is extremely sensitive in this way.</li>
</ul>
</div>

========================================================

### Gradient descent vs Newton descent


<div style="float:left; width:55%">
<ul>
  <li>Let's recall <b>Newton's method</b> to find the appropriate direction and step length:</li>
  <ul>
    <li> When we were looking for the zero of a function, we used the <strong>first order approximation and found where this function takes the value zero</strong>.</li>
  </ul>
  <li> We will consider a similar idea using the <strong>second order approximation</strong>, 
  $$\begin{align}
  m(\boldsymbol{\delta}_{x_1}) = f(\mathbf{x}_0) + \left(\nabla f(\mathbf{x}_0)\right)^\mathrm{T} \boldsymbol{\delta}_{x_1}+\frac{1}{2} \boldsymbol{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\mathbf{x}_0) \boldsymbol{\delta}_{x_1}.
  \end{align}$$</li>
  <li>By setting the derivative of $m$ with respect to the perturbation $\boldsymbol{\delta}_x$ equal to zero for some $\boldsymbol{\delta}_{x_1}$:
  $$\begin{align}
  & 0 =   \nabla f(\mathbf{x}_0)  + \mathbf{H}_f(\mathbf{x}_0) \boldsymbol{\delta}_{x_1} \\
  \Leftrightarrow & \boldsymbol{\delta}_{x_1} = -\left(\mathbf{H}_f(\mathbf{x}_0)\right)^{-1} \nabla f(\mathbf{x}_0)
  \end{align}$$</li>
  <li>If $\mathbf{H}_f$ has an inverse at $\mathbf{x}_0$, and if $\mathbf{H}_f(\mathbf{x}_0)$ has positive eigenvalues, this gives a descent direction in $f$.</li>
</ul>
</div>
<div style="float:right; width:35%" class="fragment">
<img style="width:100%", src="newton_descent_edit.png" alt="Newton descent."/><p style="text-align:center"> Courtesy of: J. Nocedal and S. Wright. <i>Numerical optimization</i>. Springer Science & Business Media, 2006.</p>
</div>
<div style="float:left; width:55%">
<ul>
  <li>Particularly, our new choice for the estimated minimum will be given by $\mathbf{x}_1 = \mathbf{x}_0 + \boldsymbol{\delta}_{x_1}$ for which $f(\mathbf{x}_0)\geq f(\mathbf{x}_1)$.</li>
  <li>We may find a lower value along the direction of gradient descent, but without sampling this direction many times over, we do not by default know where this will lie.
</ul>
</div>


========================================================

### Newton descent

* In <b>Newton descent</b>, for each $k$ we repeat the step to define the $k+1$ approximation,
  $$\begin{align}
  & 0 =   \nabla f(\mathbf{x}_k)  + \mathbf{H}_f(\mathbf{x}_k) \boldsymbol{\delta}_{x_{k+1}} \\
  \Leftrightarrow & \boldsymbol{\delta}_{x_{k+1}} = -\left(\mathbf{H}_f(\mathbf{x}_k)\right)^{-1} \nabla f(\mathbf{x}_k)\\
  &\mathbf{x}_{k+1} = \mathbf{x}_k + \boldsymbol{\delta}_{x_k}.
  \end{align}$$


* This process will continue until the approximation reaches an error tolerance (when we have a good initial guess $\mathbf{x}_0$) or terminate after timing out, failing to converge if we are not in an appropriate neighborhood $\mathcal{N}$ of a minimizer $\mathbf{x}^\ast$.

  * When we are in a <b style="color:#d95f02">locally convex neighborhood $\mathcal{N}$ containing $\mathbf{x}^\ast$</b>, we can be assured that $\mathbf{H}_f$ will be invertible, and that it will have strictly positive eigenvalues, making the initial choice very important in producing a result.

* Unlike the gradient vector alone, this <strong>gives a step size choice derived by the local geometry</strong>, and this tends to converge to the local minimum quickly as long as the initial choice $\mathbf{x}_0$ is in the neighborhood $\mathcal{N}$.

* However, this method does not know if there is a better minimizing solution $\mathbf{x}_0^\ast$ that lies in a different neighborhood $\mathcal{N}_0$.

  * The results will depend strongly on the shape of the objective function $f$, our initial choice $\mathbf{x}_0$ and whatever prior knowledge about the problem we include in making such an initial guess.
  
* <b>Newton descent</b> is the <strong>basis for a wide class of line-search methods in optimization</strong>, i.e., where we try to optimize a function by finding a good descent direction and choose an appropriate step size based on the direction.

========================================================

### BFGS

* The main issue with computing the minimization problem with a direct Newton descent as above is that $\mathbf{H}_f$ <strong>is very computationally expensive to compute for any moderate number of variables</strong>.

 * If the system size has $n$ variables, $\mathbf{x}_0 \in \mathbb{R}^n$, then $\mathbf{H}_f(\mathbf{x}_0) \in \mathbb{R}^{n\times n}$.

*  Moreover, we need to recompute this quite often if we need to make multiple steps.
  
* One popular way of avoiding the Hessian computation is the BFGS formula, named after its inventors, Broyden, Fletcher, Goldfarb, and Shanno.

  * The authors (approximately) simultaneously invented the algorithm independently, and the algorithm is named for them alphabetically.

* We will go through a short description on how the approximation is made, but we will not belabor the details which go beyond our scope.

========================================================

### BFGS

* Taylor's theorem can also be applied to the gradient of $f$ as follows to derive the necessary approximation.

* Suppose that $f$ has two continuous partial derivatives and $\mathbf{x}_1 = \mathbf{x}_0 + \boldsymbol{\delta}_{x_1}$ is a small perturbation from an initial point $\mathbf{x}_0$.

* Suppose we take $\mathbf{f}(x)$ to be the function defined by the gradient of $f$, i.e.,

  $$\begin{align}
  \mathbf{f}:\mathbb{R}^n& \rightarrow \mathbb{R}^n \\
  \mathbf{x} &\rightarrow \nabla f (\mathbf{x})
  \end{align}$$
  
* $\mathbf{f}$ also has a Taylor expansion, again taking the gradient, but of a vector valued function, i.e.,
  
  $$\nabla \mathbf{f} = \nabla^2 f = \mathbf{H}_f$$
  which can be shown by following the definition of taking the gradient of a vector valued function, and the definition of the Hessian.
  
* Therefore, taking the <strong>first order Taylor series</strong> of $\mathbf{f}$, we have

  $$\begin{align}
  \mathbf{f}(\mathbf{x}_1) &= \mathbf{f}(\mathbf{x}_0) + \nabla \mathbf{f}(\mathbf{x}_0) \boldsymbol{\delta}_{x_1} + \mathcal{o}\left(\parallel \boldsymbol{\delta}_{x_1}\parallel \right) \\
  \Leftrightarrow \nabla f (\mathbf{x}_1)&= \nabla f (\mathbf{x}_0) + \mathbf{H}_f(\mathbf{x}_0) \boldsymbol{\delta}_{x_1} +  \mathcal{o}\left(\parallel \boldsymbol{\delta}_{x_1}\parallel \right) \\
    \Leftrightarrow \nabla f (\mathbf{x}_1) - \nabla f (\mathbf{x}_0) &=   \mathbf{H}_f(\mathbf{x}_0) \boldsymbol{\delta}_{x_1} + \mathcal{o}\left(\parallel \boldsymbol{\delta}_{x_1}\parallel \right) \\
  \end{align}$$
  


========================================================

### BFGS

* Recall our approximation from the last slide
  
  $$\nabla f (\mathbf{x}_1) - \nabla f (\mathbf{x}_0) =   \mathbf{H}_f(\mathbf{x}_0) \boldsymbol{\delta}_{x_1} + \mathcal{o}\left(\parallel \boldsymbol{\delta}_{x_1}\parallel \right).$$
  
  
* We say that for a small perturbation $\boldsymbol{\delta}_{x_1}$ and for $\mathbf{x}_1,\mathbf{x}_0 \in \mathcal{N}$, the locally convex neighborhood, 

  * the action of the Hessian on the perturbation
  
  $$\mathbf{H}_f(\mathbf{x}_0) \boldsymbol{\delta}_{x_1}$$ 
  is well approximated by the finite difference:
  
  $$\mathbf{H}_f(\mathbf{x}_0) \boldsymbol{\delta}_{x_1} \approx \nabla f (\mathbf{x}_1) - \nabla f (\mathbf{x}_0).$$
  
* We thus <b>choose a Hessian approximation</b> $\widetilde{\mathbf{H}}_f$ so that it <strong>mimics the finite difference equation of the true Hessian</strong>; 

  * that is, we require it to satisfy the following condition, known as the secant equation:

  $$\widetilde{\mathbf{H}}_f(\mathbf{x}_{k}) \boldsymbol{\delta}_{x_{k+1}} = \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_{k})$$

* We will also need to impose that the estimate $\widetilde{\mathbf{H}}_f(\mathbf{x}_{k})$ is symmetric by default; 

  * also, we prefer that $\widetilde{\mathbf{H}}_f(\mathbf{x}_{k+1})$ differs only by a small amount the last-computed approximation $\widetilde{\mathbf{H}}_f(\mathbf{x}_{k})$.


========================================================

### BFGS

* For simplicity in the notations, let us denote $\mathbf{B}_k = \widetilde{\mathbf{H}}_f(\mathbf{x}_k)$, $\boldsymbol{\delta}_{x_k} = \boldsymbol{\delta}_k$ and $\mathbf{y}_k = \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k)$.

* We will define the <b>BFGS Hessian approximation</b> as,
 
 $$\begin{align}
 \mathbf{B}_{k+1} = \mathbf{B}_{k} - \frac{\mathbf{B}_{k}\boldsymbol{\delta}_{k+1} \boldsymbol{\delta}_{k+1}^\mathrm{T} \mathbf{B}_{k}}{\boldsymbol{\delta}_{k+1}^\mathrm{T} \mathbf{B}_{k} \boldsymbol{\delta}_{k+1}} + \frac{\mathbf{y}_k \mathbf{y}_k^\mathrm{T}}{\mathbf{y}_k^\mathrm{T} \boldsymbol{\delta}_{k+1}}
 \end{align}$$
  where:
  
  * the two right-hand-side terms are actually two <strong>matrix-valued, weighted outer products</strong> of the vectors $\mathbf{B}_{k}\boldsymbol{\delta}_{k+1}$ and $\mathbf{y}_k$ with themselves;
  
  * the above implies that the next approximation is only a difference from the last one by a <strong>rank-2 difference</strong>, i.e.,
  
  $$- \frac{\mathbf{B}_{k}\boldsymbol{\delta}_{k+1} \boldsymbol{\delta}_{k+1}^\mathrm{T} \mathbf{B}_{k}}{\boldsymbol{\delta}_{k+1}^\mathrm{T} \mathbf{B}_{k} \boldsymbol{\delta}_{k+1}} + \frac{\mathbf{y}_k \mathbf{y}_k^\mathrm{T}}{\mathbf{y}_k^\mathrm{T} \boldsymbol{\delta}_{k+1}}$$
  has only <strong>2 non-zero (non-trivial to compute) eigen directions</strong>, regardless of the dimension $n>2$; 
  
  * the above $\mathbf{B}_{k+1}$ remains <strong>symmetric and has only positive eigenvalues</strong> when $\mathbf{B}_0$ satisfies this condition and $\boldsymbol{\delta}_{k+1}^\mathrm{T} \mathbf{y}_k &gt; 0$.
  
  * the above can be solved for $\mathbf{B}_{k+1}$ to produce 
    $$\boldsymbol{\delta}_{x_{k+2}} = - \mathbf{B}_{k+1}^{-1} \nabla f_{k+1}$$

*  We will not discuss the approximation further, only note that this works as a fairly general and efficient approximation.


========================================================

### BFGS example

* Let us now consider a simple example with BFGS.

* Let us define the paraboloid function,
  $$f(x_1, x_2)= \left(3 - x_1 \right)^2 + \left(5 - x_2 \right)^2,$$
  
* or in R as

```{r}
f_exp <- expression( ( 3- x_1)^2 + (5 - x_2)^2) 
f <- function(x){(3 - x[1])^2 + (5 - x[2])^2}
```

========================================================

### BFGS

* Let us compute the gradient and the Hessian of the function analytically first:

```{r}
print(grad_f <- c(D(f_exp, "x_1"), D(f_exp, "x_2")))
print(hessian_f_11 <- D(D(f_exp,"x_1"),"x_1"))
print(hessian_f_12 <- D(D(f_exp,"x_1"),"x_2"))
print(hessian_f_22 <- D(D(f_exp,"x_2"),"x_2"))
```

* <b>Q:</b> what can be deduced from the above gradient and Hessian equation for the minimization problem?

  * <b>A:</b> the Hessian tells us once again the function is globally convex and there is a critical point at $(3,5)$.

========================================================

### BFGS

* We can verify this with the numerical gradient and Hessian as:

```{r}
require("numDeriv")
grad(f, x=c(3,5))
hessian(f, x=c(3,5))
```


========================================================

### BFGS

* The BFGS algorithm is provided in the package `optimx`.

* We will call the minimization procedure on the function `f` as follows:

```{r}
require("optimx")
fBFGS = optimx(fn = f, # define the objective function
   par = c(0, 0), # define the initial first guess at the minimum
   method ="BFGS") # define the method of solution
print(fBFGS)
```

* Here, this quickly finds the true minimum of the function `f` rapidly, due to the global convexity.

* More generally, it is possible that the minimization procedure will not converge due to structural issues in the problem.

* We will more examples of using BFGS and the second derivative test in multiple variables in our activities and homework.

========================================================

## Least squares problems

* In statistics and optimization problems in which we need to <strong>fit a model to data</strong>, one of the most common forms of objective functions is known as <b>least-squares</b>.

* In least-squares problems, the objective function $f$ has the following special form:

  $$\begin{align}
  f:\mathbb{R}^n &\rightarrow \mathbb{R} \\
  r_i : \mathbb{R}^n &\rightarrow \mathbb{R}\\
  \mathbf{x} :&\rightarrow  f(\mathbf{x}) = \sum_{i=1}^m r_i(\mathbf{x})^2
  \end{align}$$

* In the above expression, each of the $r_i$ ranging from $i=1,\cdots, m$ is called a <b>residual</b> where we assume $m &gt; n$;
  
  * $r_i$ <strong>measures the discrepancy of some prediction as a function of the free variables $\mathbf{x}$ from a measured value in reality</strong>.
  
  * $r_i$ thus refers to the discrepancy of the $i$-th prediction from the $i$-th case over $m &gt; n$ total observed cases.
  
* These types of problems arise in nearly any field that involve a predictive model and data, and this problem is at the heart of regression and machine learning.


========================================================

### Linear least squares problems

* Assume that $\boldsymbol{\phi}$ represents some prediction of a vector of observed values $\mathbf{y}$, but the prediction depends on the vector of free, tuneable parameters $\mathbf{x}$.

* The <strong>goal then is to find the "best" choice of tuneable parameters $\mathbf{x}$</strong> that can <b>minimize the difference between a prediction and reality</b>. 

* This gives an objective function of the form,

  $$\begin{align}
  f(\mathbf{x}) = \sum_{i=1}^m \frac{1}{2} \vert \boldsymbol{\phi}_i(\mathbf{x}) - \mathbf{y}_i\vert^2
  \end{align}$$
  where:
  1. $\boldsymbol{\phi}_i(\mathbf{x})$ and $\mathbf{y}_i$ represent the predicted and observed $i$-th case respectively; 
  2. in the above, we would call $r_i(\mathbf{x}) =  \frac{1}{2}\vert \boldsymbol{\phi}_i(\mathbf{x}) - \mathbf{y}_i\vert$; but
  3. we can generally take many other forms for the residual, amplifying specific features of the analysis with the specific choice.

* One very special case is where $\boldsymbol{\phi}(\mathbf{x})$ is actually a linear relationship in the values $\mathbf{x}$, in which case
  
  $$\boldsymbol{\Phi} \mathbf{x} = \mathbf{y}$$
  can be represented with the matrix equation in $\boldsymbol{\Phi}$.
  
* Unlike the typical linear inverse problem, $\mathbf{\Phi}\in \mathbb{R}^{m \times n}$ is not square (assuming $m &gt; n$), and no linear inverse exists in this problem for $\boldsymbol{\Phi}$.

========================================================

### The normal equations

* Using relationships of vector calculus, one can show

 $$\begin{align}
 \nabla f = \boldsymbol{\Phi}^\mathrm{T} \left(\boldsymbol{\Phi} \mathbf{x} - \mathbf{y}\right) & & \mathbf{H}_f= \boldsymbol{\Phi}^\mathrm{T}\boldsymbol{\Phi}
 \end{align}$$
 
* We note,  $\mathbf{H}_f= \boldsymbol{\Phi}^\mathrm{T}\boldsymbol{\Phi}$ is a <strong>constant-valued, matrix squared</strong> and (as long as $\boldsymbol{\Phi}$ has independent columns) the <b>Hessian has positive eigenvalues</b>;
  
  * this says that linear least squares is a globally convex problem when well posed.

* Similarly, if we take the unique minimizer to be defined as $\mathbf{x}^\ast$, 
  
  $$\begin{align}
  & \nabla f (\mathbf{x}^\ast) = 0 \\
  \Leftrightarrow & \boldsymbol{\Phi}^\mathrm{T}\boldsymbol{\Phi} \mathbf{x} =  \boldsymbol{\Phi}^\mathrm{T}\mathbf{y}
  \end{align}$$
  by definition.
  
* The above equations are known as the <b>normal equations</b> and have a <strong>unique solution by the property of the linear inverse problem</strong>

  $$ \mathbf{x} = \left(\boldsymbol{\Phi}^\mathrm{T}\boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^\mathrm{T}\mathbf{y}$$
  as long as the original $\boldsymbol{\Phi}$ is well-posed.
  
* This is the basis of linear regression, which we will return to after the midterm. 


========================================================

## Nonlinear least squares -- Gauss-Newton

* Generally, we may take <b>$\boldsymbol{\phi}$ to be nonlinear</b>, and a <strong>direct approach as above will not work</strong>.

* We can appeal to BFGS in this case to find a local minimum, but <strong>least squares has an extremely special structure that will lead to a more efficient approximation</strong>.

* Let us recall the general form for $f$ in terms of the residuals $r_i$
 
  $$\begin{align}
  f(\mathbf{x}) = \sum_{i=1}^m r_i(\mathbf{x})^2
  \end{align}$$
  
* Let us denote a vector of residuals its gradient as 
  
  $$\begin{align}
  \mathbf{r} = 
  \begin{pmatrix}
  r_1(\mathbf{x}) \\ \vdots \\ r_m(\mathbf{x})
  \end{pmatrix} 
  & &
  \nabla \mathbf{r} =
  \begin{pmatrix}
  \partial_{x_1} r_1(\mathbf{x}) & \cdots & \partial_{x_m} r_1(\mathbf{x}) \\
  \vdots & \ddots & \vdots \\
  \partial_{x_1} r_n(\mathbf{x}) & \cdots & \partial_{x_m} r_n(\mathbf{x})
  \end{pmatrix}
  \end{align}$$
  
* We can thus write,
  
  $$\begin{align}
  f(\mathbf{x}) = \mathbf{r}^\mathrm{T}\mathbf{r}
  \end{align}$$
  and utilize the special structure with the Taylor expansion.
  

========================================================

### Nonlinear least squares -- Gauss-Newton

*  Specifically, we can write

  $$\begin{align}
  \nabla f = \left(\nabla\mathbf{r}\right)^\mathrm{T} \mathbf{r} & & \mathbf{H}_f  =  \left(\nabla\mathbf{r}\right)^\mathrm{T}  \nabla\mathbf{r} + \sum_{i=1}^m r_i \mathbf{H}_{r_i}
  \end{align}$$
  
* In most cases, $\nabla \mathbf{r}$ is easy to calculate;
  
  * moreover, the term $\sum_{i=1}^m r_i \mathbf{H}_{r_i}$ tends to be much smaller than $\left(\nabla\mathbf{r}\right)^\mathrm{T}  \nabla\mathbf{r}$ when in a neighborhood $\mathcal{N}$ of a minimizer $\mathbf{x}^\ast$.

* This leads to a direct approximation of the Newton decent method where we define the second order approximation as

  $$\begin{align}
  m(\boldsymbol{\delta}_{x_1}) = f(\mathbf{x}_0) +  \left(\nabla\mathbf{r}\right)^\mathrm{T} \mathbf{r} \boldsymbol{\delta}_{x_1} + \frac{1}{2}\boldsymbol{\delta}_{x_1}^\mathrm{T} \left(\nabla\mathbf{r}\right)^\mathrm{T}  \nabla\mathbf{r}\boldsymbol{\delta}_{x_1}
  \end{align}$$
  
* Setting the derivative of the second order approximation to zero with respect to the perturbation, we get the <b>Gauss-Newton approximation </b>
  
  $$\boldsymbol{\delta}_{x_1} = - \left[ \left(\nabla\mathbf{r}\right)^\mathrm{T}  \nabla\mathbf{r} \right]^{-1}\left(\nabla\mathbf{r}\right)^\mathrm{T} \mathbf{r} $$
  which can be computed entirely in terms of the residuals and their gradient.
  
* The Gauss-Newton approximation is the basis of a variety of techniques to minimize a nonlinear least squares objective function; we will return to this in the activity / homework.
  
* One suggested final topic is to discuss the similarities and differences of this approach from Levenberg-Marquardt.
