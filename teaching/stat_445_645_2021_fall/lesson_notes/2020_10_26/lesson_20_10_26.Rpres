<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>


Numerical differentiation in R
========================================================
date: 10/26/2020
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

<ul>
  <li>The following topics will be covered in this lecture:</li>
  <ul>
    <li>Concepts in analytical differentiation</li>
    <li>An approach to numerical differentiation</li>
    <li>Newton's method in one variable.</li>
    <li>Functions of multiple variables.</li>
    <li>Gradient and Hessians.</li>
    <li>Multivariate Taylor approximation.</li>
    <li>Jacobians</li>
    <li>Newton's method in multiple variables</li>
  </ul>
</ul>

========================================================
## Concepts in analytical differentiation

* Using the rules of calculus, e.g., 
  1. the product rule;
  2. the power rule;
  3. the chain rule;
  
* we can <b style="color:#d95f02">compute derivatives of complex functions analytically</b>.

* However, if the function is only given in an <b style="color:#1b9e77">implicit form</b>, e.g., as the output of an algorithm, we still need <b style="color:#1b9e77">approximate ways to compute such derivatives</b>.

* We will begin by introducing concepts in <b style="color:#d95f02">analytical derivation</b> and then discuss <b style="color:#1b9e77">one approach to their approximation</b>.

  * This will include some examples on how to compute such analytical derivatives and approximations in R.

* We will follow this with one of the primary uses of such an approach, <strong>solving systems of nonlinear equations</strong>.

========================================================
### Concepts in analytical differentiation

* Recall, R has the ability to <strong>recognize mathematical expressions as objects</strong>.

  * E.g., let us create the following expression:

```{r}
f <- expression(x^3 * cos(x))
```

* R also has a means to differentiate such expressions automatically;

  * This is with the function 'D' that takes a syntax of the form

```
D(expression, variable_name)
```

* <b>Q:</b> suppose we use `f` as the expression and `x` as the variable name, what will be the result?

  * <b>A:</b> we can compute the above derivative analytically using a combination of the product and power rules to obtain:
  

```{r}
D(f, "x")
```


========================================================
### Concepts in analytical differentiation

<div style="float:left; width:45%">
<img style="width:100%", src="tangent.png" alt="Tangent line approximation by derivative."/>
<p style="text-align:center;">
Courtesy of   <a href="https://commons.wikimedia.org/wiki/File:Tangent-calculus.svg">Pbroks13</a>, <a href="http://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a>, via Wikimedia Commons
</p>
</div>
<div style="float:left; width:55%">
<ul>
  <li>The derivative represents the slope of a tangent line to a curve.</li>
  <li>In the figure to the left, we see the function <b style="color:#008080">$f$ represented by the blue curve</b>.</li>
  <li>The <b>derivative $f'(x)$</b> at a given point gives the infinitesimal rate of change at that point with respect to small changes in $x$, denoted $\delta_x$.</li>
  <li>Suppose we have a point $x_0$, a nearby point that differs by only a small amount in $x$
  $$x_1 = x_0+\delta_{x_1},$$</li>
  <li> The function 
  $$f(x_1) \approx f(x_0) + f'(x_0)\delta_{x_1}$$
  is what is known as the <b style="color:#663300">tangent line approximation</b> to the function $f$.</li>
  <li>Such an approximation exists when <strong>$f$ is sufficiently smooth</strong> and is <strong>accurate when $\delta_{x_1}$ is small</strong>, so that the difference of $x_1$ from the fixed value $x_0$ is small.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
   <li>We can see graphically how the approximation becomes worse as we take $\delta_{x_1}$ too large.</li>
</ul>
</div>

========================================================
### Concepts in analytical differentiation

* More generally, the tangent line approximation is one kind of <b>general Taylor approximation</b>.

* Suppose we have a point $x_0$ fixed, and define $x_1$ as a small perturbation
  $$x_1 = x_0+\delta_{x_1},$$

* If a function <strong>$f$ has $k$ continuous derivatives</strong> we can write
 $$f(x_1) =  f(x_0) + f'(x_0)\delta_{x_1} + \frac{f''(x_0)}{2!}\delta_{x_1}^2 + \cdots + \frac{f^{(k)}(x_0)}{k!} \delta_{x_1}^k + \mathcal{o}\left(\delta_{x_1}^{k+1}\right)$$
 
* The $\mathcal{o}\left(\delta_{x_1}^{k+1}\right)$ refers to terms in <b>the remainder</b>, that <strong>grows or shrinks like the size of the perturbation to the power $k+1$</strong>.
 
  * This is why this approximation works well when $\delta_{x_1}$ is a small perturbation.

* Another important practical example of using this Taylor approximation, when the function $f$ has two continuous derivatives, is
   $$f(x_0 + \delta_x) \approx f(x_0) + f'(x_0)\delta_x + f''(x_0) \frac{\delta_x^2}{2}$$
  which will be used shortly for obtaining solutions to several kinds of equations. 

* Particularly, this is strongly related to our <strong>second derivative test from univariate calculus</strong>.

========================================================
## An approach to numerical derivation

* At the moment, we consider how Taylor's expansion can be used at first order again to approximate the derivative.

* Recall, we write

  $$\begin{align}
  f(x_1) &= f(x_0) + f'(x_0) \delta_{x_1} + o\left( \delta_{x_1}^2\right) \\
  \Leftrightarrow  \frac{f(x_1) -  f(x_0)}{ \delta_{x_1}} &= f'(x_0) + o\left( \delta_{x_1}\right)
  \end{align}$$
  
* This says that for a small value of $\delta_{x_1}$, we can obtain the <b>numerical approximation of $f'(x_0)$ up to approximately to the accuracy of the largest decimal place of $\delta_{x_1}$</b> by the difference on the left hand side.

* This simple approach is the basic version of a finite difference equation, and approximation to the derivative.

* In simple cases this can be sufficiently accurate, but many more advanced variations give better approximation and numerical stability in the control of errors.

* We will return on how to compute such numerical derivatives in R when we introduce this in full generality of multiple variables.

========================================================
## Newtwon's method in one variable

* We have seen earlier the basic linear inverse problem,

  $$\begin{align}
  \mathbf{A}\mathbf{x} = \mathbf{b}
  \end{align}$$
  where $\mathbf{b}$ is an observed quantity and $\mathbf{x}$ are the unknown variables related to $\mathbf{b}$ by the relationships in $\mathbf{A}$.
  
  * We observed that a <b>unique solution exists when the $\mathrm{det}\left(\mathbf{A}\right)\neq 0$</b>, i.e., all the relationships expressed by the columns (or eigenvalues) are unique.
  
* A similar problem exists when the <strong>relationship between $\mathbf{x}$ and $\mathbf{b}$ is non-linear</strong>, but we still wish to find some such $\mathbf{x}$.

* Suppose we know the nonlinear function $f$ that gives a relationship in one variable as
  $$\begin{align}
  f(x^\ast) = b
  \end{align}$$
  for an observed $b$ but an unknown $x^\ast$.
  
* We will start by re-writing the equation into a more general form, define a function
  $$\begin{align}
  \tilde{f}(x) = f(x)-b.
  \end{align}$$

* Thus solving the nonlinear inverse problem in one variable is equivalent to finding the appropriate $x^\ast$ for which
  $$\begin{align}
  \tilde{f}(x^\ast)= 0 .
  \end{align}$$
  
* The means of finding one such $x^\ast$ is known as root finding.

* The <b>Newton-Raphson method</b> (often Newton's for short) is one classical approach which has inspired many modern techniques for complex systems of equations -- we will introduce the main concepts here.

========================================================
### Newtwon's method in one variable

<div style="float:left; width:55%">
<ul>
  <li>We are <strong>searching for the point $x^\ast\in \mathbb{R}$</strong> for which the modified equation <b>$\tilde{f}\left(x^\ast\right) = 0$</b>, and we suppose we have a good initial guess $x_0$.</li>
  <li> We define the tangent approximation as,
  $$t(\delta_x) = \tilde{f}(x_0) + \tilde{f}'(x_0) \delta_x$$
  for some small perturbation value of $\delta_x$.</li>
  <li> Recall, $\tilde{f}'(x_0)$ refers to the value of the derivative of $\tilde{f}$ at the point $x_0$ -- suppose this value is nonzero.</li>
  <li> In this case, we will <strong>examine where the tangent line intersects zero</strong> to find a better approximation of $x^\ast$.</li>
  <li> Suppose that for $\delta_{x_0}$ we have
  $$\begin{matrix}
  t(\delta_{x_0}) = 0  & 
  \Leftrightarrow & 0= \tilde{f}(x_0) + \tilde{f}'(x_0) \delta_{x_0}  &
  \Leftrightarrow  &\delta_{x_0} = \frac{-\tilde{f}(x_0)}{\tilde{f}'(x_0)}
  \end{matrix}$$
  </li>
  <li>The above solution makes sense <b>as long as $f'(x_0)$ is not equal to zero</b>;</li>
  <li>if not, this says that the tangent line intersects zero at
  $x_1 = x_0 - \delta_{x_0}$, giving a new approximation of $x^\ast$.</li>
</ul>
</div>
<div style="float:right; width:40%" class="fragment">
<img style="width:100%", src="NewtonIteration_Ani.gif" alt="Animation of Newton iterations."/>
<p style="text-align:center;">
Courtesy of  <a href="https://commons.wikimedia.org/wiki/File:NewtonIteration_Ani.gif">Ralf Pfeifer</a>, <a href="http://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a>, via Wikimedia Commons
</p>
</div>
<div style="float:left; width:100%">
<ul>
  <li>The process of recursively solving for a better approximation of $x^\ast$ terminates when we reach a certain tolerated level of error in the solution or the process times out, failing to converge.</li>
  <li>This method has a direct analog in multiple variables, for which we will need to return to the concept of the matrix inverse.</li>
  <li>We will return to this at the end of the lecture and for now consider a simple example.</li>
</ul>
</div>

========================================================
### Newtwon's method in one variable -- example

* Newton's method in a single variable is implemented by the function `uniroot` with syntax as

```
uniroot(function_to_root_find, interval_to_search_for_roots)
```

* We will consider the polynomial $x^2-4 = (x+2)(x-2)$ which clearly has roots at $\pm 2$,
```{r}
f <- function(x){
  return (x^2 - 4)
}
uniroot(f, c(-3, 0))
```

* Notice this solves for the root $-2$ in the interval.

========================================================
### Newtwon's method in one variable -- example

* Now consider,

```{r}
uniroot(f, c(0, 3))
```

========================================================
### Newtwon's method in one variable -- example

* But if we try the following interval,

```{r, error=TRUE}
uniroot(f,c(-3,3))
```

* we get an error message.

* This is because the Newton algorithm needs a good first guess, and here the values of $f(-3)$ and $f(3)$ are of the same sign 

  * in this case, the solver doesn't have enough information to begin a search with an initial $x_0$ and the interval should be shortened around the first proposal.

========================================================
## Multiple variables 


* Recall our earlier expression `f`

```{r}
f <- expression(x^3 * cos(x))
```

* <b>Q:</b> suppose we differentiate the expression with respect to `y`, what will be the answer?

  * <b>A:</b> all values in the above expression are constant with respect to the value `y` so that,
  
```{r}
D(f,"y")
```

* This is the basic principle with respect to partial derivatives: <strong>expressions that do not include a different variable, e.g., `y` can be held as constants with the derivative in the other variable</strong>.

========================================================
### Multiple variables

* Suppose we redefine our expression `f` as,

```{r}
f <- expression(x^3 * cos(x) * y + y)
```

* <b>Q:</b> what will the derivative of the above expression evaluate to when take with respect to `y`?  What about with respect to `x`?

  * <b>A:</b> we find that,
  
```{r}
D(f, "y")
D(f, "x")
```

* We can extend into arbitrary functions,

  $$\begin{align}
  f:\mathbb{R}^n& \rightarrow \mathbb{R} \\
  (x_1, x_2, \cdots, x_n) & \rightarrow f(x_1, x_2, \cdots, x_n) \\
  \mathbf{x} & \rightarrow  f(\mathbf{x})
  \end{align}$$
  
* The notation $\partial_{x_i}$ refers to the derivative with respect to the variable $x_i$ in the same sense as discussed in the above `D(f, x)` and `D(f, y)` example.

========================================================
### The gradient

<div style="float:left; width:45%">
<img style="width:100%", src="3d-gradient.png" alt="Diagram of kurtosis for different distributions."/>
<p style="text-align:center;">
Courtesy of <a href="https://commons.wikimedia.org/wiki/File:3d-gradient-cos.svg">MartinThoma</a>, CC0, via Wikimedia Commons
</p>
</div>
<div style="float:left; width:55%">
<ul>
  <li>As with the linear approximation in one variable,
  $$f(x_1) \approx f(x_0) + f'(x_0)\delta_{x_1}$$
  the derivative in multiple variables can be seen similar to the <b style="color:#d95f02">slope of a tangent line</b> but for a higher dimensional function.</li>
  <li>Instead of giving just a slope, in <b style="color:#1b9e77">higher dimensions we are looking at a tangent-vector</b>;</li>
  <ul>
    <li>this gives the <b style="color:#1b9e77">instantaneous direction and velocity</b> of the change in the function $f$ with respect to a small perturbation of, e.g., $\left(\delta_{x_1}, \delta_{y_1}\right)$.</li>
  </ul>
  <li>In the picture to the left, we see various tangent directions and velocities represented by the direction and the size of arrows.</li>
  <ul>
    <li>The arrows depend on the particular value in the $(x,y)$-plane below the surface, and the surface represents the values of 
    $$f(x, y) = -\left(\cos(x)^2 + \sin(y)^2\right)^2$$
    according to each value of $(x,y)$ below.</li>
  </ul>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>These particular tangent vectors give the direction and velocity where the change is most rapid along the surface above.</li>
  <li>This particular choice of tangent vector pictured above has a special name: the <b>gradient vector</b>.</li>
  <ul>
    <li>The <b>gradient vector</b> is <strong>the direction and velocity of the greatest rate of increase in the output of $f$ in a perturbation of $(\delta_{x_1},\delta_{y_1})$</strong> at a given point $(x_0, y_0)$.</li>
  </ul>
</ul>
</div>

========================================================
### The gradient

* Formally we will write the <b>gradient</b> as follows.

* Let $f$ be a real valued function of multiple arguments, i.e.,
  
  $$\begin{align}
  f:\mathbb{R}^n& \rightarrow \mathbb{R}\\
  \mathbf{x} & \rightarrow  f(\mathbf{x})
  \end{align}$$
  
* We define the Greek letter nabla,  $\nabla$, algebraically to be the gradient operation.
  
* When applied as follows,
  $$\nabla f$$
  we define
  $$\begin{align}
  \nabla f = \begin{pmatrix} \partial_{x_1}f  \\ \partial_{x_2} f  \\ \vdots \\ \partial_{x_n} f \end{pmatrix} \in \mathbb{R}^n
  \end{align}$$
  which is called the gradient of $f$.
  
* The partial derivatives above can be evaluated at a given point $\mathbf{x}_0\in\mathbb{R}^n$ and this will then give the direction and velocity of greatest accent in the value of $f$.

========================================================
### The gradient

* Recall our function `f`

```{r}
f
```

* <b>Q:</b> using the definition of the gradient on the last slide, what will this vector be?

  * <b>A:</b> this can be computed with the `D` function as,
  
```{r}
c(D(f,"x"), D(f, "y"))
```

* I.e., 
  $$\nabla f = \begin{pmatrix}\partial_x f \\ \partial_y f \end{pmatrix}.$$


========================================================
### The gradient

* We developed the first order approximation of the univariate function $f(x)$ as
  $$f(x_1) \approx f(x_0) + f'(x_0)\delta_{x_1}.$$

* We also use the gradient similar to the above as a linear approximation for the multivariate function $f(\mathbf{x})$.
  
* Let $\mathbf{x}_0\in \mathbb{R}^n$ be some vector and $\mathbf{x}_1 = \mathbf{x}_0 + \boldsymbol{\delta}_{x_1}$, where $\boldsymbol{\delta}_{x_1}$ is now a vector of small perturbations.

* At <b>first order, the Taylor series</b> is given as
  
  $$f(\mathbf{x}_1) = f(\mathbf{x}_0) + \left(\nabla f(\mathbf{x}_0)\right)^\mathrm{T} \boldsymbol{\delta}_{x_1} + \mathcal{o}\left(\parallel \boldsymbol{\delta}_{x_1}\parallel^2\right).$$
  
* The approximation above when $\boldsymbol{\delta}_x$ is a smaller perturbation of the vector $\mathbf{x}_0$, that is when the Euclidean norm $\parallel \boldsymbol{\delta}_{x_1}\parallel$ is small.


========================================================
### The Hessian

* A similar statement to the second order approximation we developed for univariate function $f(x)$,
  $$f(x_1) \approx f(x_0) + f'(x_0)\delta_{x_1}  + f''(x_0)\frac{\delta_{x_1}^2}{2},$$
  can be developed for the multivariate case.
  
* In order to do so, we will need to introduce the idea of the <b>Hessian as the second multivariate derivate of $f$</b>.

* We suppose again that $f$ is a scalar-valued function of multiple variables,
  $$\begin{align}
  f: \mathbb{R}^n & \rightarrow \mathbb{R} \\
  \mathbf{x} &\rightarrow f(\mathbf{x})
  \end{align}$$

* We will define the Hessian matrix for the function $f$ as $\mathbf{H}_f$ with,
  $$\begin{align}
  \mathbf{H}_{f} = 
  \begin{pmatrix}
  \partial_{x_1}^2 f & \partial_{x_1}\partial_{x_2} f & \cdots & \partial_{x_1}\partial_{x_n}f \\
  \partial_{x_2}\partial_{x_1} f & \partial_{x_2}^2 f & \cdots & \partial_{x_2} \partial_{x_n}f \\
  \vdots & \vdots & \ddots & \vdots \\
  \partial_{x_n}\partial_{x_1} f & \partial_{x_n}\partial_{x_2} & \cdots & \partial_{x_n}^2 f
  \end{pmatrix}
  \end{align}$$
  
* For short, this is often written as $\mathbf{H}_f = \left\{ \partial_{x_i}\partial_{x_j} f\right\}_{i,j=1}^n$ where this refers to the $i$-th row and $j$-th column.

* $\mathbf{H}_f$ can be evaluated at a particular point $\mathbf{x}_0$, and notice that $\mathbf{H}_f$ is always symmetric.

========================================================
### The Hessian

* As before, let $\mathbf{x}_1 = \mathbf{x}_0 + \boldsymbol{\delta}_{x_1}$ be given as a small perturbation of $\mathbf{x}_0$.

* Using the Hessian as defined on the last slide, if $f$ has continuous second order partial derivatives, the <b>Taylor series is given at second order</b> as
  $$\begin{align}
  f(\mathbf{x}_1) = f(\mathbf{x}_0) + \left(\nabla f(\mathbf{x}_0)\right)^\mathrm{T} \boldsymbol{\delta}_{x_1} + \boldsymbol{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\mathbf{x}_0) \boldsymbol{\delta}_{x_1} + \mathcal{o}\left(\parallel \boldsymbol{\delta}_{x_1}\parallel^3\right)
  \end{align}$$

* Similarly, our second order approximation is defined by

$$\begin{align}
  f(\mathbf{x}_1) \approx f(\mathbf{x}_0) + \left(\nabla f(\mathbf{x}_0)\right)^\mathrm{T} \boldsymbol{\delta}_{x_1}+ \boldsymbol{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\mathbf{x}_0) \boldsymbol{\delta}_{x_1}
  \end{align}$$
  which is accurate when the size of the perturbation $\parallel \boldsymbol{\delta}_{x_1}\parallel$ is small.

* We will consider how to use this approximation directly in our next lecture.

* For now, we will look an example of how we can compute this.

========================================================
### An example of computing the gradient and the Hessian


<div style="float:left; width:45%">
<img style="width:100%", src="Simple_paraboloid.png" alt="Graph of simple paraboloid."/>
<p style="text-align:center;">
Courtesy of <a href="https://commons.wikimedia.org/wiki/File:Simple_paraboloid.png">Babak. K. Shandiz</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons
</p>
</div>
<div style="float:left; width:55%">
<ul>
  <li>Consider the function for the simple paraboloid in two variables $f(x_1,x_2) = x_1^2 + x_2^2$. </li>
  <li>We will consider how to compute both the analytical gradient and Hessian, and how to perform this numerically.</li>
  <li><b>Q:</b> using the definitions, what are the gradient vector and Hessian matrix for $f$ defined above?
  <ul>
    <li><b>A:</b> we recall,
    $$\begin{align}
    \nabla f &=
    \begin{pmatrix}
    \partial_{x_1}f \\ \partial_{x_2}f 
    \end{pmatrix}
    = 
    \begin{pmatrix}
    2x_1 \\ 2x_2
    \end{pmatrix}
    \\
    \mathbf{H}_f& =
    \begin{pmatrix}
    \partial_{x_1}^2 f & \partial_{x_1}\partial_{x_2} f \\ \partial_{x_2}\partial_{x_1} f & \partial_{x_2}^2 f
    \end{pmatrix}
    =
    \begin{pmatrix}
    2 & 0 \\ 0 & 2
    \end{pmatrix}
    \end{align}$$
    </li>
  </ul>
  <li>Very similar to the notion of a critical point in one variable, we have that $\nabla f = \boldsymbol{0}$, the zero vector, at a critical point for $f$.</li>
  <li><b>Q:</b> in the graph, what is the critical point pictured?</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li><b>A:</b> the critical point is the unique minimum at $(0,0)$.</li>
  <li><b>Q:</b> what is special about the Hessian above?</li>
  <li><b>A:</b> we can see that this is in an eigenvalue decomposition and all eigenvalues are positive -- we will return to this next lecture.</li>
</ul>
</div>


========================================================
### An example of computing the gradient and the Hessian


* For most functions it is not a trivial step to compute the gradient and the Hessian and we might need to compute this numerically instead of analytically.

* The package `numDeriv` has tools for computing these by forms of the finite different approximations discussed earlier.

```{r}
require(numDeriv)
f <- function(x){
  return (sum(x^2))
}
```

* Now, we consider the evaluation of the gradient and the Hessian at the critical point to find:

```{r}
grad(f, x=c(0,0))
hessian(f, x=c(0,0))
```


========================================================
### The Jacobian

*  Not all functions give a single value as an output, for example we can instead write an expression as,
  
  $$\begin{align}
  \mathbf{f} : \mathbb{R}^n & \rightarrow \mathbb{R}^m\\
  \mathbf{x} &\rightarrow \begin{pmatrix} f_1\left(\mathbf{x}\right) \\ \vdots \\ f_m(\mathbf{x})\end{pmatrix}
  \end{align}$$
  
* In the above, each of the component-functions $f_i$ for $i=1,\cdots , m$ is a function like we saw before,
  
  $$\begin{align}
  f_i :\mathbb{R}^n &\rightarrow \mathbb{R} \\
  \mathbf{x} &\rightarrow f_i (\mathbf{x})
  \end{align}$$

* This could look like, for example, in dimensions $\mathbb{R}^2 \rightarrow \mathbb{R}^2$ 

 $$\begin{align}
 \mathbf{f}(x, y) = \begin{pmatrix} x^2  \\ y^2 \end{pmatrix}
 \end{align}$$
 where
 
 * $f_1(x,y) = x^2$, and 
 * $f_2(x,y) = y^2$.

========================================================
### The Jacobian

* For a vector valued function such as $\mathbf{f}$ we can define the action of the gradient once again, but to define a <b>matrix of first derivatives called the Jacobian</b>.

* This is given as,
  $$\begin{align}
  \nabla \mathbf{f} &=  
  \begin{pmatrix}
  \partial_{x_1} f_1 & \partial_{x_2} f_1 & \cdots & \partial_{x_n} f_1 \\
  \partial_{x_1} f_2 & \partial_{x_2} f_2 & \cdots & \partial_{x_n} f_2 \\
  \vdots &  \vdots & \ddots & \vdots \\
  \partial_{x_1} f_m & \partial_{x_2} f_m & \cdots & \partial_{x_n} f_m
  \end{pmatrix}\in\mathbb{R}^{m\times n}.
  \end{align}$$

* The Jacobian has a similar interpretation to the gradient, but the meaning is more subtle due to the multiple inputs and outputs of $\mathbf{f}$ simultaneously.

* However, we can easily restate the first order Taylor expansion as,

$$\begin{align}
\mathbf{f}\left(\mathbf{x}_1\right) = \mathbf{f}\left(\mathbf{x}_0\right) + \nabla \mathbf{f}\left(\mathbf{x}_0\right) \boldsymbol{\delta}_{x_1} + \mathcal{o}\left(\parallel \boldsymbol{\delta}_{x_1} \parallel^2\right)
\end{align}$$

* This again gives a linear approximation, but in higher dimensions in terms of a vector tangent direction $\nabla \mathbf{f}\left(\mathbf{x}_0\right) \boldsymbol{\delta}_{x_1}$ away from the value $\mathbf{f}(\mathbf{x}_0)$ approximating $\mathbf{f}\left(\mathbf{x}_1\right)$. 

========================================================
### Multivariate Newton

* The Newton-Raphson method can now be restated in terms of multiple variables as follows.

* Suppose that we have a nonlinear inverse problem stated as follows:

$$\begin{align}
\mathbf{f} :\mathbb{R}^n &\rightarrow \mathbb{R}^n  \\
          \mathbf{x} & \rightarrow \mathbf{f}(\mathbf{x}) \\
          \mathbf{f}\left(\mathbf{x}^\ast\right)& = \mathbf{b}
\end{align}$$

* We redefine this in terms of the adjusted function $\tilde{\mathbf{f}}\left(\mathbf{x}^\ast\right) = \mathbf{f}\left(\mathbf{x}^\ast\right) - \mathbf{b} = \boldsymbol{0}$, and we wish to make the same first order approximation as before.

* Supposing we have a good initial guess $\mathbf{x}_0$ for $\mathbf{x}^\ast$, we look for the point where the linear approximation equals zero, i.e.,
$$\begin{align}
 0 &= \mathbf{f}\left(\mathbf{x}_0\right) + \nabla \mathbf{f}\left(\mathbf{x}_0\right) \boldsymbol{\delta}_{x_1} \\
\Leftrightarrow \boldsymbol{\delta}_{x_1} &= -\left(\nabla \mathbf{f}\left(\mathbf{x}_0\right) \right)^{-1} \mathbf{f}\left(\mathbf{x}_0\right). 
\end{align}$$

* The above makes sense as an approximation as long as $\left(\nabla \mathbf{f}\left(\mathbf{x}_0\right) \right)^{-1}$ exists, i.e., as long as it has a non-zero determinant (no zero eigenvalues).

* We can thus once again update the approximation recursively so that $\mathbf{x}_1 = \mathbf{x}_0 - \mathbf{x}_0$, as long as the inverse exists.

* We will continue this idea in the next lecture, looking at how to minimize or maximize functions.