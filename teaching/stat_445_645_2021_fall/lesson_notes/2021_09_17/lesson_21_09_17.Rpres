<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>


A review of discrete probability and sets
========================================================
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

<ul>
  <li>The following topics will be covered in this lecture:</li>
  <ul>
    <li>A review of basic set theory</li>
    <li>A review of basic probability theory</li>
    <li>Some basic probabilistic experiments with finite sample spaces</li>
    <li>Sampling procedures</li>
  </ul>
</ul>


========================================================
## Basic set theory: a history of sets


<div style="float:left; width:40%">
<ul>
  <li>In the nineteenth century, the German mathematician Georg Cantor developed the greater part of today’s set theory.</li>
  <li>Set theory is at the basis of mathematical logic and how, similar to computing, mathematical objects are ordered in hierarchies of classes with certain properties.</li>
  <li>At the turn of the nineteenth and twentieth centuries, Ernst Zermelo, Bertrand Russell, Cesare Burali-Forti and others found contradictions in the originally proposed set theory, with one of the famous results being Russell's paradox.</li>
</ul>
</div>
<div style="float:right; width:50%" class="fragment">
<img src="./barber_1.png" style="width:100%" alt="Description of Russell's paradox.">
<p style="text-align:center"> Courtesy of Doxiadis, Apostolos, and Christos Papadimitriou. <i>Logicomix: An epic search for truth</i>. Bloomsbury Publishing USA, 2015.</p>
</div>





========================================================
### A history of sets
<ul>
  <li>Moreover, it is decreed that the barber is the one who shaves those that do not shave themselves.</li>
  <li>The question then is, who shaves the barber if the barber is the one who shaves those that do not shave themselves?</li>
  <li>In terms of sets, there was an early approach that defined a set as any identifiable collection; however, the question arose:</li>
  <ul>
    <li>if a set $A$ is identified as the collection of all collections that are not contained in themselves, does $A$ belong to itself or not?</li> 
  </ul>
  <li>Contradictions like the above led Ernst Zermelo in 1908 gave an axiomatic system which precisely described the existence of certain sets
and the formation of sets from other sets.</li>
  <li>This Zermelo–Fraenkel set theory is still the most common axiomatic system for set theory.</li>
  <li>There are 9 axioms, amongst
others, that deal with set equality, regularity, pairing sets, infinity, and power sets.</li>
  <li>We will be interested in this lecture how this will allow us to calculate probability for discrete distributions with finite possible outcomes as sets or collections.</li>
</ul>


  
========================================================
### Common sets in mathematics
  
<ul>
  <li>In mathematics, the most commonly considered sets are the following:</li>
  <ul>
    <li> $\mathbb{N}$: the set of natural numbers, i.e. $\{1, 2, 3, 4 \cdots \}$.
    <li> $\mathbb{Z}$: the set of integer numbers, i.e. $\{\cdots − 3, −2, −1, 0, 1, 2, 3 \cdots \}$.
    <li> $\mathbb{Q}$: the set of rational numbers, i.e., all numbers $q$ that can be represented as ratios of two numbers $q=\frac{z_1}{z_2}$ for $z_1,z_2\in \mathbb{Z}$.</li>
    <li>$\mathbb{R}$: the set of real numbers, i.e., all rationals and irrational numbers like $\sqrt{2}$.</li> 
    <li>$\mathbb{C}$: the set of complex numbers, i.e., all real numbers and all extensions of the real numbers to the complex plane.</li>
  </ul>
  <li>For each set there is a cardinal number which stands for the magnitude or number of
elements in the set, even for infinite sets.</li> 
  <ul>
    <li>The cardinal number of $\mathbb{N}$ is $\aleph_0$ (‘Aleph null’), which represents a "smaller" infininity than the infinity of $\mathbb{R}$,
    $${\mathfrak {c}}=2^{\aleph _{0}} &gt; \aleph_{0}.$$</li>
  </ul>
  <li>In R, of course, one cannot create infinite sets due to its limited storage capacity.</li>
  <li>The sets above are named by a fixed character or letter, whereas other sets in the literature are labelled arbitrarily with a Latin or Greek letter.</li>
</ul>


========================================================
### Basics of set theory in R

* Most objects we have worked with in R can be treated as mathematical sets.

* Consider the following vector:

```{r}
example_vector <- c(1, 2, 3, 4)
```

* Vectors have additional structure that sets don't impose -- i.e., there is an order to the elements of the `example_vector`.

* However, we can treat the `example_vector` as a set by using set operations on it.

* The statement $x\in A$ can be read as "x is in the collection named A". 
  
* <b>Q:</b> can you think of an R operation that would give a logical output if an element of some collection `B` was contained in the `example_vector`?

  * <b>A:</b> This can performed with our `%in%` operator, as in
  
```{r}
B <- c(2, 5, 6, 7, 1)
A <- example_vector
A %in% B
```
  

========================================================
### Basics of set theory in R

* With the notion of set containment, we can consider the definition of a set intersection:
  $$\begin{align}
  A \cap B = \{\text{All }x\text{ such that }x\in A\text{ and } x\in B\}
  \end{align}$$

* This operation can be replicated with the `%in%` operator, but there are also built-in set operations such as the intersection,

```{r}
A[ A %in% B]
intersect(A,B)
```

* There are several other relations besides the subset relation, including 

* the union, 
  $$\begin{align}
  A \cup B = \{\text{All }x\text{ such that }x\in A\text{ or } x\in B\}
  \end{align}$$

```{r}
union(A,B)
```


========================================================
### Basics of set theory in R

* the difference, 

```{r}
setdiff(A,B)
setdiff(B,A)
```

* and test for equality, 

```{r}
setequal(A,B)
```

* All these functions contained in base R can be applied to vectors or matrices. 


========================================================
## Probabilistic experiments with finite outcome spaces


<ul>
  <li>When working with data that is subject to random variation, the theory behind this probabilistic situation becomes important.</li>
  <li> There are two types of experiments: </li>
  <ol>
    <li>deterministic, in which all elements of the experiment are controlled precisely, and </li>
    <li>random, usually in the form of how the data is sampled or by the way error is introduced by uncertainty in measurements and experimental parameters.</li>
  </ol>
  <li> In the following we will review the main ideas about nondeterministic processes with a finite number of possible outcomes.</li>
  <li> A random trial (or experiment) yields one of the distinct outcomes that altogether form the sample or event space, which we will call $\Omega$.</li> 
  <li> All possible outcomes constitute the universal event $\Omega$, and for an experiment with finite possible outcomes like a single coin flip, we can actually write out $\Omega$ as a finite collection or set.</li>
  <li> Subsets of $\Omega$ are called events,</li>
  <ul>
    <li>e.g. $\Omega$ itself is the universal event.</li>
  </ul>
  <li><b>Q:</b> suppose our experiment is to make a single, fair coin flip where a "heads" is represented by $H$ and a tails is represented by $T$.  Can you identify the collection of all possible outcomes $\Omega$?</li>
  <ul>
    <li><b>A:</b> in this case $\Omega = \{H,T\}$.</li>
  </ul>
</ul>


========================================================
### Probabilistic experiments with finite outcome spaces

* A combination of several rolls of a die or tosses of a coin leads to more possible results, such as tossing a coin twice, with the sample space  
$$\Omega = \{\{H, H \}, \{H, T \}, \{T, H \}, \{T, T \}\}.$$ 

* Generally, the combination of several different experiments yields a sample space with all possible combinations of the single events. 

* If, for instance, one needs two coins to fall on the same side, then the favored event is a set of two elements: $\{H, H \}$ and $\{T, T \}$.

* The `prob` package, developed by G. Jay Kerns, is specifically for probabilistic experiments like the above. 

* It provides methods for elementary probability calculation on finite sample spaces, including counting tools, defining probability spaces discussed later, performing set algebra, and calculating probabilities.


========================================================
### Probabilistic experiments with finite outcome spaces

* In the below, we will run the two fair coin flip experiment and provide a summary of the probability distribution of the events in $\Omega$:

```{r}
require(prob)
ev = tosscoin(2)
probspace(ev)
```

* The argument of `tosscoin()` refers to how many coin tosses we perform
```{r}
probspace(tosscoin(3))
```

========================================================
### Probabilistic experiments with finite outcome spaces

<ul>
  <li>In the simple probability model, we follow the logic that is used in games of chance like the above:</li>
  <ul>
    <li>We will assume that the experiment's outcome can be represented by an element of $\omega \in \Omega$, and range of possible outcomes can be represented by combinations of these elements.</li>
    <li>We will assume that all possible outcomes of the experiment are equally likely.</li>
    <li>We will assume that we can list out all possible outcomes into a finite collection.</li>
    <li>Therefore, the probability of attaining some outcome in a collection of outcomes $A \subset \Omega$ can be written as,<br><br>
    $$P(A) = P(\omega \in A) = \frac{\text{All possible ways that the experiment's outcome }\omega\in A}{\text{All possible ways for the experiment to have an outcome }\omega\in \Omega}$$</li>
  </ul>
  <li><b>Q:</b> given the above "simple probability model", what possible values can $P(A)$ take and what is the meaning of the maximum and the minimum value?</li>
  <ul>
    <li><b>A:</b> if there are no possible ways to attain some outcome in $A$, the smallest numerator is zero.</li>
    <li>If the number of ways for an outcome to occur in $A$ equals the total number of ways the experiment can end, the $P(A)=1$.</li>
    <li>If we have $P(A)=0$ then we say we are certain that $A$ is impossible while $P(A)=1$ means we are certain it will occur.</li>
  </ul>
</ul>

========================================================
### Basic probability 

<div style="float:left; width:40%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>We will recall some basic properties of probability, as they relate to sets.</li>
  <li>These rules can be easily understood graphically in terms of the venn diagram.</li>
  <ul>
    <li>$P(\Omega)=1$ and $P(\emptyset)=0$;</li>
    <li>the probability of a set union is given as
    $$P(A\cup B) = P(A) + P(B) - P(A\cap B);$$</li>
    <li>consequentially, if $A$ and $B$ are disjoint,
    $$P(A\cup B) = P(A) + P(B);$$</li>
    <li>The conditional probability of $A$ given $B$ intuitively restricts the outcome space $\Omega$ to $\Omega \cap B =  B$ and therefore,
    $$P(A\vert B) = \frac{P(A \cap B)}{P(B)}.$$</li>
  </ul>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>Note, the above only makes sense when $P(B)\neq 0$ and we should only consider the conditional probability when conditioning on <strong>possible events</strong>.</li>
  <li>Consequently, when $B$ is possible, we recover the notion,
  $$P(A \cap B) = P(A\vert B) P( B).$$</li>
</ul>
</div>
 
========================================================
### Basic probability 

<ul>
  <li>Let's assume that both $A$ and $B$ are possible.</li>
  <ul>
    <li>Consider applying the relationship for $P(A \cap B)$ symmetrically in $A$ and $B$:
     $$\begin{align}
     P(A \cap B) &= P(A\vert B) P( B);\\
     P(B\cap A) &= PA(B \vert A) P(A); \\
     P(A \cap B)& = P(B\cap A);
     \end{align}$$
     </li>
    <li>applying these together we get Bayes' law:
    $$P(A \vert B) = \frac{P(B \vert A) P(A)}{P(B)}.$$</li>
  </ul>
  <li> Define $A^\mathrm{c}$ to be the complement of an event, i.e.,
      $$A^\mathrm{c} = \{x \in \Omega \vert x \notin A\},$$</li>
  <li>then the probability of unions also shows us that
$$\begin{align}
& P\left(A \cup A^\mathrm{c}\right) = P(\Omega) \\
\Leftrightarrow & P(A) + P\left(A^\mathrm{c}\right) - P\left(A \cap A^\mathrm{c}\right) = 1 \\
\Leftrightarrow & P(A) + P\left(A^\mathrm{c}\right) - P(\emptyset) = 1 \\
\Leftrightarrow &P(A) + P\left(A^\mathrm{c}\right) =1
\end{align}$$
</li>
<li>Therefore it is equivalent to compute the probability of $A$ as 
$$P(A) = 1 - P\left(A^\mathrm{c}\right).$$</li>
</ul>

========================================================
### Simple probability experiments in R

* The following are common experiments that can be run in the `prob` package for R:

```
 urnsamples(x, size, replace = FALSE, ordered = FALSE, ...),
 tosscoin(ncoins, makespace = FALSE),
 rolldie(ndies, nsides = 6, makespace = FALSE),
 cards(jokers = FALSE, makespace = FALSE),
 roulette(european = FALSE, makespace = FALSE).
```

* If the argument makespace is set TRUE, the resulting data frame has an additional column showing the (equal) probability of each single event. 

* In the simple probablity model, the probability of an event can be computed as the relative frequency of possible outcomes as discussed earlier.

* However, a general, finite probability space can be formed using
```
probspace(outcomes, probs)
```
 with a vector of possible simple outcomes and a vector of the associated probabilities for each simple outcome.
 
* The full probability space will then be created as all set combinations of the simple outcomes, with their probabilities constructed using the relationships described earlier.

========================================================
## Simple probability experiments in R

* For the remaining lecture, we will be focusing on the classical urn sampling problem, and how we can generate different kinds of random sampling.

* The basic idea behind an "urn" problem is that we have an urn or a bin in which different objects are stored.

* These objects have no order and are assumed to be well-mixed;

  * if the proportion of each type of object is even, the events can be described by a set and the simple probability model in terms of any type of object.

* We will take two turns drawing one toy vehicle at a time from the urn, without replacement:

```{r}
ev <- urnsamples(c("bus", "car", "bike", "train"), replace=FALSE, size= 2, ordered=TRUE)
probspace(ev)
```

========================================================
### Simple probability experiments in R

* In the last example we specified that we do not return an object once it is drawn, that the order in which the objects are drawn matters and that there is only one copy of each object in the urn.

* When we sample without replacement, and with order mattering, the number of all possible samples of size $k$ from a set of $n$ objects is given by,

$$\begin{align}
\frac{n!}{(n-k)!} & & n! = n (n-1) (n-2) \cdots (2)(1) 
\end{align}$$


* Suppose we want to find the probability that the second object drawn is a bike,

  * we can pair "bike" with the other objects as the first draw in three different ways.

* In total there are 
  $$\frac{4!}{(4-2)!} = \frac{4!}{2!} = 3\times 4 = 12$$
  total possible outcomes so that

```{r}
Prob(probspace(ev), X2 == "bike" )
```


========================================================
### Simple probability experiments in R

* If on the other hand we do replace objects in the urn before we draw new ones notice the difference in the outcome space

```{r}
ev <- urnsamples(c("bus", "car", "bike", "train"), replace=TRUE, size= 2, ordered=TRUE)
probspace(ev)
```

* If we replace an object after we draw it, we can calculate the total number of possible outcomes as a choice among $n$ objects $k$ times with $n^k$ total outcomes.


========================================================
### Simple probability experiments in R

* In this case,
```{r}
Prob(probspace(ev), X2 == "bike" )
```

* because we now have one more possible way to draw `bike` on the second draw.


========================================================
### Simple probability experiments in R

* Instead of replacing objects, we can also consider the experiment where we only care which two objects are retrieved, but we do not care about the order in which they are drawn.

```{r}
ev <- urnsamples(c("bus", "car", "bike", "train"), replace=FALSE, size= 2, ordered=FALSE)
probspace(ev)
```

* In this case, the probability space has only six possible outcomes, given by all un-ordered combinations of the different objects in the urn.

* Mathematically, this is described by the "choose" function for $n$ objects choosing combinations of size $k$
$$
{n \choose k} = \frac{n!}{k!(n-k)!}.
$$

* In our example, we have
$$
{4 \choose 2} = \frac{4!}{2!(4-2)!} = \frac{4!}{2! 2!} = 3 \times 2
$$

========================================================
## Sampling procedures


* The examples above are very specific and restricted to a particular sample space, but we can address sampling from a general perspective with the same principles.

* In each case, some random selection mechanism is involved: the theory behind this is called probabilistic sampling. 

* An important example of randomized sampling is the following:

* In 1954, a large-scale experiment was designed to test the effectiveness of the Salk
vaccine in preventing polio.
  * <b>Treatment group</b> -- 200,745 children were given a treatment consisting of Salk vaccine injections.
  * <b>Control group</b> -- 201,229 children were injected with a placebo that contained no drug.

* The 401,974 children in the Salk vaccine experiment were assigned to the Salk vaccine treatment group or the placebo group via a process of random selection equivalent to flipping a coin. 

* You can encode wheter someone is in the treatment or control group in a <strong>binary way</strong>:

  * <b>Treatment / 1 / H</b>
  * <b>Control / 0 / T</b>  

* Randomly drawing "Treatment" or "Control" with equal probability is equivalent to flipping a fair coin.

* The logic behind randomization is to use chance as a way to create two groups that are similar.

* With a large enough sample for both treatment and control groups, this can be very effective when it is difficult to balance factors like age, gender, height, weight, etc... across groups that might affect the outcome.


========================================================
## Sampling procedures


* In randomized sampling, chance is being utilized to balance the many population factors across the <strong>control</strong> and <strong>treatment</strong> groups.

  * This makes it so that each <b style="color:#d95f02">sub-sample</b> better reflects the full <b style="color:#1b9e77">population</b>.

* The most basic way to perform such a randomized sample is like the coin flip or urn example, where all outcomes have equal probability.

* <b>Simple random sample:</b> a simple random sample of $n$ subjects is selected in such a
way that every possible sample of the same size $n$ has the same chance of being
chosen. 

  * Suppose we are taking a poll of UNR students and we will have a sample size of 1000. 
  
  * This means, every possible combination 1000 UNR students is <strong>must be equally likely to be selected</strong> based on our sampling method.
  
  * E.g., we can  randomly select students to give interview responses based on their student ID numbers.

  * <b>Note:</b> a simple random sample is often called a random sample, but strictly speaking, a random sample has the weaker requirement that all members of the population have the same chance of being selected.
  

========================================================
### Stratified sampling

* The purpose of this randomized sampling is to produce a sample that gives an accurate representation of the population being studied with a limited number of observations.

* However, especially with small sample sizes, it is possible that a random sample can be biased and not adequately represent the true population.

  * For example, if a small number of interviews are to be conducted with UNR students, randomly selected based on their student number, there may be a disproportionate number of undergraduates versus graduate students interviewed.

* One approach to handle this is known as stratified sampling.

  * <b>Stratified sampling:</b> we subdivide the population into at least two different <strong>subgroups (or strata)</strong>.
  
* Groups are chosen such that <strong>subjects within the same subgroup share the same characteristics</strong> (such as undergraduate versus graduate student) and each population member belongs to one and only one subgroup.

* We then draw a simple random sample from each subgroup (or stratum) with a total number of observations taken from each subgroup proportional to the group's membership in the population.


========================================================
### Stratified sampling

*  In the following example, it is assumed that a university student body consists of exactly 1000 students, with $80\%$ being undergraduate and $20\%$ being graduate.

```{r}
require(sampling)
set.seed(0)
student_df <- data.frame(graduate_status =  c(rep("U", 800), rep("G", 200)), student_number = sample(1:1000, size=1000))
head(student_df)
tail(student_df)
```


========================================================
### Stratified sampling

* Now, let's randomly select 10 students based on their student number for interviews,

```{r}
selected_numbers <- sample(1:1000, size=10)
student_df[student_df$student_number %in% selected_numbers,]
```

* In this particular random outcome, we have only selected a single graduate student and we might not get all the perspectives we want.

========================================================
### Stratified sampling

* If we take a larger sample size, the proportion of undergraduate students and graduate students in the sample will become closer to the proportion in the true population in general
```{r}
selected_numbers <- sample(1:1000, size=100)
selected_students <- student_df[student_df$student_number %in% selected_numbers,]
cat(sum(selected_students$graduate_status=="U"), "total undergraduates selected\n")
cat(sum(selected_students$graduate_status=="G"), "total graduates selected\n")
```

========================================================
### Stratified sampling

* However, if we have reason to believe that it is important to keep this proportion of the interviews exact according to the population, we can make a stratified sampling as

```{r}
require(sampling)
st <- strata(student_df, stratanames="graduate_status", size=c(16,4), method="srswor")
statified_sample = getdata(student_df, m=st)
statified_sample
```


========================================================
### Sampling

* A variety of different sampling procedures can be introduced to better balance a sample with respect to how it will reflect a population.

  * We will not cover sampling in general, but we mention it here to demonstrate how randomness is often introduced into an experiment, even if there isn't randomness in the experimental process itself.
  
* Because of the notion of randomness in our observations, probability is essential for modeling the results we see in observed data sets.

* Particularly, in the next session we will review the notion of a random variable and some fundamental probability distributions.
