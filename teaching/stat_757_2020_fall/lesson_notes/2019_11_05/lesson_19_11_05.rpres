<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

Generalized least squares
========================================================
date: 11/05/2019
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style='color:black'>Instructions:</h2>
<p style='color:black'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

========================================================

## Correlated errors

* When there is non-zero correlation of the error (or non-constant variance), in certain cases we can successfully reduce this back to our normal regression framework with generalized least squares.

* Particularly, let's assume that the error takes a particular (but more general) form,

  $$\begin{align}
  cov\left(\boldsymbol{\epsilon}\right) = \sigma^2 \boldsymbol{\Sigma}
  \end{align}$$
  
  for a <em>known structure matrix</em> for the error $\boldsymbol{\Sigma}$ but perhaps an <em>unkown scale coefficient</em> $\sigma^2$.  
  
* This describes a sittuation in which we have knowledge of the correlation and relative variance between the errors, but we don't know the absolute scale of the variation.

* Let us recall some properties of postive-semidefinite, symmetric matrices:

  <ol>
    <li> Positive-semidefinite matrices (by definition) have all non-negative eigenvalues.</li>
    <li> Symmetric matrices (by the spectral theorem) must have a decomposition into an orthogonal eigenbasis, one for which each basis vector is a principle axis corresponding to one of the eigenvalue scales of growth or decay.</li>
  </ol>
  
* All covariance matrices satisfy the above two properties by their construction. Therefore, it makes sense to say that $\boldsymbol{\Sigma}$ has a well defined square root, 

  $$\begin{align}
  \sqrt{\boldsymbol{\Sigma}} \triangleq \mathbf{S}
  \end{align}$$

========================================================

<h3>Cholesky decomposition</h3>

* A simple choice of a square root is the <b>symmetric</b> square root,

  $$\begin{align}
  \mathbf{S} &\triangleq \mathbf{E} \mathrm{diag}\left(\sqrt{\lambda_i}\right) \mathbf{E}^\mathrm{T} \\
  &= \mathbf{E} 
  \begin{pmatrix}
  \sqrt{\lambda_1} & 0 & 0  &\cdots & 0 \\
  0 & \sqrt{\lambda_2} &  0 &\cdots & 0 \\
  0 & 0 &  \sqrt{\lambda_3} & \cdots & 0 \\
  \vdots & \vdots & \ddots & \ddots & \vdots \\
  0 &  \cdots & \cdots & \cdots & \sqrt{\lambda_n}
  \end{pmatrix} \mathbf{E}^\mathrm{T}
  \end{align}$$

  where $\lambda_i$ are the non-negative eigenvalues, and $\mathbf{E}$ are the associated eigenvectors, of $\boldsymbol{\Sigma}$.
  
* By the orthogonality of the eigenvectors, it is easy to verify that $\mathbf{S}^2 = \boldsymbol{\Sigma}$.

* However, for matrices, we can define the square root in several different ways with different properties.  A special alternative type of square root is given by a Cholesky factorization...

========================================================

<h3> Cholesky decomposition continued...</h3>

* When $\boldsymbol{\Sigma}$ is positive-definite (all strictly positive $\lambda_i$) there is a <b>unique</b> decompostition of $\mathbf{\Sigma}$ as,

  $$\begin{align}
  \boldsymbol{\Sigma} \triangleq \mathbf{L} \mathbf{L}^\mathrm{T}
  \end{align}$$
  
  such that $\mathbf{L}$ is lower triangular, with positive entries on the diagonal.
  
* That is, the matrix $\mathbf{L}$ is of the form,

  $$\begin{align}
  \mathbf{L} =  
  \begin{pmatrix}
  L_{11} & 0 & 0  &\cdots & 0 \\
  L_{21} & L_{22} &  0 &\cdots & 0 \\
  L_{31} & L_{31} &  L_{33} & \cdots & 0 \\
  \vdots & \vdots & \ddots & \ddots & \vdots \\
  L_{n1} &  \cdots & \cdots & \cdots & L_{nn}
  \end{pmatrix}
  \end{align}$$
  
* When we allow $\lambda_i=0$ for some $i$, the decomposition <b>will not be unique</b> but will still exist.
  
* This is a weak generalization of the idea of a "square-root" where now we ask that $\mathbf{S} \mathbf{S}^\mathrm{T} = \boldsymbol{\Sigma}$.

* Cholesky decompositions are extremely useful in many areas of mathematics, and especially for our case where the covariance matrix will be strictly positive-definite.


========================================================

### Correlated errors continued

* Returning to the regression analysis, we suppose that

  $$\begin{align}
  cov\left(\boldsymbol{\epsilon}\right) = \sigma^2 \boldsymbol{\Sigma}
  \end{align}$$
  
*  We will write $\boldsymbol{\Sigma} = \mathbf{S} \mathbf{S}^\mathrm{T}$ in a Cholesky decomposition.

* Then, considering our form for the signal

  $$\begin{align}
   \mathbf{Y}& = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}&\\
  \Leftrightarrow  \mathbf{S}^{-1} \mathbf{Y}& =  \mathbf{S}^{-1}\mathbf{X} \boldsymbol{\beta} + \mathbf{S}^{-1}\boldsymbol{\epsilon}\\
  \Leftrightarrow \mathbf{Y}' &=  \mathbf{X}' \boldsymbol{\beta} + \boldsymbol{\epsilon}'
  \end{align}$$
  
  for $\mathbf{Y}',\mathbf{X}', \boldsymbol{\epsilon}'$ as defined by the relationship above.
  
* <b> Exercise (2 minutes):</b> compute the mean of $\boldsymbol{\epsilon}'$.

* <b> Solution:</b> this can be performed as usual, distributing the expectation through the non-random component,

  $$\begin{align}
  \mathbb{E}\left[ \boldsymbol{\epsilon}' \right] &= \mathbb{E} \left[ \mathbf{S}^{-1}\boldsymbol{\epsilon}\right] \\
  &= \mathbf{S}^{-1} \mathbb{E}\left[ \boldsymbol{\epsilon}\right] \\
  &= 0
  \end{align}$$
  
========================================================

### Correlated errors continued

* <b>Exercise (2 minutes):</b> assuming that $cov(\boldsymbol{\epsilon}) = \sigma^2 \boldsymbol{\Sigma}$, compute the covariance of $\boldsymbol{\epsilon}'\triangleq \mathbf{S}^{-1}\boldsymbol{\epsilon}$.

* <b>Answer:</b> noting that $\boldsymbol{\epsilon}'$ is mean zero, then we can compute,

  $$\begin{align}
  cov\left(\boldsymbol{\epsilon}\right) &= \mathbb{E}\left[ \left(\boldsymbol{\epsilon}'\right) \left(\boldsymbol{\epsilon}'\right)^\mathrm{T}\right] \\
  &=\mathbb{E}\left[\left(\mathbf{S}^{-1}\boldsymbol{\epsilon}\right)\left(\mathbf{S}^{-1}\boldsymbol{\epsilon}\right)^\mathrm{T}\right]\\
  &=\mathbf{S}^{-1}\mathbb{E}\left[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^\mathrm{T}\right] \mathbf{S}^{-\mathrm{T}}\\
  &= \mathbf{S}^{-1} \sigma^2 \boldsymbol{\Sigma} \mathbf{S}^{-\mathrm{T}} \\
  &= \sigma^2\mathbf{S}^{-1}\left(\mathbf{S} \mathbf{S}^\mathrm{T} \right) \mathbf{S}^{-\mathrm{T}} \\
  &= \sigma^2 \mathbf{I} 
  \end{align}$$
  
* <b>Q:</b> How might we use this analysis to transform the variables into something easier to work with?  In particular, what variables might satisfy the assumptions of ordinary least squares?

* <b>A:</b> Supposing that we can compute the variables,

  $$\begin{align}
  \mathbf{X}' &\triangleq \mathbf{S}^{-1} \mathbf{X} \\
  \mathbf{Y}' &\triangleq \mathbf{S}^{-1} \mathbf{Y}
  \end{align}$$
  
  the regression of $\mathbf{Y}'$ in terms of $\mathbf{X}'$ satisfies our usual Gauss-Markov assumptions, for error $\boldsymbol{\epsilon}'$ such that $cov\left(\boldsymbol{\epsilon}\right) = \sigma^2 \mathbf{I}$.
  
========================================================

## Generalized least squares

* Because the assumptions of the Gauss-Markov theorem hold for the transformed variables, we can re-derive the least-squares solution minimizing the associated residual sum of squares error.

* Specifically, we can write,

  $$\begin{align}
  \hat{\boldsymbol{\epsilon}}' &= \mathbf{Y}' - \hat{\mathbf{Y}}'\\
  & =\mathbf{S}^{-1}\left(\mathbf{Y} - \mathbf{X} \hat{\boldsymbol{\beta}}'\right)
  \end{align}$$
  
  for some choice of $\hat{\boldsymbol{\beta}}'$.

* This choice of $\hat{\boldsymbol{\beta}}'$ should be the one that minimizes the equation,

  $$\begin{align}
  \left(\hat{\boldsymbol{\epsilon}}'\right)^\mathrm{T}\left(\hat{\boldsymbol{\epsilon}}'\right) & = \left(\mathbf{Y} - \mathbf{X} \boldsymbol{\beta}\right)^\mathrm{T}\mathbf{S}^{-\mathrm{T}}\mathbf{S}^{-1}\left(\mathbf{Y} - \mathbf{X} \boldsymbol{\beta}\right)\\
  &= \left(\mathbf{Y} - \mathbf{X} \boldsymbol{\beta}\right)^\mathrm{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{Y} - \mathbf{X} \boldsymbol{\beta}\right)
  \end{align}$$
  
  for all choices of $\boldsymbol{\beta}$.

========================================================

### Generalized least squares

* By substitution for our usual solution for the minimizer, with $\mathbf{X}'$ and $\mathbf{Y}'$, we find

  $$\begin{align}
  \hat{\boldsymbol{\beta}}' &\triangleq  \left( \left(\mathbf{X}'\right)^\mathrm{T} \left(\mathbf{X}'\right)\right)^{-1}\left(\mathbf{X}'\right)^\mathrm{T} \mathbf{Y}'\\
  &=\left(\mathbf{X}^\mathrm{T} \mathbf{S}^{-\mathrm{T}} \mathbf{S}^{-1} \mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T} \mathbf{S}^{-\mathrm{T}} \mathbf{S}^{-1}\mathbf{Y} \\
  & =\left(\mathbf{X}^\mathrm{T} \boldsymbol{\Sigma}^{-1} \mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T} \boldsymbol{\Sigma}^{-1}\mathbf{Y}.
  \end{align}$$
   
   

* <b>Q:</b> recalling that in ordinary least squares, $cov\left(\hat{\boldsymbol{\beta}}\right)= \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{1}\sigma^2$ is given  how can we compute $cov\left(\hat{\boldsymbol{\beta}}'\right)$

* <b>Answer:</b> knowing that the regression of $\mathbf{Y}'$ in terms of $\mathbf{X}'$ satisfies the hypotheses of ordinary least squares, the simple method is substitution.  I.e.,

  $$\begin{align}
  cov\left(\hat{\boldsymbol{\beta}}'\right) &= \left(\left(\mathbf{X}'\right)^\mathrm{T} \left(\mathbf{X}'\right)\right)^{-1} \sigma^2\\
  &=\left(\mathbf{X} \mathbf{S}^{-\mathrm{T}} \mathbf{S}^{-1} \mathbf{X}\right)^{-1} \sigma^2\\
  &= \left(\mathbf{X}^\mathrm{T} \boldsymbol{\Sigma}^{-1} \mathbf{X} \right)^{-1} \sigma^2
  \end{align}$$

========================================================

### Generalized least squares

*  Because the error $\boldsymbol{\epsilon}' = \mathbf{S}^{-1} \boldsymbol{\epsilon}$,
  $$\begin{align}
  cov\left(\boldsymbol{\epsilon}'\right) = \sigma^2 \mathbf{I}
  \end{align}$$
the transformation allows us to perform the usual usual diagnostics.

* Specifically, the residuals are likewise transformed along with the other variables to give,

  $$\begin{align}
  \hat{\boldsymbol{\epsilon}}'&\triangleq \mathbf{Y}' - \hat{\mathbf{Y}}' \\
  &=\mathbf{S}^{-1} \hat{\boldsymbol{\epsilon}}
  \end{align}$$
  
  where the residuals $\hat{\boldsymbol{\epsilon}}'$ will have the same properties as usual when $\boldsymbol{\Sigma}$ is chosen correctly.
  
  * Therefore, our normal diagnostics applied to $\hat{\boldsymbol{\epsilon}}'$ will remain the same.
  
* However, the issue is typically that $\boldsymbol{\Sigma}$ is unkown in practice, except in special cases, and typically must be estimated.

<ul>
  <li>As noted before, the correlation structure will have much to do with the problem at hand, e.g.,</li>
  <ol>
    <li> spatial correlations in spatial data;</li>
    <li> auto-correlations in time series data;</li>
    <li> batch correlations in batch collected data.</li>
  </ol>
</ul>
  
========================================================

<h2> Weighted least squares</h2>

* As a special case of generalized least squares, we can consider the case when the errors are uncorrelated, but have unequal variances.

* That is to say, $cov\left(\boldsymbol{\epsilon}\right) = \boldsymbol{\Sigma}$ where,

  $$\boldsymbol{\Sigma} = 
  \begin{pmatrix}
    1/w_1& 0 & \cdots & 0 \\    
    0 & 1/w_2 & \cdots & \vdots \\
    \vdots & \ddots & \ddots & \vdots \\
    0 & 0 & \cdots & 1/w_n
  \end{pmatrix}$$
  for possibly $w_i \neq w_j$, but $\boldsymbol{\Sigma}$ is strictly diagonal.
  
* Here, we will refer to the reciprocals of the individual variances, $w_i$, as the <b>weights</b>.

========================================================

### Weighted least squares

* <b>Exercise (2 minutes):</b> Discuss with a partner, in the case   
  $$\boldsymbol{\Sigma} = 
  \begin{pmatrix}
    1/w_1& 0 & \cdots & 0 \\    
    0 & 1/w_2 & \cdots & \vdots \\
    \vdots & \ddots & \ddots & \vdots \\
    0 & 0 & \cdots & 1/w_n
  \end{pmatrix}$$
what is the square root of $\boldsymbol{\Sigma}$?

* <b>A:</b> in this case, we can compute the square root identically by the component-wise square root, as diagonal matrices are a special class of matrices that are already in eigen-decomposition,

  $$\mathbf{S} = 
  \begin{pmatrix}
    1/\sqrt{w_1}& 0 & \cdots & 0 \\    
    0 & 1/\sqrt{w_2} & \cdots & \vdots \\
    \vdots & \ddots & \ddots & \vdots \\
    0 & 0 & \cdots & 1/\sqrt{w_n}
  \end{pmatrix}$$

========================================================

### Weighted least squares



* As a special case of generalized least squares, we can compute the transformation into the variables $\mathbf{X}', \mathbf{Y}', \boldsymbol{\epsilon}'$ as

  $$\begin{align}
  \mathbf{X}' &\triangleq \mathbf{S}^{-1} \mathbf{X} = \mathrm{diag}\left(\sqrt{w_i} \right)\mathbf{X}, \\
  \mathbf{Y}' & \triangleq \mathbf{S}^{-1}  \mathbf{Y} = \mathrm{diag}\left(\sqrt{w_i} \right) \mathbf{Y}, \\
  \boldsymbol{\epsilon}' & \triangleq \mathbf{S}^{-1} \boldsymbol{\epsilon} = \mathrm{diag}\left(\sqrt{w_i} \right)\boldsymbol{\epsilon};
  \end{align}$$
  
  * By direct matrix computation, we see that the <b>$i$-th row</b> of the matrix $\mathbf{X}$ is scaled by the inverse of the standard deviation of the $i$-th observation.

* We note, the column corresponding to the intercept term in the matrix $\mathbf{X}$ will be a vector of the form,
  
  $$\begin{align}
  \begin{pmatrix} \sqrt{w_1}, &\cdots, & \sqrt{w_n}\end{pmatrix}^\mathrm{T},
  \end{align}$$
  instead of the usual vector of ones.

========================================================

### Weighted least squares

* Qualitatively, we can note that 
<ol>
  <li> the observations of high variance 
  
  $$\begin{align}
  \frac{1}{\sqrt{w_i}} \gg 1 
  \end{align}$$
  
  will recieve low weight in the regression $\sqrt{w_i} \ll 1$, while</li> 
  
  <li> the observations of low variance 
  
  $$\begin{align}
  \frac{1}{\sqrt{w_i}} \ll 1 
  \end{align}$$
  
  will recieve high weight in the regression $\sqrt{w_i} \gg 1$</li>
  
</ol>

========================================================

### Weighted least squares -- heuristics


<ul>
  <li>As some general considerations with weighted least squares, we have the following heuristic rules:</li>
  <ol>
    <li>When the errors are propotional to the predictor $X_i$, i.e., $var(\epsilon_i) \propto X_i$ suggests the weights $w_i = X_i^{-1}$.  This might be suggested by, e.g., seeing a positive relationship in $\vert \hat{\boldsymbol{\epsilon}}_i \vert$
    versus the predictor $x_i$.</li>
    <li>When the response $Y_i$ is the average of $n_i$ observations, then $var(Y_i) =var(\epsilon_i) = \frac{\sigma^2_i}{n_i}$; this suggests using $w_i = n_i$ for each case.</li>
    <ul>
      <li>Responses that are averages arise quite commonly, but
take care that the variance in the response really is proportional to the group size.</li>
     <li>For example, consider the life expectancies for different countries.</li>
     <ul>
       <li>At first glance,
one might consider setting the weights equal to the populations of the countries,
but notice that there are many other sources of variation in life expectancy that
would dwarf the population size effect.</li>
    </ul>
    </ul>
    <li> When the observed responses are known to be of varying quality, weights may be
assigned $w_i = \frac{1}{sd\left(Y_i\right)}$.</li>
  </ol>
  <li> We will examine how the re-weighting of the least-squares fit affects the outcome in an example...</li>
</ul>



========================================================

<h2>An example of weighted least squares</h2>

<div style="float:left; height:800px; width:30%">

<img src="Monet-montorgueil.JPG" alt="Claude Monet, painting of Rue Montorgueil, Paris, Festival of June 30, 1878" />
<br>
Claude Monet, <em>Rue Montorgueil, <br>Paris, Festival of June 30, 1878</em>. <br>
Public Domain.
</div>

<div style="width:70%; float:left">
<ul>
  <li> France follows run-off elections in which the leading two candidates in the first election will be voted on once again to decide a final outcome.</li>

  <li> The intention of this election style is to allow voters to pick a "second-choice" candidate, if their first choice wasn't successful in the first round.</li>
  
 <li> In 1981 there were 10 candidates in the first round which was narrowed down to François Mitterand and Valéry Giscard-d’Estaing, who held the two highest vote totals in the first round.
 <div style="width:100%">
 <div style="float:left; width:50%">
 <img src="Mitterrand.jpg", style="width:35%" alt="Image of François Mitterand" ><br>
 Courtesy of Philippe Roos <a href="https://creativecommons.org/licenses/by-sa/2.0">CC 2.0</a><br><br>
 </div>
 <div style="float:left; width:50%">
 <img src="Giscard.jpg", style="width:35%" alt="Image of Valéry Giscard-d’Estaing" ><br>
 Courtesy of White House Staff Photographers. Public domain.<br><br>
 </div>
 </div>
 </li>
 
 <li> This kind of voting is the subject to poltical coalition building in which candidates who are elminated in the first round can encourage their supporters to vote for one of the final round candidates.</li>

<li> However, the voting is done privately and anonymously, and voters don't have to follow the advice of their first round pick.</li>
</ul>
</div>  
  
========================================================

### An example of weighted least squares

* We will try to infer, based on published vote totals, how votes were transferred between candidates. 

* In the the dataset "fpe" we find the published vote totals in every fourth department for France.

```{r}
library("faraway")
head(fpe)
```

* The variables "A" and "B" correspond to the vote totals for Mitterand and Giscard in the first round, respectively. 
  
  * The variables "A2" and "B2" correspond to their second round vote totals.
  
* The variable "EI" is the number of registered votes, and all numbers are measured in thousands.

* The difference between the number of voters in the second round and the first is given by the variable "N".

  * In this case, we will treat this as any other explanatory variable, though it could be treated differently in principle.
  
========================================================

### An example of weighted least squares


* The varible that we will model for the response is "A2", the number of second round votes given to Mitterand.

* Suppose we can write the signal an <b>exact</b> equation for the <b>national</b> vote totals

  $$\begin{align}
  A_2  =  \beta_A A+ \beta_B B+\beta_C C+\beta_D D+\beta_E E+\beta_F F+\beta_G G+\beta_H H+\beta_J J+\beta_K K+\beta_N N
  \end{align}$$
  where each $\beta_i$ represents the <b>true parameters</b>, giving the proportion of votes transferred to Mitterand in the second round.
  
* This is different from the regression equation, defined by
    
  $$\begin{align}
  y_i = \mathbf{x}_i^\mathrm{T} \boldsymbol{\beta} + \epsilon_i, 
  \end{align}$$
  
  where for some choice of explanatory variables $\mathbf{x}_i$ (the vote totals of <b>a particular department</b>) the will be missmatch ($\epsilon_i$) between the proportion of votes transferred from the overall totals ($\beta_i$) and the local totals.

* The error term will likely have different variance with respect to different departments.

  * Assuming (naively) that the total votes for Mitterand in the second round for a particular district is the sum of uncorrelated random variables (the first round candidate totals for a particular department), then
  
  * we might treat the variance of the second round outcome for Mitterand in a district as proportional to the number of voters in the district;
  
  * this is because the variance of the left side of the equation ($y_i$) will be equal to a weighted sum of the variances of the right side ($x_i$).
  
  
========================================================

### An example of weighted least squares


* If the weights for each department are to be inversely proportional to their variance, we may set the weights equal to 1/EI.

* Note, it is natural in this equation to neglect the intercept, as there should be no vote transfer when there are no voters.

```{r}
lmod <- lm(A2 ~ A+B+C+D+E+F+G+H+J+K+N-1, fpe, weights=1/EI)
coef(lmod)
```

* Fitting the model without weights, we find a substantial difference,

```{r}
lm(A2 ~ A+B+C+D+E+F+G+H+J+K+N-1, fpe)$coef
```

* However, there is an issue about the physicallity of the model, where proportions should be between zero and one.

* To fix this, we can logically remove the voters for Giscard and likewise automatically set the vote transfer for Mitterand to one (assuming that each block votes once again for the same candidate in the second round).


========================================================

### An example of weighted least squares


* As an ad hoc fix for the other parameters greater than one, we set these to one exactly (along with the parameter for Mitterand) with the "offset" function;

* Likewise, for the other coefficients that are negative, we set these to zero (removing them from the regression equation):

```{r}
lmod <- lm(A2 ~ offset(A+G+K)+C+D+E+F+N-1, fpe, weights=1/EI)
coef(lmod)
```

* The above coefficients suggest (roughly true to historical understanding) that:

  1. Nearly all communist party voters (D) supported Mitterand in the second round;
  2. Surprisingly, a signficant number $\approx 20\%$ of Gaullist voters ( C ) voted for Mitterand, shifting the overall balance;
  3. Ecology party voters (E) split their votes almost evenly in the second round;
  4. Non-voters in the first round split their vote rougly evenly.

* We note, the analysis we had on the French 1981 presidential performed more rigorously with a form of constrained least squares;

  * That is, using a more sophisticated package, we can explicitly set the range for the parameters $0 \leq \hat{\boldsymbol{\beta}}_i \leq 1$.
  
  * This is discussed explicitly by Faraway.


========================================================

## A second example of weighted least squares


* In examples where the variances are unknown, we may consider an ansatz for the form of the <b>dependence</b> of the variance on the observation.

* Let's consider data on the stopping distance of cars with respect to the observed speed.

```{r fig.width=24, fig.height=6}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
lmod <- lm(dist ~ speed, cars)
plot(residuals(lmod) ~ speed, cars,  cex=3, cex.lab=3, cex.axis=1.5)
```

* Plotting the residuals versus the fitted values, we see what appears to be increasing variance with respect to the increasing speed.


========================================================

### A second example of weighted least squares


* As an ansatz, we may consider a dependence relationship as,

  $$\begin{align}
  \mathrm{sd}(\boldsymbol{\epsilon}_i )= \gamma_0 + x_i^{\gamma_1}
  \end{align}$$

* These coefficients, representing an exponential increase in the standard deviation with the speed of the vehicle, can be estimated simultaneously with the parameters for the regression.

* Here, we will use the generalized least squares function "gls" in the "mgcv" library, with a function in the argument for the weights.

```{r}
library("mgcv")
wlmod <- gls(dist ~ speed, data=cars, weight = varConstPower(1, form= ~ speed))
```

* The above assigns weights in terms of the assumption that the variance of the observations is a function of a constant, plus a power rule in relation to the value of the explanatory variable, as in the ansatz.

========================================================


```{r}
summary(wlmod)
```


========================================================

```{r}
summary(lm(dist ~ speed, data=cars))
```

* Compared to the weighted least squares formulation, with the exponential increase in variance, we have a significantly poorer fit.

  <ul> 
    <li>This is evidenced in terms of the drastic difference in the residual standard error for each model:</li>
  <ol>
  <li> weighted $\approx 0.76$</li>
  <li> ordinary $\approx 15.38$</li>
  </ol>
</ul>

========================================================

<h2>Testing for lack of fit</h2>

* When our assumptions (e.g., G-M) hold then the residual standard error,

  $$\begin{align}
  \hat{\sigma}^2 \triangleq \frac{\hat{\boldsymbol{\epsilon}}^\mathrm{T} \hat{\boldsymbol{\epsilon}}}{n-p} 
  \end{align}$$

  is an unbiased estimate of the true variance $\sigma^2$ of the error.
  
* However, we have now seen many ways how these assumptions can break down.

  * When the model doesn't have the appropriate structure or is not complex enough to fit the data, we will typically overestimate $\sigma^2$ with the emprical $\hat{\sigma}^2$;
  
  * likewise, if we over parameterize the model, then $\hat{\sigma}^2$ will typically underestimate the true variance $\sigma^2$.
  
* In an ideal sittuation, where we may actually know $\sigma^2$, comparing the two values would lead us to understanding the fit of the model by our empirical estimate versus the known value.

  * One example may be when we have only measurement errors for the observations with known variance in their measurments.

* By the same principle, we may more generally get a good estimate of $\sigma^2$ when we have multiple observations of the response for the same values of the explanatory variables.

========================================================

### Testing for lack of fit

* We note that this requires an assumption of the <b>independence</b> of the measurements.

  * That is to say, we need to require that we have multiple independent observations corresponding to the same value of the explanatory variable.
  
* For example, we may be measuring blood presssure as the response variable and using age, height, weight, and other measurements as the explanatory varibles in a model.

* It won't suffice to re-measure the same individual to obtain an estimate of $\sigma^2$ as this will only measure the within-subjet variability.

* Rather, we would need to find multiple individuals with the same measurements for the explanatory varibles to estimate the variance $\sigma^2$ of the error around the signal.

* We call these multiple, independent observations of the response, with the identical explanatory variables <b>replicates</b>.

========================================================

### Replicates

* Let $\mathbf{Y}_{ij}$ be the $i$-th observation of the $j$-th group of replicates.

  * This is to say that all $\mathbf{Y}_{ij}$ correspond to the same values of $\mathbf{x}_j$ where $\mathbf{x}_j$ will be a set of measurements of the explanatory variables.
  
  
* With the mean over all replicates in the $j$-the group, $\overline{\mathbf{Y}}_j$, we can then estimate $\sigma^2$ independently of the regression model as

  $$\begin{align}
  \frac{SS_{pe}}{df_{pe}} \triangleq& \frac{\sum_{j}\sum_{i}\left(\mathbf{Y}_{ij} - \overline{\mathbf{Y}}_{j}\right)^2}{\sum_j \left( \text{number of replicates of type }j - 1\right)} \\
  &=\frac{\sum_{j}\sum_{i}\left(\mathbf{Y}_{ij} - \overline{\mathbf{Y}}_{j}\right)^2}{n - \text{number of groups}}
  \end{align}$$
  
* The estimate given above is known as the <b>pure error</b>.

* We can formulate a test for lack of fit in terms of the ratio of the two estimates for variance, i.e.,
<ol>
  <li> the estimate by the pure error above;</li>
  <li> and the regression based estimate of $\hat{\sigma}^2$.</li>
  
<b>Q:</b> what test statistic can we use to test if two empirical variances match?

<b>A:</b> this can be done with the F-test.

========================================================

### F-test for checking for lack of fit

* A convenient way to perform the last F-test is as follows:

 1. We may fit a model in which each value for a replicate group is treated as a factor;
 
 2. by treating these as a categorical factor, our model becomes saturated as there will be one parameter in the model per replicate group;
 
 3. the fitted value for each group of replicates will be given as the mean for this group of replicates;
 
 4. while this doesn't provide a model for explaining the phenomenon, the standard error of this dummy model is equal to the pure error;
 
 5. thus, we can use the F-test in an ANOVA table to compare the regression model standard error with the pure error.

* We will demonstrate this process in an example.

========================================================

<h3> An example of testing for lack of fit</h3>

* We can perform a lack of fit test with the corrosion data in the Faraway package, studying the loss of weight in samples of copper/ nickel alloy, due to corrosion, when placed in sea water. 

```{r}
library("faraway")
head(corrosion)
```

* Each sample has varying ammounts of iron, and the loss is measured in terms of milligrams per day.

* We fit a simple regression model for the rate of loss per day based on the iron content:

```{r}
lmod <- lm(loss ~ Fe, corrosion)
sumary(lmod)
```

========================================================

### F-test for checking for lack of fit

* We plot the simple regression along with the data points:

```{r fig.width=24, fig.height=7}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(loss ~ Fe, corrosion,xlab="Iron content",ylab="Weight loss",  cex=3, cex.lab=3, cex.axis=1.5)
abline(coef(lmod))
```

* although the $R^2$ value is strong, and the fit looks "OK" we want to test this intuition more formally.

========================================================

### F-test for checking for lack of fit

* Here, in the ANOVA table, the null hypothesis is that the standard error estimate and the pure error estimate are empirical variances that approximate the same $\sigma^2$.

* The alternative hypothesis is that these do not estimate the same quantity, i.e., that the regression model lacks fit.

  * In particular, this would say that the form of the model,
    
    $$\begin{align}
    \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
    \end{align}$$
    
    is not an accurate form for the model, as the pure error doesn't match the error given by this formulation. 

========================================================

### F-test for checking for lack of fit


```{r}
lmoda <- lm(loss ~ factor(Fe), corrosion)
anova(lmod, lmoda)
```

* Due to the extremely small p-value, we reject the null at $5\%$ significance.

* In particular, the pure error is only 1.4, while the standard error is significantly larger at 3.06.

* In this case, it appears that there is still a structural issue in the model itself, which makes the $R^2$ misleading.

========================================================

## Evaluating goodness of fit

* A more general question is of how close of a fit to the data is actually appropriate for the problem at hand.

* By including up to sixth degree polynomial terms in the explanatory variable, we can acheive an almost perfect fit, but the model is completely unphysical and clearly will lack any inference power.

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
lmodp <- lm(loss ~ Fe+I(Fe^2)+I(Fe^3)+I(Fe^4)+I(Fe^5)+I(Fe^6),corrosion)
plot(loss ~ Fe,  data=corrosion,ylim=c(60,130), cex=3, cex.lab=3, cex.axis=1.5)
points(corrosion$Fe,fitted(lmoda),pch=3)
grid <- seq(0,2,len=50)
lines(grid,predict(lmodp, data.frame(Fe=grid)))
```

========================================================

### Evaluating goodness of fit

<div style="width:60%; float:left">
<img src="quadratic.png" style="width:100%" alt="Image of data points distributed around mean, quadratic in the predictor."/>
</div>
<div style="width:40%; float:left">
<ul>
  <li> In the previous example, including higher order terms increased the $R^2$ value significantly, but became a case of obvious over-fitting. </li>
  <li> In general, we cannot fixate too much on $R^2$ for this reason, but we must be aware of the structure of the data and the model.</li>
  <li> In a different example, to the left, it is natural to use quadratic terms in the explanatory variable, rather than linear;</li>
  <li> a model that is linear in the explanatory variable, versus quadratic, will overestimate $\sigma^2$ via $\hat{\sigma}^2$, but the model that is quadratic in $x$ will be unbiased.</li>
  <li> Starting next time, we will start discussing transformations of the variables and model selection...</li>
</ul>
