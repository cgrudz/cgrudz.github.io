<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

Analysis of variance approach to simple linear regression
========================================================
date: 09/14/2020
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>


========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:

  * Analysis of variance approach to regression
  * Decomposing the variation
  * Degrees of Freedom
  * Mean squares and the ANOVA table


========================================================

<h2>Analysis of variance approach</h2>

<ul>
 <li>We have seen one approach now for regression analysis which will be the basic framework in which we consider these linear models.</li>
 <li>However, there are additional ways to approach the regression model, among which is known as Analys of Variance or ANOVA.</li>
 <li>This approach, which we will introduce in the following, seeks to partition the variation in the signal into different components for creating hypothesis tests.</li>
 <li>We will introduce the main concepts here, which will underpin a number of the techniques which we will introduce in full generality in multiple regression.</li>
</ul>

========================================================
<h3>Total sum of squares </h3>

<ul>
  <li>We note, there are several forms of variation in our regression analysis.</li>
  <li>Among these is the variation of the response variable around its empirical, sample-based mean,
  $$ Y_i - \overline{Y}$$</li>
  <li>Analogously to how we earlier defined the RSS in terms of the squared-deviations of $Y_i$ from the regression-estimated mean response,
  $$RSS = \sum_{i=1}^n \hat{\epsilon}_i^2;$$</li>
  <li>we will define the <b>Total Sum of Squares (TSS)</b> in terms of the squared-deviations of $Y_i$ from the sample-based mean of the response:
  $$TSS\triangleq \sum_{i=1}^n \left( Y_i - \overline{Y}\right)^2.$$</li>
  </li>
  <li><b>Q:</b> if all observations of the response variable have the same value, then what value does the TSS is attain?</li>
  <li><b>A:</b> the TSS must equal zero, as $Y_i = \overline{Y}$ for all $i$.</li>
  <li>In this regard, the greater the overall variation in the response variable across all cases, then the greater is the TSS.</li>
  <li>The TSS represents the variation around a null model, in which we would consider the variation present in the response to be random variation around its sample-based mean, irrespective of the explantory variable $X$.</li>
  <li>In general, the RSS does not equal the TSS for the reason described above </li>
  <ul>
    <li>in particular, if there is a signal in the data, we expect there to be less variation in the RSS than in the TSS.</li>
  </ul>
</ul>

========================================================
<h3>Residual sum of squares </h3>

<ul>
  <li> While we can consider the TSS a measure of the total variation around the null model of random variation around the mean, we can also consider how much of this variation is "explained".</li>
  <li> Particularly, consider the quantity, the <b>Explained Sum of Squares (ESS)</b>
  $$ESS = \sum_{i=1}^n\left(\hat{Y}_i - \overline{Y}\right)^2;$$
  <li>This represents how much variation in the signal is explained by our regression model;</li>
  <ul>  
    <li>if our regression model is the null model, i.e., the $i$-th fitted value is just the sample-based mean of the observed responses, $\hat{Y}_i =\overline{Y}$, then $ESS=0$.</li> 
  </ul>
  <li> Therefore, as we will show in the following, we can generally consider a larger $ESS$ corresponding to a regression model with better performance.</li>
</ul>

========================================================
<h3>Partitioning the errors </h3>

<ul>
  <li>To demonstrate the meaning of the ESS corresponding to a better performance, we consider the following partition of the variation in the response,
  $$\underbrace{Y_i - \overline{Y}}_{TSS} = \underbrace{\hat{Y}_i - \overline{Y}}_{ESS} +\underbrace{ Y_i - \hat{Y}_i}_{RSS},$$
  where we say each term loosely corresponds to the TSS, ESS, or RSS as above.</li>
  <li>This corresponds in a loose sense to decomposing the total deviation of the response around the mean into:</li>
  <ol>
    <li>the deviation of the fitted values around the mean (ESS), plus</li>
    <li>the deviation of the observed values from the fitted values (RSS)</li>
  </ol>
  <li><b>Q:</b> how do we obtain the equality of the right-hand-side with the left-hand-side above?</li>
  <li><b>A:</b> we can always add zero to any equation to acheive equality, i.e.,
  $$Y_i - \overline{Y} = Y_i - \overline{Y} + \left(\hat{Y}_i - \hat{Y}_i \right).$$
  <li> Re-arranging terms recovers the decomposition as above.</li>
</ul>

========================================================
<h3>Partitioning the errors -- continued </h3>

<ul>
  <li>While we have motivated the decomposition of the TSS, we haven't actually shown the decomposition.</li>
  <li> Specifically, we need to demonstrate that,
  $$\sum_{i=1}^n \left(Y_i - \overline{Y}\right)^2 = \sum_{i=1}^n \left(\hat{Y}_i - \overline{Y}\right)^2 + \sum_{i=1}^n \left(Y_i - \hat{Y}_i \right)^2,$$
  which is non-trivial, and is a consequence of the choice of the estimation by least squares.</li>
  <li> We will begin by adding zero and expanding terms,
  $$\begin{align}
  TSS&= \sum_{i=1}^n \left[Y_i  - \overline{Y}\right]^2 \\
  & = \sum_{i=1}^n \left[ \left(\hat{Y}_i - \overline{Y}\right) + \left(Y_i - \hat{Y}_i \right)\right]^2\\
  &= \sum_{i=1}^n \left[ \left(\hat{Y}_i - \overline{Y}\right)^2 + \left(Y_i - \hat{Y}_i \right)^2 + 2 \left(\hat{Y}_i - \overline{Y}\right)\left(Y_i - \hat{Y}_i \right)\right]\\
  &= ESS + RSS + 2\sum_{i=1}^n  \left(\hat{Y}_i - \overline{Y}\right)\left(Y_i - \hat{Y}_i \right).
  \end{align}$$
  <li>Therefore, we need to demonstrate that the sum of cross terms vanishes to prove the partition.</li>
</ul>

========================================================
<h3>Partitioning the errors -- continued </h3>
<ul>
  <li>It will be sufficient to show that
  $$\sum_{i=1}^n  \left(\hat{Y}_i - \overline{Y}\right)\left(Y_i - \hat{Y}_i \right) =0$$
  </li>
  <li>We will study this property in the class activity, proving that
  $$TSS = ESS + RSS.$$</li>
  <li> In the above form, we see the tradeoff between the two terms in the $TSS$, particularly,
    <ol>
      <li>When the $RSS$ is large, this says:
      <ul>
        <li>the squared-distance between the fitted and the observed values, $RSS= \sum_{i=1}^n\hat{\epsilon}_i^2$, is large;</li>
        <li>particularly, the $ESS$ (explained variation) is small and the fit is close to the null model.</li>
      </ul>
      <li>When the $ESS$ is large, this says:</li>
      <ul>
        <li> the squared-distance between the fitted and the empirical, sample-based mean, $ESS=\sum_{i=1}^n \left(\hat{Y}_i - \overline{Y}\right)^2$, is large;</li>
        <li> particularly, the $RSS$ is small, implying a close fit between the predicted and the observed values.</li>
      </ul>
    </ol>
</ul>

========================================================
<h2>Goodness of fit</h2>

<ul>
  <li>With the last discussion as a motivation, we can introduce our first metric for the "goodness of fit" of a regression model.</li>
  <li>A common choice to examine how well the regression model actually fits the data is called the "coefficient of determination" or "the percentage of variance explained".</li>
  <li>For short, we define,
  $$R^2 = 1 - \frac{\sum_{i=1}^n \left( Y_i - \hat{Y}_i\right)^2}{\sum_{i=1}^n \left(Y_i - \overline{Y}\right)^2} = 1 - \frac{RSS}{TSS}$$</li>
  <li><b>Q:</b> recalling the realtionship $TSS = ESS + RSS$, what is the possible range of $R^2$ and what the maximal and minimal value correspond to.</li>
  <li><b>A:</b> if $RSS=TSS$, then we have a value of $R^2=0$, corresponding to a null model, i.e., simply random variation about the sample-based mean.</li>
  <li>The smallest value $RSS$ can attain is $0$, in which case $R^2=1$, corresponding to the case where all fitted values equal the observed value.</li>
  <li> Generally, we consider a model with $R^2$ close to one a "good" fit, and $R^2$ close to zero a bad fit.</li>
  <ul>
    <li><b>Note:</b> this metric has a number of flaws, which we will discuss further in the course.</li>
    <li>However, this metric is commonly used enough and is of great enough historical importance that we should understand it.</li>
  </ul>
</ul>


========================================================

<h3> A visual representation </h2>


<div style="float:left; width:60%">
<img style="width:100%", src="./estimation.png" alt="Visualization of the total variation of data points which is greater than the variation of the data points around the regression function."/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p>
</div>

<div style="float:left; width:40%">
<ul>
  <li> In the case of simple linear regression, we can visualize the meaning of $R^2$ directly in terms of the variation of the observations around the regression function.</li>
  <ul>
    <li> The solid arrow represents the variance of the data about the sample-based mean of the response.</li>
    <li> The dashed arrow represents the variance of the data about the least squares predicted mean response.</li>
    <li> $R^2$ is defined by one minus the ratio of these two variances.</li>
  </ul>
  <li> Intuitively, by the "picture-proof", we want the variation of the cases about the predicted mean response to be much smaller than the variation around the empirical mean.</li>
  </ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li> This corresponds, intuitively, to the idea that the response varies tightly with respect to the regression function, and there is indeed structure to the signal.</li>
  <li> If we had a null model, where the response is flat with respect to the change in the predictor $X$, then the $RSS$ and the $TSS$ would be the same.</li>
</ul>
</div>


========================================================

<h3> Computing $R^2$ in the R language</h3>

<ul>
  <li>Our definition for $R^2$ is the same one used in the language R, so we emphasize this.</li>
  <ul>
    <li> However, this definition assumes that there is an intercept term for the model.</li>
  </ul>
  <li> If there is <b>no intercept term</b>, i.e., $\beta_0 = 0$, then $R^2$ is equal to the correlation of the fitted values with the observed values, squared:
  $$\begin{align}
  R^2 & = cor^2 \left(\hat{Y}, Y\right)
  \end{align}$$
  </li>
  <li>The value of $R^2$ should be computed from this definition if the model is fitted without intercept.</li>
  <li>If we don't take care to do it this way, the <strong>coefficient of determination will be misleadingly high</strong>.</li>
  <li><b>Note:</b> for this reason, we must take care about using the model summary command in the R language, when we have a model without intercept.</li>
  <li>Defining a model without intercept $\beta_0=0$ is usually an uncommon assumption, and it is taken only when there is a good "physical" meaning to taking a model without intercept.</li>
  <li>For example, if we are forming a model for a population size based on the food supply as the predictor, there is a clear "physical" meaning for $\beta_0=0$.</li>
  <ul>
    <li>It would be reasonable to require that if there is zero food supply, the population should be zero or randomly fluctuate around zero due to migrations through the study area.</li>
  </ul>
  <li>In general, however, the $beta_0$ is usually used simply as the intercept and may not have a real meaning for the relationship, only that it sets a base level for the response, appropriate for the scope of the model.</li>
</ul>

========================================================
<h3> What are "good" values of $R^2$?</h3>

<ul>
  <li>There is no universal "good" value for $R^2$ to attain in practice.</li>
  <li> For physics and engineering applications, data will be produced in tightly controlled experiments.</li>
  <ul>
    <li>Measurement noise will typically be low, and there are strong correlation and causality relationships in these settings.</li>
    <li> In that case, we expect $R^2$ to be close to one in order to say the fitted values model the observations well.</li>
  </ul>  
  <li> In the social sciences, there is much more variability, typically causal relationships (if they exist) are not well understood and correlations are weaker.</li>
  <ul>
    <li> In this case, we will typically expect a "good fit" to have a much lower $R^2$ score.</li>
  </ul>
</ul>

========================================================
<h3> Simulated examples of $R^2\approx.65$</h3>

<div style="float:left; width:50%">
<img style="width:100%", src="./R_2.png" alt="Figure of different arrangements of data points, all with $R^2$ approximately .65.  Descriptions are in the text"/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p>
</div>
<div style="float:right; width:50%">
<ul>
  <li>On the left, we see how different configurations of data can all result in the same $R^2$ score.</li>
  <ul>
    <li> <b>Upper left:</b> the plot is well-behaved for R2 -- there is a clear trend with some variation.</li>
    <li> <b> Upper right: </b> the residual variation is smaller than the first plot but the variation in the observations is also smaller, so the $R^2$ score is about the same.</li>
    <li> <b> Lower left: </b> the fit looks strong except for a couple of outliers, which lower the overall score. </li>
    <li> <b> Lower right: </b> the relationship is quadratic, leading to some irregularity in the fit with a straight line.</li>
  </ul>
</ul>
</div>


========================================================
<h2>Breakdown of degrees of freedom</h2>

<ul>
  <li>We have used the notion of the "degrees of freedom" loosely up to this point, and we want to formalize this analysis.</li>
  <li>The degrees of freedom refer to the number of values that are free to vary (the number of free parameters or independent variables) in the computation of some statistic.</li>
  <li>In particular, this can be considered geometrically for a set of of $n$ observations of the response, $\left\{Y_i\right\}_{i=1}^n$;</li>
  <ul>
    <li>If we identify the $n$ observations as an $n$-dimensional vector
    $$\mathbf{Y} = \begin{pmatrix} Y_1,& \cdots, &Y_n\end{pmatrix}^\mathrm{T} ,$$
    we say that as a random vector, it can attain a value in any subspace  of the $n$-dimensional space $\mathbb{R}^n$.</li>
    <li>Suppose we have the sample-based mean defined as before as $\overline{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$.</li>
    <li>Then, we can re-write the random vector $\mathbf{Y}$  in terms of two objects, one which lives in $1$-dimensional space and one  which lives in $n-1$-dimensional space:
    $$\mathbf{Y} = \overline{Y} \begin{pmatrix}1 & \cdots & 1\end{pmatrix}^\mathrm{T} + \begin{pmatrix} Y_1 - \overline{Y} & \cdots & Y_n - \overline{Y}\end{pmatrix}^\mathrm{T}$$
    <li>The first quantity on the right-hand-side is constrained to live in the $1$-dimensional subspace that is spanned by the vector $\begin{pmatrix}1 & \cdots & 1\end{pmatrix}^\mathrm{T}$; here the only "free" parameter is the value of $\overline{Y}$.</li>
  </ul>
</ul>

========================================================
<h3>Breakdown of degrees of freedom -- continued</h3>

<ul>
  <li> Continuing our ananlysis:
 $$\mathbf{Y} = \overline{Y} \begin{pmatrix}1 & \cdots & 1\end{pmatrix}^\mathrm{T} + \begin{pmatrix} Y_1 - \overline{Y} & \cdots & Y_n - \overline{Y}\end{pmatrix}^\mathrm{T}$$</li>
  <ul>
    <li>The second quantity on the right-hand-side may appear have $n$-dimensions of possible values, but there is a constraint implied by the used degree of freedom:
    $$\sum_{i=1}^n \left(Y_i - \overline{Y}\right) =0,$$
    such that there are only $n-1$ degrees of freedom left over.</li>
    <li> Particularly, the second quantity (sometimes referred to as the anomalies) live in a $n-1$-dimensional subspace of the full $\mathbb{R}^n$ space.</li>
    </ul>
    <li>Therefore, when we compute the unbiased sample-based variance, the normalization by $n-1$ makes sense by the fact that the quantity
    $$\sum_{i=1}^n \left(Y_i - \overline{Y}\right)^2,$$
    has only $n-1$ degrees of freedom, or values that are not yet determined.</li>
  <li>This is analogous to the earlier lecture when we discussed the over constrained/ under constrained/ unique solution to finding a  line through data points in the plane.</li>
</ul>

========================================================
<h3>Breakdown of degrees of freedom -- continued</h3>


<ul>
  <li>In the case of estimating the regression function, we see similarly,
  $$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X$$
  we estimate two parameters as linear combinations of the observed cases $(X_i,Y_i)$.</li>
  <li>We consider the $Y_i$ to be the free values here, while the two normal equations provide two constraints to the estimated regression function.</li>
  <li> Correspondingly, as we introduce more parameters $p$ in the model, we will use more degrees of freedom, solving a system of equations for $p$ parameters;</li>
  <li> equivalently, we will put more constraints on the system until it becomes uniquely (or eventually over-) constrained, with $n-p$ degrees of freedom available to determine the regression function.</li>
  <li><b>Q:</b> We said that the definition of the unbiased estimate for $\sigma^2$ generalizes to multiple regression, simply by increasing the number of $p$ parameters to account for the additional estimated quantities in our regression.</li>
  <li>Suppose $n=p$, what is our unbiased estimate of the variance $\sigma^2$?</li>
</ul>

========================================================
<h3>Breakdown of degrees of freedom -- continued</h3>

<ul>
  <li><b>A:</b> recall the definition,
  $$\hat{\sigma}^2 \triangleq \frac{RSS}{n-p},$$
  such that if $n-p=0$, the equation is undefined.</li>
    <li>Indeed, if $n-p=0$ this is a completely constrained system, with a unique value for the regression fuction --- this is actually a serious issue of overfitting, which we will return to later.</li>
    <li>Particularly, one issue we can see already is that we do not have a means of uncertainty quantification for our estimates.</li>
    <li>If $n-p<0$, we have an overconstrained or "super-saturated" model for which different techniques entirely are needed for the analysis.</li>
</ul>


========================================================
<h3>Degrees of freedom of TSS and RSS</h3>

<ul>
  <li>By the earlier discussion, we say that the TSS,
  $$TSS = \sum_{i=1}^n \left(Y_i - \overline{Y}\right)^2$$
  has $n-1$ degrees of freedom.</li>
  <ul>
    <li>This corresponds to the fact that there are $n$ values that the observations can attain, with one constraint from the sample-based mean.</li>
  </ul>
  <li>Similarly, we find that the RSS,
  $$\begin{align}
  RSS &= \sum_{i=1}^n \left( Y_i - \hat{Y}_i \right)^2\\
  &= \sum_{i=1}^n \left( Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i \right)^2
  \end{align}$$
  has $n-p$ degrees of freedom $(p=2)$, because there are $p$ constraints on this relationship given any $n$ possible values that $Y_i$ attain.</li>
</ul>

========================================================
<h3>Degrees of freedom of ESS</h3>

<ul>
  <li>Let us derive the number of degrees of freedom of the explained sum of squares,
  $$ESS = \sum_{i=1}^n \left(\hat{Y}_i - \overline{Y}\right)^2$$
  </li>
  <li><b>Q:</b> we will use the property that the mean of the fitted values is equal to the mean of the observed values, i.e.,
  $$\frac{1}{n}\sum_{i=1}^n \hat{Y}_i = \overline{Y};$$
  using any of the properties we have proven already about the regression function, can you show why this is?</li>
  <li><b>A:</b> one useful property we have shown with the normal equations is that the sum of the residuals is zero, i.e.,
  $$\sum_{i=1}^n \hat{\epsilon}_i = 0.$$
  </li>
  <li> Therefore, we can consider that
  $$\overline{Y} =\frac{1}{n} \sum_{i=1}^n Y_i =  \frac{1}{n}\sum_{i=1}^n\left( \hat{Y}_i + \hat{\epsilon}_i\right) = \frac{1}{n}\sum_{i=1}^n \hat{Y}_i$$
</ul>

========================================================
<h3>Degrees of freedom of ESS -- continued</h3>

<ul>
  <li>Recalling the form for the explained sum of squares,
  $$\begin{align}
  ESS &= \sum_{i=1}^n \left(\hat{Y}_i - \overline{Y}\right)^2 \\
  &=\sum_{i=1}^n \left[\hat{\beta}_0 + \hat{\beta}_1X_i - \left(\frac{1}{n}\sum_{i=1}^n \hat{Y}_i \right) \right]^2 
  \end{align}$$
  where the above used the relationship we just proved.</li>
  </li>
  <li><b>Q:</b> in what way can we simplify the above expression?</li>
  <li><b>A:</b> one way is to substitute the definition of the fitted value $\hat{Y}_i$ once again and cancel terms,
  $$\begin{align}
  ESS &=\sum_{i=1}^n \left\{ \hat{\beta}_0 + \hat{\beta}_1X_i - \left[\frac{1}{n}\sum_{i=1}^n  \left(\hat{\beta}_0 + \hat{\beta}_1 X_i \right)\right] \right\}^2 \\
  &=\sum_{i=1}^n \left\{ \hat{\beta}_0 + \hat{\beta}_1X_i - \hat{\beta}_0 - \hat{\beta}_1 \overline{X}  \right\}^2\\
  &= \hat{\beta}_1^2 \sum_{i=1}^n \left(X_i - \overline{X}\right)^2
  \end{align}$$</li>
</ul>

========================================================
<h3>Degrees of freedom decomposition</h3>

<ul>
  <li>From the last derivation, we have that
  $$\begin{align}
  ESS &= \hat{\beta}_1^2 \sum_{i=1}^n \left(X_i - \overline{X}\right)^2
  \end{align}$$</li>
  <li>Although the the $ESS$ is computed from $n$ deviations, they are all derived from the same regression line.</li>
  <li>If we suppose the regression line is the free value in this case, it has two degrees of freedom described by its slope $\hat{\beta}_1$ and its intercept $\hat{\beta}_0$.</li>
  <li>However, as we saw earlier, we cancel the terms with the intercept such that $\hat{\beta}_1$ is the only degree of freedom (free parameter) in the $ESS$.</li>
  <li>Therefore, we say that the $ESS$ has one degree of freedom.</li>
  <li>An important consequence of this for the analysis of variance approach is that the degrees of freedom, like the total variation, are <b>additive</b>:,
  $$\underbrace{TSS}_{n-1} = \underbrace{ESS}_{p - 1} + \underbrace{RSS}_{n-p},$$
  where $p=2$ in simple regression.</li>
  <li> The above concept and the geometry likewise generalize to multiple regression, which we will come to shortly.</li>
</ul>

========================================================
<h3>Mean Squares</h3>

<ul>
  <li>A sum of squares, such as, the $TSS$, $ESS$ or $RSS$ when divided by its associated degrees of freedom is referred to as a mean square.</li>
  <li>Therefore, we will identify the following quantities:</li>
  <ol>
    <li>the regression mean square: $\frac{ESS}{p-1}$;</li>
    <li>the residual mean square error: $\frac{RSS}{n-p}$;</li>
  </ol>
  <li><b>Q:</b> we have mentioned once before that one of the above is an unbiased estimator --- can you recall what is the value of,
  $$\mathbb{E}\left[\frac{RSS}{n-p}\right]?$$</li>
  <li><b>A:</b> the residual mean square error, denoted $\hat{\sigma}^2$ is an unbaised estimator for $\sigma^2$; therefore,
  $$\mathbb{E}\left[\frac{RSS}{n-p}\right] = \sigma^2$$</li>
</ul>

========================================================
<h3>Mean Squares -- continued</h3>

<ul>
  <li>It can be shown that similarly, the regression mean square has an expectation,
  $$\mathbb{E}\left[\frac{ESS}{1} \right]= \sigma^2 + \beta_1^2 \sum_{i=1}^n \left(X_i - \overline{X}\right)^2$$
  </li>
  <ul>
    <li> Note, however, while the residual mean square error takes the form for higher dimensions, the above regression mean square does not.</li>
  </ul>
  <li><b>Q:</b> suppose $\beta_1\neq 0$, which is larger, the expected regression mean square or the expected residual mean square error?</li>
  <li><b>A:</b> provided all cases don't correspond to the same value $X_i$, the sum of squares $\sum_{i=1}^n\left(X_i - \overline{X}\right)^2$ is positive.</li>
  <li>Therefore, comparing the two values of the regression mean square and the residual mean square error provides some means to determine "how likely is it that $\beta_1=0$?"</li>
    <li>Particularly, the expected value of a mean square gives the mean around which the sample-based estimate will vary;</li>
    <ul>
      <li> if $\beta_1 \neq 0$, we expect the regression mean square to attain a value greater than the RSS.</li>
    </ul>
  <li>This type of comparison will underpin our hypothesis tests, which we will introduce shortly in multiple regression.</li>
</ul>

========================================================
<h3>The ANOVA table</h3>

<ul>
  <li> Collecting all the information we have developed so far in the analysis of variance framework, we arrive at the ANOVA table.</li>
  <ul>
    <li>A sample ANOVA table:
    <table>
      <tr><th>Source</th> <th>Degrees of freedom</th> <th>Sum of squares</th> <th>Mean square</th> <th> F-statistic</th></tr>
      <tr><td>Regression</td> <td>$p-1$</td> <td>$ESS$</td> <td>$\frac{ESS}{p-1}$</td> <td>$F$</td></tr>
      <tr><td>Residual</td> <td>$n-p$</td> <td>$RSS$</td> <td>$\frac{ESS}{n-p}$</td> <td></td></tr>
      <tr><td>Total</td> <td>$n-1$</td> <td>$TSS$</td> <td>$\frac{TSS}{1}$</td><td></td></tr>
    </table>
    </li>
  </ul>
  <li>The R language will provide an ANOVA table that arranges the information we have discussed, similarly to the above.</li>
  <ul>
    <li>The piece of data we haven't discussed so far is the one we have been alluding to --- the value of the F-statistic for hypothesis testing.</li>
  </ul>
  <li>It is not strictly necessary to compute all the elements of the table --- as the originator of the table, Fisher said in 1931, it is "nothing but a convenient way of arranging the arithmetic."</li>
  <li>When we introduce multiple regression, we will return to this table to interpret our results in terms of hypothesis testing versus the null model.</li>
</ul>


