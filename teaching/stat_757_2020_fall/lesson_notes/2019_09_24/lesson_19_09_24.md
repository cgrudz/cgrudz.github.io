<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

The Gauss-Markov Theorem
========================================================
date: 09/24/2019
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:
  * A proof of the Gauss-Markov Theorem


========================================================

<h2>The Gauss-Markov theorem</h2>

<ul>
  <li>We have now seen a number of reasons why we can believe that least squares is a good "out-of-the-box" approach to creating a statistical model.</li>
  <li>While more complicated situations may warrant more complex solutions, the estimate by least squares is a good first choice for the following reasons.</li>
  <ol>
    <li>The solution by least squares arises geometrically by the orthogonal projection, and thus is a naturally motivated choice by the geometry.</li>
    <li>If the error distribution for $\boldsymbol{\epsilon}\sim N\left(\boldsymbol{0}, \sigma^2 \mathbf{I}\right)$, it is apparent that least squares yields the maximum likelihood estimate.</li>
    <ul>
      <li>Moreover, even when errors aren't stictly Gaussian, this is often a "good approximation" by the Central Limit Theorem.</li>
    </ul>
    <li>Finally, we claim that the solution by least squares is the "Best Linear Unbiased Estimator";</li>
    <ul>
      <li>we have shown already that the estimate is indeed linear and unbiased, but we will finally verify that this estimate is moreover the minimum variance estimate and that it is unique in this property.</li>
    </ul>
  </ol>
</ul>

========================================================

<h3>Estimation space</h3>

<ul>
  <li>We will begin by introducing the notion of an "estimable" function.</li>
  <li>Recall the form for our model, in which we take the following assumptions:</li>
  <ol>
    <li>We suppose that there is a linear signal in the data where the response vector $\mathbf{Y}\in\mathbb{R}^n$ can be written
    $$\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},$$
    for a fixed, but unknown vector of parameters $\boldsymbol{\beta}\in \mathbb{R}^p$.</li>
    <li>We suppose that the variation in the signal $\boldsymbol{\epsilon}$ is mean zero, uncorrelated across cases, and of finite and constant variance, i.e.,
    $$\begin{align}
    \mathbb{E}\left[\boldsymbol{\epsilon}\right] &= 0 \\
    cov(\boldsymbol{\epsilon}) &= \sigma^2 \mathbf{I}
    \end{align}$$
  </ol>
  <li>With this description of the signal, we will make a decomposition of the space $\mathbb{R}^n$ --- we will call a subspace $\mathcal{E} \subset \mathbb{R}^n$ the estimation space.</li>
  <li>The orthogonal complement to the estimation space $\mathcal{E}^\perp$ will be called the error space.</li>
</ul>

========================================================

<h3>Estimation space -- continued</h3>

<ul>
  <li>In particular, the estimation space will be viewed as the range space of the extended matrix of predictors, i.e., the design matrix.</li>
  <li>Therefore, for any vector $\mathbf{v}\in\mathcal{E}$, there will exist some combination of parameters $\left\{\overline{\beta}_i\right\}_{i=1}^p$ for which,
  $$\mathbf{v} = \sum_{i=1}^n \mathbf{X}^{(i)} \overline{\beta}_i, $$
  i.e., $\mathbf{v}\in \mathcal{E} = span(\mathbf{X})$ so it is a linear combination of the columns of the design matrix.</li>
  <li>The $\left\{\overline{\beta}_i\right\}_{i=1}^p$ represent the coordinates of the vector $\mathbf{v}$ relative to the estimation space.</li>
  <li>Note, when the columns of the design matrix are linearly independent, this choice of $\left\{\overline{\beta}_i\right\}_{i=1}^p$  will be unique, but non-unique otherwise.</li>
  <li> Then recall, from our assumptions on the structure of the signal, 
  $$\mathbf{E}\left[ \mathbf{Y}\right] =  \mathbf{X} \boldsymbol{\beta},$$
  and we thus suppose that the mean response is in the estimation space.</li>
</ul>

========================================================

<h3>Estimable functions</h3>
  
<ul>  
  <li>In this context, we will define an <b>estimable function</b> as follows:</li>
  <ul>
    <li>A linear combination of the parameters $\psi = \mathbf{C}^\mathrm{T} \overline{\boldsymbol{\beta}}$ is called estimable if and only if there exists a linear combination $\mathbf{a}^\mathrm{T}\mathbf{Y}$ for which,
    $$\mathbb{E}\left[ \mathbf{a}^\mathrm{T} \mathbf{Y}\right] = \mathbf{c}^\mathrm{T}\boldsymbol{\beta},$$
    for all choices of $\overline{\boldsymbol{\beta}} \in \mathbb{R}^p$.</li>
  </ul>
  <li>That is, estimable functions must be equivalently written (in expectation) as a linear combination of the observed response.</li>
  <li>If the columns of $\mathbf{X}$ are linearly independent, then all functions as above are estimable.</li>
  <ul>
    <li>However, if the columns of $\mathbf{X}$ contain a dependence relationship, not all functions will be estimable.</li>
  </ul>
  <li>Therefore, the condition of estimability will restrict our consideration to a sub-class of functions of interest, allowing for the case of degeneracy.</li>
  <li>Note, estimable functions include, e.g., predictions of future observations and thus are of intrinsic interest for a linear model.</li>
</ul>

========================================================

<h3>The Gauss-Markov Theorem -- a formal statement</h3>
  
<ul>
  <li>Suppose that $\psi = \mathbf{c}^\mathrm{T} \overline{\boldsymbol{\beta}}$ is an estimable function;</li>
  <li>The Gauss-Markov theorem formally states:
  <ul>
    <li>under our previous assumptions, in the class of all linear, unbiased estimates of $\psi$, the least squares estimate
    $$\hat{\psi} = \mathbf{c}^\mathrm{T} \hat{\boldsymbol{\beta}}$$
    is the minimum variance estimate and is unique.</li>
  </ul>
  <li>We will now formally prove the formal statement of the Gauss-Markov theorem.</li>
</ul>

========================================================

<h3>The Gauss-Markov Theorem -- a proof</h3>

<ul>
  <li>Given that $\psi = \mathbf{c}^\mathrm{T} \overline{\boldsymbol{\beta}}$ is an estimable function, there exists some $\mathbf{a}$ for which $\mathbf{a}^\mathrm{T} \mathbf{Y}$ is an unbiased estimate for all $\overline{\boldsymbol{\beta}}$, i.e.,
  $$\begin{align}
  & \mathbb{E}\left[ \mathbf{a}^\mathrm{T} \mathbf{Y}\right] = \mathbf{c}^\mathrm{T}  \overline{\boldsymbol{\beta}} \\
  \Leftrightarrow &  \mathbf{a}^\mathrm{T} \mathbf{X} = \mathbf{c}^\mathrm{T}  \overline{\boldsymbol{\beta}}
  \end{align}$$</li>
  <li>If we suppose the above relationship holds for every choice of $\overline{\boldsymbol{\beta}}$, we can choose, e.g.,
  $$\overline{\boldsymbol{\beta}} = \begin{pmatrix} 1 & 0 & \cdots & 0 \end{pmatrix}^\mathrm{T};$$</li>
  <li>this shows that the first component of $\mathbf{a}^\mathrm{T} \mathbf{X}$ must equal $c_1$.</li>
  <li>Using a similar argument for each component, we find,
  $$\mathbf{a}^\mathrm{T}\mathbf{X} = \mathbf{c}^\mathrm{T}$$</li>
  <li>Particularly, taking the transpose $\mathbf{X}^\mathrm{T} \mathbf{a} = \mathbf{c}$ such that $\mathbf{c}$ lies in the range space of $\mathbf{X}^\mathrm{T}$.</li>
</ul>

========================================================

<h3>The Gauss-Markov Theorem -- a proof</h3>

<ul>
  <li>We want to show from the fact that $\mathbf{c}$ lies in the range space of $\mathbf{X}^\mathrm{T}$, that $\mathbf{c}$ also lies in the range space of $\mathbf{X}^\mathrm{T} \mathbf{X}$.</li>
  <li>One way to approach this is to recall the <b>singular value decomposition</b> of a matrix.</li>
  <li>Given a matrix $\mathbf{X}\in \mathbb{R}^{n\times p}$ with $n>p$, there exsist the following,</li>
  <ul>
    <li>an orthogonal matrix $\mathbf{U}\in \mathbb{R}^{n\times n}$;</li>
    <li>an orthogonal matrix $\mathbf{V}\in \mathbb{R}^{p\times p}$;</li>
    <li>a rectangular diagonal matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{n\times p}$, i.e., with non-zero elements only on the principal diagonal;</li>
    <li> such that,
    $$\mathbf{X} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^\mathrm{T}.$$</li>
  </ul>
  <li>The diagonal entries $\sigma_i$ of the matrix $\boldsymbol{\Sigma}$ (ordered decendingly in size) 
  $$\boldsymbol{\Sigma} = 
  \begin{pmatrix} 
  \sigma_1 & 0 & \cdots & 0 \\
  0 & \sigma_2 & \cdots & 0 \\
  \vdots &  & \ddots &   \vdots \\
  0 &  0 & \cdots & \sigma_p \\
  0 & 0 & \cdots & 0 \\
  \vdots & \vdots & &\vdots  \\
  0 & 0 & \cdots & 0
  \end{pmatrix}$$
  are the <b>singular values</b> of the matrix $\mathbf{X}$.</li>
  <li><b>Exercise (2 minutes):</b> prove that the singular values are equal to the square roots of the non-negative eigenvalues of $\mathbf{X}^\mathrm{T} \mathbf{X}$.</li>
</ul>

========================================================

<h3>The Gauss-Markov Theorem -- a proof</h3>

<ul>
  <li><b>Solution:</b> recall, by the definition of an orthogonal matrix, we can write
  $$\begin{align}
  \mathbf{V}^\mathrm{T}\mathbf{V} &  =  \mathbf{V}\mathbf{V}^\mathrm{T} = \mathbf{I}_p \in\mathbb{R}^{p \times p}\\
  \mathbf{U}^\mathrm{T} \mathbf{U} &  = \mathbf{U} \mathbf{U}^\mathrm{T} = \mathbf{I}_n \in \mathbb{R}^{n\times n}
  \end{align}$$</li>
  <li>Then we consider,
  $$\begin{align}
  \mathbf{X}^\mathrm{T} \mathbf{X} &= \mathbf{V} \boldsymbol{\Sigma}^\mathrm{T}\mathbf{U}^\mathrm{T}\mathbf{U} \boldsymbol{ \Sigma}\mathbf{V}^\mathrm{T}\\
  &=\mathbf{V} \boldsymbol{\Sigma}^\mathrm{T}\boldsymbol{ \Sigma}\mathbf{V}^\mathrm{T}.
  \end{align}$$</li>
  <li>Given the form of the matrix $\boldsymbol{\Sigma}$, we know that the product of $\boldsymbol{\Sigma}^\mathrm{T}\boldsymbol{\Sigma}$ will be a $p\times p$ diagonal matrix, with $\sigma^2_i$ on the $i$-th diagonal entry.</li>
  <li>Therefore, we have constructed an eigenvalue decomposition for the symmetric matrix $\mathbf{X}^\mathrm{T}\mathbf{X}$.</li>
  <li>Note, a similar argument shows that,
  $$\mathbf{X}\mathbf{X}^\mathrm{T} = \mathbf{U}\boldsymbol{\Sigma}\boldsymbol{\Sigma}^\mathrm{T}\mathbf{U}$$
  is also an eigenvalue decomposition.</li>
  <li>However, the product $\boldsymbol{\Sigma}\boldsymbol{\Sigma}^\mathrm{T}$ will be an $n\times n$ matrix, will zeros on the diagonal after the $p$-th entry.</li>
</ul>

========================================================

<h3>The Gauss-Markov Theorem -- a proof</h3>

<ul>
  <li>Recall, we want to show from the fact that $\mathbf{c}$ lies in the range space of $\mathbf{X}^\mathrm{T}$, that $\mathbf{c}$ also lies in the range space of $\mathbf{X}^\mathrm{T} \mathbf{X}$.</li>
  <li>Suppose that $\mathbf{X}^\mathrm{T}$ has rank $r\leq p$.</li>
  <li>Then we can see from the decomposition
  $$\mathbf{X}^\mathrm{T} = \mathbf{V} \left(\boldsymbol{\Sigma}^\mathrm{T} \mathbf{U}^\mathrm{T}\right),$$
  the range space of $\mathbf{X}^\mathrm{T}$ is in the span of the first $r$ columns of $\mathbf{V}$;  all other columns are scaled by zeros.</li>
  <li>Therefore, we can see the range space of
  $$\begin{align}
  \mathbf{X}^\mathrm{T} \mathbf{X}
  &=\mathbf{V} \left(\boldsymbol{\Sigma}^\mathrm{T}\boldsymbol{ \Sigma}\mathbf{V}^\mathrm{T}\right).
  \end{align}$$
   likewise corresponds to the leading $r$ columns of $\mathbf{V}$.</li>
   <li>Therefore, $\mathbf{c}$ is in the range space of $\mathbf{X}^\mathrm{T} \mathbf{X}$, and there exists some vector $\boldsymbol{\Gamma}$ for which,
   $$\mathbf{c} = \mathbf{X}^\mathrm{T}\mathbf{X} \boldsymbol{\Gamma}$$</li>
</ul>

========================================================

<h3>The Gauss-Markov Theorem -- a proof</h3>

<ul>
  <li>Given the last proven statement,
  $$\mathbf{c} = \mathbf{X}^\mathrm{T}\mathbf{X} \boldsymbol{\Gamma},$$
  we know that for the least squares choice of $\hat{\boldsymbol{\beta}}$,
  $$\begin{align}
  \mathbf{c}^\mathrm{T}\hat{\boldsymbol{\beta}}& = \boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{X} \hat{\boldsymbol{\beta}} \\
  & = \boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T} \mathbf{X}\left[\left( \mathbf{X}^\mathrm{T} \mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}\mathbf{Y}\right]\\
  &= \boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y}.
  \end{align}$$</li>
  <li>We will use this above equality momentarily...</li>
  <li>For now, we will suppose that we have an arbitrary, linear unbiased estimator $\mathbf{a}^\mathrm{T} \mathbf{Y}$.</li>
  <li>Then,
  $$\begin{align}
  var\left(\mathbf{a}^\mathrm{T}\mathbf{Y}\right) &=var\left(\mathbf{a}^\mathrm{T} -\mathbf{c}^\mathrm{T} \hat{\boldsymbol{\beta}} + \mathbf{c}^\mathrm{T} \hat{\boldsymbol{\beta}}\right) \\
  &= var\left(\mathbf{a}^\mathrm{T} -\boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y} + \mathbf{c}^\mathrm{T} \hat{\boldsymbol{\beta}}\right),
  \end{align}$$
  by the above relationship.</li>
</ul>

========================================================

<h3>The Gauss-Markov Theorem -- a proof</h3>

<ul>
  <li>Then consider, for the arbitrary, linear unbiased estimator $\mathbf{a}^\mathrm{T} \mathbf{Y}$,
  $$\begin{align}
  var\left(\mathbf{a}^\mathrm{T}\mathbf{Y}\right)& =var\left(\mathbf{a}^\mathrm{T}\mathbf{Y} -\boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y} + \mathbf{c}^\mathrm{T} \hat{\boldsymbol{\beta}}\right) \\
  &=var\left(\mathbf{a}^\mathrm{T}\mathbf{Y} -\boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y} \right) + var\left(\mathbf{c}^\mathrm{T} \hat{\boldsymbol{\beta}}\right) + 2 cov\left(\mathbf{a}^\mathrm{T}\mathbf{Y} -\boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y}, \boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y}\right),
  \end{align}$$
  due to an identity for distributing the variance.</li>
  <li>We note,
  $$\begin{align}
  \mathbb{E}\left[\mathbf{a}^\mathrm{T}\mathbf{Y} - \boldsymbol{\Gamma}^\mathrm{T}\mathbf{X}^\mathrm{T} \mathbf{Y}  \right] &=\left( \mathbf{a}^\mathrm{T} -\boldsymbol{\Gamma}^\mathrm{T}\mathbf{X}^\mathrm{T} \right) \mathbb{E}\left[ \mathbf{Y}\right] \\
  &=\left( \mathbf{a}^\mathrm{T} -\boldsymbol{\Gamma}^\mathrm{T}\mathbf{X}^\mathrm{T} \right) \mathbf{X}\boldsymbol{\beta},
  \end{align}$$
  </li>
  <li>Similarly,
  $$\begin{align}
  \mathbb{E}\left[ \boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y}\right] &=\left(\boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T} \mathbb{E}\right)\left[ \mathbf{Y}\right]\\
  &=\left( \boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\right) \mathbf{X}\boldsymbol{\beta}.
  \end{align}$$</li>
  <li>Thus,
  $$\begin{align}
  cov\left(\mathbf{a}^\mathrm{T}\mathbf{Y} -\boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y}, \boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y} \right) &= \left( \mathbf{a}^\mathrm{T} -\boldsymbol{\Gamma}^\mathrm{T}\mathbf{X}^\mathrm{T} \right) \mathbb{E}\left[ \boldsymbol{\epsilon} \boldsymbol{\epsilon} \right] \mathbf{X}\boldsymbol{\Gamma} \\
  &=\left( \mathbf{a}^\mathrm{T} -\boldsymbol{\Gamma}^\mathrm{T}\mathbf{X}^\mathrm{T} \right) \sigma^2 \mathbf{I} \mathbf{X}\boldsymbol{\Gamma}\\
  &= \sigma^2\left( \mathbf{a}^\mathrm{T}\mathbf{X} -\boldsymbol{\Gamma}^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{X} \right) \boldsymbol{\Gamma} \\
  &= \sigma^2\left(\mathbf{c}^\mathrm{T} - \mathbf{c}^\mathrm{T}\right)\boldsymbol{\Gamma} \\
  &= 0
  \end{align}$$
</ul>

========================================================

<h3>The Gauss-Markov Theorem -- a proof</h3>

<ul>
  <li>We have thus shown,
  $$\begin{align}
  var\left(\mathbf{a}^\mathrm{T}\mathbf{Y}\right)& =var\left(\mathbf{a}^\mathrm{T}\mathbf{Y} -\boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y} \right) + var\left(\mathbf{c}^\mathrm{T} \hat{\boldsymbol{\beta}}\right)
  \end{align}$$</li>
  <li>Note, variances are non-negative, and therefore,
  $$\begin{align}
  var\left(\mathbf{a}^\mathrm{T}\mathbf{Y}\right)& \geq var\left(\mathbf{c}^\mathrm{T} \hat{\boldsymbol{\beta}}\right),
  \end{align}$$
  such that $\hat{\boldsymbol{\beta}}$ is the minimum variance estimate.
  </li>
  <li>Note that the inequality attains equality if an only if,
  $$\begin{align}
  &var\left(\mathbf{a}^\mathrm{T}\mathbf{Y} -\boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y} \right) = 0\\
  \Leftrightarrow &\mathbf{a}^\mathrm{T}\mathbf{Y} = \boldsymbol{\Gamma}^\mathrm{T} \mathbf{X}^\mathrm{T}\mathbf{Y} = \mathbf{c}^\mathrm{T} \hat{\boldsymbol{\beta}}
  \end{align}$$
  by construction.</l>
  <li>Thus, the minimum variance estimator is unique.</li>
  <li>As a reminder, none of this will be required for the midterm, and is only intended so that everyone has exposure to the fully technical side of regression.</li>
  <li>The remainder of the course will be focused on applications of this powerful result, and methods of remediation for when the assumptions do not hold perfectly.</li>
</ul>
