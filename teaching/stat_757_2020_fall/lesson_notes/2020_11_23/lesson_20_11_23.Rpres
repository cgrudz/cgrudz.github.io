
<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

Missing data
========================================================
date: 11/23/2020
autosize: true
incremental: true 
width: 1920
height: 1080

<h2 style='color:black'>Instructions:</h2>
<p style='color:black'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>



========================================================
## Outline 

* The following topics will be covered in this lecture:
  * Types of missing data
  * Summarizing missing data
  * Single imputation
  * Multiple imputation


========================================================

## Introduction

* Missing data causes both technical data processing and statistical challenges in regression analysis.

* By creating a framework for different kinds of missing data, we have some mathematical tools for understanding how it will affect our analysis.

* Certain kinds of missing data are harder than others to treat mathematically -- at times, we won't have good tools for filling missing data values, or treating the implicit bias therein.

* However, certain kinds of missing data can be modeled statistically, along with the uncertainty when treating these cases.

========================================================

<h2>Types of missing data</h2>

* Here is a summary of different missing data regimes we might find ourselves:

  <ol>
    <li> <b>Missing cases</b> -- this is typically the situation we are in during any statistical study.  </li>
    <ul>
      <li>Particularly, we must infer how the signal will generalize to the population at large, using observations of only a small sub-sample.</li>
      <li> In special circumstances, if the cases that we don't observe are not observed due to the signal we are studying, our sample will be biased.</li>
    </ul>
    <li> <b>Incomplete values</b> -- often times when examining medical outcomes, we will not know the final outcomes of patients before the medical study ends.  </li>
    <ul>
      <li>In this situation, data on participants may still hold useful information but we have to deal with the fundamental incompleteness. Methods to handle this include survival analysis</li>
    </ul>
    <li> <b>Missing values</b> -- it is quite often that samples will have some of the observations of the response or explanatory variables missing or corrupted. </li>
    <ul>
      <li>Depending on the type of "missingness" we will have different tools to handle this.</li>
    </ul>
  </ol>
  
========================================================

<h2> Types of missing values</h2>

* The following types of missing values are distinguished as follows:

 <ol>
  <li> <b>Missing Completely At Random (MCAR)</b> </li>
  <ul>
    <li>The probability of any value being missing is the same for every sample.</li>  
    <li> In this case, there is <em>no bias induced</em> by how the values are missing, though we lose information on the signal.</li>
  </ul>
  <li> <b>Missing At Random (MAR) </b></li>
  <ul>
    <li>Here we suppose the probability of a value being missing depends on a systematic mechanism with the known explanatory variables.</li>
    <li> E.g., in social surveys certain groups may be less likely to respond. </li>
    <ul>
      <li>If we know what sub-group the sample belongs to, we can typically delete the incomplete observation provided we adjust the model for the group membership as a factor.</li>
    </ul>
  </ul>
  <li> <b>Missing Not At Random (MNAR)</b>
  <ul>
    <li> The probability that a value is missing from a sample depends on an unknown, latent variable that we don't observe, or based on the response we wish to observe itself.</li>
  <ul>
    <li> E.g., if an individual has something to hide that is embarrassing or illegal, they may quite often avoid disclosing information that would suggest so.
    </li>
    <li>This is difficult, and often mathematically intractible to handle.</li>
  </ul>
 </ol> 

* Amongst the above, when the data is MAR, we can adjust based on observed variables and therefore handle missingness and bias mathematically -- we will focus on this situation.

========================================================

<h2> A concrete example of missing values</h2>

* If we wish to understand methods of treating missing data, we can take a data set and delete values to compare how conclusions might be affected by our treatment.

* Prototypically, we will study the Chicago Insurance data once again, but with values missing at random.

```{r}
library("faraway")
summary(chmiss)
```

========================================================

### Summarizing missing values

* Before we saw how many NA values were in the dataset, but we will also want to know which cases have missing values (and how many).  

```{r}
rowSums(is.na(chmiss))
```

* Here there is at most one value missing from each row; likewise, the missing data is basically evenly spaced in the data.

  * If there was a large number of missing values in a few cases, we could likely drop these cases without loss of information.
  
  * However, in this example, dropping the cases with NA's would delete 20 out of 47 of the cases.
  
========================================================

### Summarizing missing values

* We can also view how clumped or dispersed missing values are graphically as a diagnostic:

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
image(is.na(chmiss),axes=FALSE,col=gray(1:0))
axis(2, at=0:5/5, labels=colnames(chmiss), cex=3, cex.lab=3, cex.axis=1.5)
axis(1, at=0:46/46, labels=row.names(chmiss),las=2, cex=3, cex.lab=3, cex.axis=1.5)
```

========================================================

## Deleting missing values

* Suppose we favor a deletion approach to all cases with missing values.

* We will compare the full model with all values versus the situation in which we delete missing values:

```{r}
modfull <- lm(involact ~ .  - side, chredlin)
sumary(modfull)
```

========================================================

### Deleting missing values -- continued


* On the other hand, when we fit with the cases with missing values deleted (the default for "lm"):

```{r}
modmiss <- lm(involact ~ ., chmiss)
sumary(modmiss)
```

* The standard error increases because the estimates are less precise, due to the loss of information.


========================================================

## Single imputation

* We may consider thus to "fill-in" data into the missing values for various cases -- this is known as data imputation.

* One approach is to fill in values by something "representative" of the known population, 

  * e.g., fill in by each variable mean.

```{r}
(cmeans <- colMeans(chmiss,na.rm=TRUE))
```

* However, we don't want to fill in values for the response, as this is what we are trying to model:

```{r}
mchm <- chmiss
for(i in c(1:4,6)) mchm[is.na(chmiss[,i]),i] <- cmeans[i]
imod <- lm(involact ~ ., mchm)
```

========================================================

### Single imputation -- continued

* Then, we look at the model summary, based on the imputation of the means:

```{r}
sumary(imod)
```

* In this case, there isn't just loss of fit, but also qualitative differences in the parameters.

* Particularly, theft and age have lost significance in this model versus the iterations previously seen.

* Also, the parameters themselves are smaller in magnitude, describing less "effect" in the signal overall.

========================================================

### Single imputation -- continued

* In this case, we see significant bias induced by the imputation (toward the mean values); 

  * this shows how we usually will only consider mean imputation when the number of missing values is small relative to the full population.
  
* In the case that there is a small number of missing values for a categorical variable, we can typically model the "missing-value" as a category of its own.

* We can consider a more sophisticated approach for handling missing values using regression.

* Particularly, if the variables are strongly (anti)-correlated with each other, there is information in the explanatory variables telling how they vary together (or oppostitely).

========================================================

### Single imputation -- continued

* Recall, for percent variables, it is sometimes useful to make a change of scale to the real line when this is a response.

* The logit transformation is given as, $y \mapsto \log\left(\frac{y}{1 - y}\right)$

* The "logit" and "ilogit" (inverse) map are available in the Faraway package.

* We will model the percent ethnic minority in a zip code as regressed on the other variables with the missing cases removed:

```{r}
lmodr <- lm(logit(race/100) ~ fire+theft+age+income,chmiss)
ilogit(predict(lmodr,chmiss[is.na(chmiss$race),]))*100
chredlin$race[is.na(chmiss$race)]
```

* Two of the predictions are reasonable, but two are significantly off.

* This can be performed for each of the explanatory variables, and while preferable to mean imputation, still leads to bias.

  * In a way, we can start over-fitting to our known values and loose the true variance in the population.
  
========================================================

<h2> Multiple imputation</h2>

* If we understand that the loss of the variation of the population is the issue with the earlier approaches, we can try to enforce some variance in the imputation mathematically.

  * If we re-introduce variation on the missing value, but only once, this would just be a less-optimal choice for the single imputation as we performed previously.
  
  * Instead, we will treat this as a re-sampling problem, and re-input multiple cases of perturbed values for the missing term that takes into account the uncertainty of this value.

* We will create 25 different versions of the data "re-sampled" back with uncertainty for each missing value.

* The known values will be the same, but we will have "m" different versions of the missing values, drawn from a Bayesian posterior estimate.

  * The function output of Amelia will include the "m" different datasets:

========================================================

### Multiple imputation -- continued
  
```{r}
library(Amelia)
set.seed(123)
chimp <- amelia(chmiss, m=25)
```

========================================================

### Multiple imputation -- continued

* All imputations are stored in the object that is output by the Amelia function:

```{r}
str(chimp, 1)
```


========================================================

### Multiple imputation -- continued

* To extract the imputations, we can select any particular list value from the object, extracting the field "imputations":

```{r}
str(chimp$imputations, 1)
```

========================================================

### Multiple imputation -- continued

* We will fit a model over each of the versions, and try to find a best model over all perturbed datasets by an averaging.

  <ul>
    <li>Specifically, we will take the average of the parameters as:
  
  $$\begin{align}
    \hat{\boldsymbol{\beta}}_j \triangleq \frac{1}{m} \sum_{j=1}^m \hat{\boldsymbol{\beta}}_{ij},
  \end{align}$$
  
  where here each $i$ represents one of the imputations.</li>
  
  </ul>

* In this case, we can estimate the overall standard error in terms of the variance of the estimated parameters $\hat{\boldsymbol{\beta}}$ derived over the different versions of the imputed data.

 <ul>
  <li> Specifically, the combined standard errors are estimated as,
  
  $$\begin{align}
  s_j^2 \triangleq \frac{1}{m} \sum_{i=1}^m s_{ij}^2 + \mathrm{var}\left(\hat{\boldsymbol{\beta}}_j\left( 1 + \frac{1}{m}\right)\right)
  \end{align}$$
  
  where the variance taken above is the sample variance over the parameters, derived by the imputation. </li>
</ul>


========================================================

### Multiple imputation -- continued

* The "mi.meld" function will make these computations, though we automate the re-fitting of the models with the for loop below:

```{r}
betas <- NULL
ses <- NULL
for(i in 1:chimp$m){
  lmod <- lm ( involact ~ race + fire + theft +age + income, chimp $ imputations [[ i ]])
  betas <- rbind ( betas , coef ( lmod ))
  ses <- rbind (ses , coef ( summary ( lmod )) [ ,2])
}
(cr <- mi.meld(q=betas,se=ses))
modfull$coef
```


========================================================

### Multiple imputation -- continued

* The t-stastistics can be computed similarly,

```{r}
t_stats <- cr$q.mi/cr$se.mi
t_stats
```

* We compute the $\alpha = 5\%$ critical value,

```{r}
crit <- qt(.025, 47 - 6)
crit
```

* and compare,

```{r}
abs(t_stats) > abs(crit)
```

* Here the results are fairly similar, though in this case theft is no longer a significant parameter.

* Many more techniques exist studying missing data, and this is just an introduction on how to approach this with additional references in Faraway.


========================================================

## Summary of missing data

* In almost any situation, we have missing cases in our data

  * we rely on the assumption that the cases which we have in our data is essentially representative of the larger population.

* In real observational data, furthermore, we will often have missing values in cases.

  * In the case the values are missing completely at random, this doesn't essentially bias the analysis.
  
  * However, it is typical that the missing values will vary systematically with the <b>known explanatory values</b>.
  
  * We have tools to adjust for this in our regression by including, e.g. the missingness as a factor in the regression.
  
* The strategies for handling missing values usually depend on how much data is missing.

  * If the proportion of cases with missing values is small, we can usually delete the cases with missing values or perform mean imputation without a bias introduced.
  
  * If this proportion is larger, deletion will degrade the estimation of the signal due to the loss of information.
  
  * Imputation of the mean will bias the regression, and reduce the estimated variance in the signal.
  
  * With a larger proportion of values missing, it is thus usually preferable to regress on the missing values for predictors in terms of the other predictors.
  
  * Resampling values around the mean function of the regression can then introduce the variance lost by a single imputation by the estimated mean value.

========================================================

## Summary of course work

* We have now been through a complete example that utilizes the major themes in this course.

* Specifically, we have analyzed how to perform a multiple regression, with attention to:

  * the adherence of the model to the assumptions of the Gauss-Markov theorem;
  
  * the remediation of the model (if possible) when these assumptions do not hold well;
  
  * the uncertainty of parameters in the model, from the frequentist perspective;
  
  * the explanatory and predictive power of the model, and the reliability of the model for these purposes;
  
  * and how we can make conclusions based on the model, qualified by the uncertainty in our analysis.
  
* In addition, we have introduced a number of special topics in regression analysis, e.g., Generalized least squares, missing values, advanced model selection and transformation techniques, etc...

  * However, the possible techniques one can use in regression is vast;
  
  * therefore, the main goal of this course is that everyone is comfortable with the fundamentals (both theoretically and practically) and is therefore equipped to study additional advanced methods as needed.
  
  

