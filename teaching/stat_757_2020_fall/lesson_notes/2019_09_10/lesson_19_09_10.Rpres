<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

Simple linear regression -- part 3
========================================================
date: 09/10/2019
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>



========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:

  * Gaussian error regression models
  * Estimation by Maximum Likelihood
  * Gaussian correlation models

========================================================

<h2>Reveiew -- regression Assumptions</h2>

<ul>
  <li>Recall our regression equation,
  $$\begin{align}
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i.
  \end{align}$$
  </li>
  <li> This represents our hypothesis for the form of the signal in the data, i.e., the way we believe the response varies systematically (but with variation) when compared with the explanatory variable.</li>
  <li> We suppose that the observed cases $Y_i$ are random outcomes which depend on the realizations of the random variable $\epsilon_i$.</li>
  <li>For every case $Y_i$, we suppose that there is an associated probability distribution from which these outcomes are drawn, with variance $\sigma^2$ and a mean parametrized by
  $$\mathbb{E}\left[Y\right] = \beta_0 + \beta_1 X$$</li>
  <li> This model posits the existence of fixed, but unkown parameters $\beta_0,\beta_1$ that describe the mean response above, which we call the regression function.</li>
  <li> We assume futhermore that each case is uncorrelated, i.e.,
  $$\mathbb{E}\left[\epsilon_i \epsilon_j\right] = 0 $$
  for each $i\neq j$.</li>
  <li>It is under the above conditions that the Gauss-Markov theorem gives a plausible way to estimate the true parameters $\beta_0,\beta_1$ with some guarantees of "how good" the estimates are.</li>
</ul>


========================================================

<h3>Review of ideas -- continued</h3>

<ul>
  <li>In particular, the Gauss-Markov theorem tells us that the estimate for the true parameters $\beta_0,\beta_1$, given by the least squares estimate $\hat{\beta}_0,\hat{\beta}_1$ are the BLUE:</li>
  <ul>
    <li><b>B</b>est</li>
    <li><b>L</b>inear</li>
    <li><b>U</b>nbiased</li>
    <li><b>E</b>stimate</li>  
  </ul>
  <li>We denote these parameter, estimated by least squares, $\hat{\beta}_0,\hat{\beta}_1$.</li>
  <li>These parameters are the minimum variance estimate of the true relationship $\beta_0,\beta_1$, among all linear, unbiased estimators.</li>
  <li> This is to say, suppose we have any other linear, unbiased estimator $\tilde{\beta}_0,\tilde{\beta}_1$,
  $$\mathbb{E}\left[\tilde{\beta}_j\right] = \beta_j;$$</li>
  <li>then,
  $$\mathrm{var}\left(\hat{\beta}_j\right) \leq \mathrm{var}\left(\tilde{\beta}_j\right).$$
</ul>

========================================================

<h3>Review of ideas -- continued</h3>

<ul>
  <li>With respect to the least squares estimate $\hat{\beta}_0,\hat{\beta}_1$, we have the estimated value for the mean response given by,
  $$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X, $$
  for arbitrary $X$.</li>
  <li>When considering a specific case $i$, we can compute a fitted value as,
  $$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i.$$</li>
  <li>For a given case, this defines the residual $\hat{\epsilon}_i$ as the difference between the observed value $Y_i$ and the fitted value $\hat{Y}_i$,
  $$\hat{\epsilon}_i = Y_i - \hat{Y}_i.$$</li>
  <li> The residual is not the same as the (unknown) variation $\epsilon_i$,
  $$\begin{align}
  \epsilon_i &= Y_i - \mathbb{E}\left[Y_i\right], \\
  \hat{\epsilon}_i & = Y_i - \hat{Y}_i.
  \end{align}$$</li>
  <li>Using the sample-based estimate of the mean response $\hat{Y}$, we will create a sample-based estimate of the (unknown) true variance $\sigma^2$.</li>
</ul>

  
========================================================


<h3>Review of ideas -- continued</h3>

<ul>
  <li>Let us denote the sum of square residuals as,
  $$\begin{align}RSS &= \sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2\\
  & = \sum_{i=1}^n \hat{\epsilon}_i^2
  .\end{align}$$</li>
  <li>Then, the general form for the sample-based estimate of $\mathrm{var}(\epsilon)=\sigma^2$ is given as,
  $$\hat{\sigma}^2 \triangleq \frac{RSS}{n-p},$$
  which will extend as well to multiple regression.</li>
  <ul>
    <li>In the case of multiple regression, the meaning of the $RSS$ and $n$ will remain the same, only the number of parameters $p$ will change.</li>
  </ul>
  <li>Though it is not unbiased, we will typically defer to the square root of $\hat{\sigma}^2$ as the estimate of the standard deviation.</li>
  <ul>
    <li>Indeed, as a consequence of concavity of the square root function, we can use Jensen's inequality to demonstrate that $\hat{\sigma}$ estimate will generally underestimate the true standard deviation $\sigma$.</li>
  </ul>
</ul>

========================================================


<h2>Gaussian Error Regression Models</h2>

<ul>
  <li>Regardless of the actual distribution of the error/ variation $\epsilon_i$, the Gauss-Markov theorem holds and the solution by least squares is the BLUE.</li>
  <li>However, to create confidence intervals and  perform uncertainty quantification, we will need to assume some additional structure.</li>
  <li> It is often assumed that the error distribution for $\epsilon_i$ is Gaussian for several reasons: </li>
  <ol>
    <li>Gaussian errors simplify the regression analysis significantly; </li>
    <li>likewise, Gaussian distributions are quite common in real applications; and</li>
    <li>even when errors are not strictly Gaussian, a Gaussian distribution is often a "good" approximation.</li>
  </ol>
  <li> The notion of a Gaussian distribution being a "good" approximation is formalized with the Central Limit Theorem, which we now recall.</li>
</ul>

========================================================

<h3> Central limit theorem</h3>

<ul>
  <li>Suppose we have a sequence of random variables, $\left\{Y_i \right\}_{i=1}^\infty$ which are independent, identically disributed <emph>from any distribution</emph>, such that for all $i$
<ol>
  <li> $\mathbb{E}[Y_i] = \mu$; and</li>
  <li> $\mathrm{var}(Y_i) = \sigma^2$, for some finite $\sigma$.</li>
</ol>
<li> For each $n\geq 1$ define the sample-based mean of the sequence $\left\{Y_i \right\}_{i=1}^n$

  $$\begin{align}
  \tilde{Y}_n \triangleq \frac{1}{n} \sum_{i=1}^n Y_i
  \end{align}$$</li>

<li> Then, as the number of random variables $n\rightarrow \infty$, the sample-based means
  
  $$\begin{align}
  \sqrt{n}\left(\tilde{Y}_n - \mu\right) {\xrightarrow {d}} N(0, \sigma^2)
  \end{align}$$
  where the convergence is in distribution.</li>
  
<li> Put another way, for $n$ sufficiently large, $\tilde{Y}_n$ has <b>approximately</b> a $N\left(\mu, \frac{\sigma^2}{\sqrt{n}}\right)$ distribution.</li>

<li> Heuristically, 
  <ul>
    <li><b>for large sample sizes</b>, we can produce confidence intervals for the mean of the unbiased estimator (e.g., the true $\beta_0,\beta_1$),</li>
    <li>from the sample based estimate (the least squares solution $\hat{\beta}_0 ,\hat{\beta}_1$) with a Gaussian assumption;</li>
    <li>this is a <b>good approximation</b> if not strictly accurate.</li>
  </ul>
<li> The "goodness" of this approximation can be made more formal in another course.</li>
</ul>

========================================================


<h3>Gaussian Error Regression</h3>

<ul>
  <li>The Gaussian assumption will be introduced on top of our existing assumptions as follows:</li>
  <ol>
    <li>We suppose that we have a signal in the data, linear in the parameters $\beta_0,\beta_1$,
    $$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$</li>
    <li>$X_i$ is a known constant, while $\beta_0,\beta_1$ are unkown but fixed values for all $i$.</li>
    <li>$\epsilon_i$ are drawn independently and identically distributed (iid) as,
    $$\epsilon_i \sim N\left(0, \sigma^2\right)$$
    for each $i$.</li>
  </ol>
<li>Independence of the $\epsilon_i$ implies that the cases are once again uncorrelated.</li>
<li>Additionally, as we have assumed the form of the distribution as Gaussian, and the form of the first two moments, we have entirely parametrized the distribution of the variation.</li>
</ul>

========================================================


<h3>Gaussian Error implications</h3>

<ul>
  <li>It can be shown that linear combinations of Gaussian random variables are also Gaussian.</li>
  <li><b>Exercise (5 minutes):</b> Discuss the following with a partner, "if the variation is drawn iid $\epsilon_i \sim N(0,\sigma^2)$, which of the following can we surmise are also Gaussian distributed?"</li>
  <ol>
    <li>$X_i$;</li>
    <li>$Y_i$;</li>
    <li>$\hat{\beta}_0, \hat{\beta}_1$;</li>
    <li>$\hat{Y}_i$;</li>
    <li>$\hat{\epsilon}_i$.</li>
  </ol>
</ul>

========================================================


<h3>Gaussian Error Implications -- continued</h3>

<ul>
  <li><b>Solutions</b></li>
  <ol>
    <li>$X_i$ -- no, this is assumed to be a deterministic value.</li>
    <li>$Y_i$ -- yes, this is essentially the same proof as earlier, but where we now have a distribution to associate to $Y_i$. Specifically,
    $$Y_i \sim N\left(\beta_0 +\beta_1 X_i, \sigma^2\right)$$</li>
    <li>$\hat{\beta}_0, \hat{\beta}_1$ -- indeed, recall the definitions of the point estimators,
    $$\begin{align}
  \hat{\beta}_1 &= \frac{\sum_{i=1}^n\left(X_i - \overline{X} \right)\left(Y_i - \overline{Y}\right)}{\sum_{i=1}^n\left(X_i - \overline{X} \right)}\\
  \hat{\beta}_0 &= \frac{1}{n} \sum_{i=1}^n\left( Y_i - \hat{\beta}_1 X_i \right). \\
  \end{align}$$
  If the $X_i$ values are deterministic constants, then $\hat{\beta}_1$, and thus $\hat{\beta}_0$, are linear combinations of Gaussian random variables.</li>
  </li>
  <li>$\hat{Y}_i$ -- yes, by the definition,
  $$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i,$$
  this is also a linear combination of Gaussian variables.</li>
  <li>$\hat{\epsilon}_i$ -- yes, once again by definition, and the above,
  $$\hat{\epsilon}_i = Y_i - \hat{Y}_i $$
  is a linear combination of Gaussian variables.</li>
  </ol>
</ul>

========================================================


<h3>Gaussian Error Implications -- continued</h3>

<ul>
  <li>As was mentioned before, Gaussianity thus simplifies the regression analysis significantly.</li>
  <li> It is easy to construct confidence intervals for the mean of a sample-based estimate of the mean of a Gaussian random variable from the Student t distribution.</li>
  <li> Likewise, the t-test derived from the Student t distribution can be used to perform hypothesis tests.</li>
  <li>We will return to both of these points when we introduce multiple regression more formally.</li>
  <li> For the moment, we will return to the addendum of the Gauss-Markov theorem:
  <ul>
    <li>If in addition, $\epsilon\sim N\left(0,\sigma^2\right)$, then the solution by least squares is the "maximum likelihood estimator".</li>
  </ul>
  </li>
</ul>

========================================================


<h2>Estimation of parameters by maximum likelihood</h2>

<ul>
  <li> A concept widely used in statistical inference is a "likelihood" function.</li>
  <li>Consider a probability distribution, denoted $P_\theta(y)$, which depends on the value of the parameter $\theta$.</li> 
  <ul>
    <li>Suppose we are investigating some random variable $Y$, for which there is some "true" parameter $\theta_0$, such that 
    $$Y\sim P_{\theta_0}.$$</li>
  </ul>
  <li>We assume that even though $\theta_0$ is not known, <b>we can evaluate</b>
 $$\begin{align}
 P_\theta(y) = P_\theta(Y=y)
 \end{align}$$
 for any choice of $\theta$ and some observed piece of data $y$.
 </li>
 <li>Then, the likelihood of $\theta$ based on an observed $y$ is defined
 $$\begin{align}
 \mathcal{L}(\theta \vert y) = P_\theta(y) = P_\theta(Y=y)
 \end{align}$$
 where we evaluate the probability of $Y$ attaining an observed piece of data $y$ given a <b>choice</b> of $\theta$.</li>
 <li> The "likelihood" is thus a measure of how well does our choice of parameter make the distribution describe the observed data.</li>
 <li>Maximum likelihood estimation is thus the process of finding a $\hat{\theta}$ which maximizes the likelihood, 
 <ul>
  <li>i.e., $\hat{\theta}$ which maximizes the probability of the observed data $y$ given the proability distribution $P_\theta$.</li>
 </ul>
</ul>

========================================================


<h3>Estimation of parameters by maximum likelihood -- an example</h3>

 <div style="float:left; width:50%">
<img style="width:100%", src="likelihood_1.png" alt="Image of Gaussian distribution with mean 230, standard deviation 10, and samples at Y_1 = 250, Y_2=265, Y_3=259"  />
<p style="text-align:center"> Courtesy of: Kutner, M. et al. Applied Linear Statistical Models 5th Edition</p>
</div>
<div style="float:left; width:50%">
<ul>
  <li>Suppose that we have three samples $Y_1,Y_2,Y_3$ from what we know is a population that is Gaussian distributed.</li>
  <ul>
    <li>Moreover, we suppose that while we know the standard deviation $\sigma=10$, we do not know the true mean $\mu$.</li>
  </ul>
  <li>Let $\mathrm{exp}\left\{x\right\}$ denote the function $e^x$.</li>
  <li>By varying the choice fo $\mu$, we can evaluate the density of any particular sample with the Gaussian density
  $$f_j = \frac{1}{\sqrt{2\pi}10}\mathrm{exp}\left\{-\frac{1}{2}\left(\frac{Y_j - \mu}{10}\right)^2\right\},$$
  denoted $f_j$ for sample $j$, based on the <b>choice</b> of $\mu$.</li>
  <li>We suppose that the observed data points are $Y_1 = 250, Y_2=265,Y_3=259$</li>
  <li><b>Q:</b> given the above data points, and the associated graph to the left, does $\mu=230$ appear to be the "most likely" choice for the true mean?</li>
</ul>
</div>

========================================================


<h3>Estimation of parameters by maximum likelihood -- an example continued</h3>

 <div style="float:left; width:50%">
<img style="width:100%", src="likelihood_2.png" alt="Image of Gaussian distribution with mean 259, standard deviation 10, and samples at Y_1 = 250, Y_2=265, Y_3=259" />
<p style="text-align:center"> Courtesy of: Kutner, M. et al. Applied Linear Statistical Models 5th Edition</p>
</div>
<div style="float:left; width:50%">
<ul>
  <li>Intuitively, we can tell that there are better choices for the "center of mass" of the data, given by our choice of $\mu$.</li>
  <li>One particular choice may be to set $\mu= 259= Y_3$ as on the left.</li>
  <li>Indeed, we can compare the values for the density function for each choice of $\mu$ as in the table below:
  <table>
  <tr>
  <th>Density</th><th>$\mu=230$ </th><th>$\mu= 259$</th>
  </tr>
  <tr><td>$f_1$</td> <td>.005399</td> <td>.026609</td> </tr>
  <tr><td>$f_2$</td> <td>.000087</td> <td>.033322</td> </tr>
  <tr><td>$f_3$</td> <td>.000595</td> <td>.039894</td> </tr>
  </table></li>
  <li>The likelihood value for the data above, assuming that these are independent samples of the true Gaussian distribution, is given by the product of the respective densities.</li>
  <li>By performing a numerical optimization, we can recover the "most likely" choice of $\mu$ given our observations.</li>
  <li>However, for a Gaussian distribution as above, it can be demonstrated that the sample mean is the maximum likelyhood estimate of $\mu$.</li>
</ul>
</div>

========================================================


<h3>Estimation of the regression function by maximum likelihood</h3>

<ul>
  <li>As with the toy example before, we can apply the same method to estimating the regression parameters $\beta_0,\beta_1$.</li>
  <li>Recall that we assume $\epsilon_i \sim N(0,\sigma^2)$, such that</li>
    <ul>
      <li>each observation is iid drawn from a Gaussian distribution,
      $$Y_i \sim N(\beta_0+ \beta_1 X_i, \sigma^2)$$</li>
    </ul>
    <li>To put this in the framework of a likelihood function, we suppose that $\overline{\beta}_0,\overline{\beta}_1,\overline{\sigma}$ are free parameters, such that
    $$\begin{align}\mathcal{L}\left(\overline{\beta}_0,\overline{\beta}_1,\overline{\sigma}\vert Y_i\right)& = P_{\overline{\beta}_0,\overline{\beta}_1,\overline{\sigma}}\left(Y_i\right) \\
    &=\frac{1}{\sqrt{2\pi}\overline{\sigma}}\mathrm{exp}\left\{-\frac{1}{2}\left(\frac{Y_i - \overline{\beta}_0 - \overline{\beta}_1 X_i}{\sigma}\right)^2\right\}
    \end{align}$$</li>
    <li>Then, due to the independence, the likelihood over the set of the $n$ observations is given simply as the product of the individual likelihoods:
    $$\mathcal{L}\left(\overline{\beta}_0,\overline{\beta}_1,\overline{\sigma}\vert Y_{i=1,\cdots,n}\right)
    =\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\overline{\sigma}}\mathrm{exp}\left\{-\frac{1}{2}\left(\frac{Y_i - \overline{\beta}_0 - \overline{\beta}_1 X_i}{\sigma}\right)^2\right\}$$
    </li>
    <li> By maximizing the above product over all the observed data, with respect to the choice of the free parameters $\overline{\beta}_0,\overline{\beta}_1,\overline{\sigma}$, we obtain the maximium likelihood estimate.</li>
</ul>

========================================================


<h3>Estimation of the regression function by maximum likelihood -- continued</h3>

<ul>
  <li><b>Q:</b>Suppose we want to maximize the likelihood function,
  $$\mathcal{L}\left(\overline{\beta}_0,\overline{\beta}_1,\overline{\sigma}\vert Y_{i=1,\cdots,n}\right)
    =\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\overline{\sigma}}\mathrm{exp}\left\{-\frac{1}{2}\left(\frac{Y_i - \overline{\beta}_0 - \overline{\beta}_1 X_i}{\sigma}\right)^2\right\};$$
    what approach can we take to make this tractible?</li>
  <li>The function $\log()$ is monotonic, so that an increase in the argument corresponds to an increase in the output.</li>
  <li>Using this fact, we can reduce our analysis of the likilhood function to something more simple, the "log-likelihood",
  $$\log(\mathcal{L}) = constant - n\log{\overline{\sigma}} - \frac{1}{2\overline{\sigma}^2}\sum_{i=1}^n \left(Y_i - \overline{\beta}_0 - \overline{\beta}_1 X_i \right)^2;$$</li>
  <li>finding the maximum value of the above equation is equivalent to finding the maximum of the likelihood, due to the monotonicity.</li>
  <li>Note, we write a number of values as "constant", because for purposes of optimization in the free parameters $\overline{\beta}_0,\overline{\beta}_1,\overline{\sigma}$, these make no difference in the outcome.</li>
  <li><b>Q:</b> what would be a plausible approach of finding the maximum of the above?</li>
  <li><b>A:</b> one approach is to take the derivative in each free parameter, as before.</li>
  <li>This will result in this case in three equations which will resemble the normal equations of the least squares approach.</li>
</ul>


========================================================


<h3>Estimation of the regression function by maximum likelihood -- a remark</h3>


<ul>
  <li>We stated that, as an addendum to the Gauss-Markov theorem, the parameter estimates by least squares $\hat{\beta}_0,\hat{\beta}_1$ are also the maximum likelihood parameters in the case of Gaussian errors.</li>
  <li> However, we note, the maximum likelihood estimate above simultaneously estimates the unknown variance $\sigma^2$.</li>
  <li> As opposed to the unbiased estimate of the variance,
  $$\hat{\sigma}^2 \triangleq \frac{RSS}{n-p},$$
  the maximum likelihood estimate is biased.</li>
  <li> Indeed, the maximum likelihood estimate is given by the naive normalization,
  $$\sum_{i=1}^n \frac{\left(Y_i - \hat{Y}_i\right)^2}{n}.$$</li>
  <li>For large values of $n$, the difference in the denominator between $n$ and $n-1$ is negligible, but for small sample sizes this is quite important.</li>
</ul>


========================================================


<h2>Gaussian correlation models</h2>

<ul>
  <li>So far, we have taken the relatively simple formulation of the regression problem in which the explanatory variable $X$ is assumed to be a controllable constant value.</li>
  <ul>
    <li>In this context, the variation that we refer to in $\epsilon_i$ is prototypically defined in terms of repeated sampling of replicated cases.</li>
    <li> Particularly, we are considering scenarios in which we can hold the value of $X_i$ constant, and take multiple independent samples of the associated value of the response $Y_i$.</li>
  </ul>
  <li>However, there are situations in which we want to consider <b>both</b> the response $Y$ and the predictor $X$ to be random variables.</li>
  <ul>
    <li>For example, an analyst may may be interested in two variables "height of person" and "weight of person" in a study of a sample of persons, with each variable being taken as random.</li>
    <li> The analyst might wish to study the relation between the two variables or might be interested in
    <ul>
      <li> making inferences about weight of a person on the basis of the person's height;</li>
      <li> making inferences about height on the basis of weight;</li>
      <li> or in both.</li>
    </ul>
  </ul>
  <li> In this context, we can treat our regression as a <b>correlation model</b>.</li>
  <li> In the case of a joint Gaussian distribution for $Y$ and $X$, our techniques in regression remain essentially the same, but the framework our our analysis and the meaning of parameters will change.</li> 
  <li> We will discuss this change of framework in the remaining lecture.</li>
</ul>

========================================================


<h3>Bivariate Gaussian distribution</h3>


 <div style="float:left; width:30%">
<img style="width:100%", src="bivariate_gaussian.png" alt="Image of bivariate Gaussian in the real 2-D plane, with vertical direction corresponding to the density value at each point"/>
<p style="text-align:center"> Courtesy of: Kutner, M. et al. Applied Linear Statistical Models 5th Edition</p>
</div>
<div style="float:left; width:70%">
<ul>
  <li>To begin, we will recall the basis of the Gaussian correlation model --- the multivariate Gaussian distribution.</li>
  <li>Reframing slightly, we suppose that $Y_1$ and $Y_2 = X$ are jointly distributed with respect to a Gaussian distribution as on the left.</li>
  <ul>
    <li>In this case, we will re-write $X$ as above, as its meaning is symmetric with the response.</li> 
  </ul>
  <li> The bivariate Gaussian has the functional form of the below, where</li> 
  <ol>
    <li>$\mu_i,\sigma_i$ are the mean and standard deviation of $Y_i$ respectively;</li>
    <li>$\rho_{12} \triangleq \frac{\sigma_{12}}{\sigma_1 \sigma_2}$ is the <b>coefficient of correlation</b> between $Y_1,Y_2$;</li>
    <li>in the figure on the left, we identify $f(Y_1,Y_2)$ as $p_{Y_1,Y_2}(y_1,y_2)$.</li>
  </ol>
</ul>

</div>
<div style="float:left; width:100%">
$$\begin{align}
  p_{Y_1,Y_2}(y_1,y_2) = \frac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1 - \rho_{12}^2}}\mathrm{exp}\left\{\frac{1}{2\left(1 -  \rho_{12}^2\right)} \left[\left(\frac{y_1 - \mu_1}{\sigma_1}\right)^2 + \left(\frac{y_2 - \mu_2}{\sigma_2}\right)^2 - 2\rho_{12}\left( \frac{y_1 - \mu_1}{\sigma_1}\right)\left( \frac{y_2 - \mu_2}{\sigma_2}\right)\right] \right\}
\end{align}$$
</div>
