<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

<!-- foo 
Highlighting
bold
strong
orange <b style="color:#d95f02"> text </b>
green  <b style="color:#1b9e77"> text </b>
purple <b style="color:#d24693"> text </b>
red    <b style="color:#FF0000"> text </b>
blue   <b style="color:#0000FF"> text </b>
-->

The ensemble Kalman filter and smoother part II
========================================================
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>

========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:
  * The iterative ensemble Kalman smoother
  * The single-iteration ensemble Kalman smoother

========================================================
## Motivation

* In the last lecture, we saw how the <b>ensemble Kalman filter (EnKF)</b> can be used to <strong>propagate covariance estimates in the nonlinear model</strong>, while making the linear-Gaussian approximation at first order.

* Rather than analytically computing a large covariance matrix, and its evolution using the tangent-linear approximation, a <b>nonlinear sample is drawn</b> and a <strong>sample-based estimate is performed for the covariance</strong>.

* This sample-based covariance thus forms the <b>background weights</b> for the optimization of the <strong>nonlinear filtering cost function</strong>.

* This can be extended, like the smoothing problem in 4D-VAR, to a global analysis over a time series.

  * Such an approach is what is often known as an <b>ensemble-variational (EnVAR)</b> technique, in which a 4D iterative optimization is made over an initial condition,
  
  * but the <strong>weights and the estimate are constructed with the ensemble</strong>.
  
* This approach can be considered to thus extend 4D-VAR to include a time-varying background covariance using the ensemble-based estimates.

  * Often, however, due to the small feasible ensemble size that can be simulated in the nonlinear model, the <b>ensemble-based background weights</b> are <strong>interpolated with a climatological covariance to regularize the problem</strong>.

========================================================
## Hybrid EnVAR in the EnKF analysis

* The ensemble-variational approach is at the basis of the <b>iterative ensemble Kalman filter / smoother (IEnKF/S)</b>.

* This technique seeks to perform an ensemble analysis like the square root ETKF by <strong>defining the ensemble estimates and the weight vector in the ensemble span</strong>

  $$\begin{alignat}{2}
  & & {\color{#d95f02} {\widetilde{\mathcal{J}} (\pmb{w})} } &= {\color{#d95f02} {\frac{1}{2} \parallel \hat{\pmb{x}}_{0|L-S}^\mathrm{smth} - \mathbf{X}^\mathrm{smth}_{0|L-S} \pmb{w}- \hat{\pmb{x}}^\mathrm{smth}_{0|L-S} \parallel_{\mathbf{P}^\mathrm{smth}_{0|L-S}}^2} } + {\color{#7570b3} {\sum_{k=L-S+1}^L \frac{1}{2} \parallel \pmb{y}_k - \mathcal{H}_k\circ {\color{#1b9e77} { \mathcal{M}_{k:1}\left( {\color{#d95f02} { \hat{\pmb{x}}^\mathrm{smth}_{0|L-S} +  \mathbf{X}^\mathrm{smth}_{0|L-S} \pmb{w} } } \right)}}\parallel_{\mathbf{R}_k}^2 } }\\
 \Leftrightarrow & & {\color{#d95f02} {\widetilde{\mathcal{J}} (\pmb{w})} } &= {\color{#d95f02} { \frac{1}{2} \parallel \pmb{w}\parallel^2} } + {\color{#7570b3} {\sum_{k=L-S+1}^L \frac{1}{2} \parallel \pmb{y}_k - \mathcal{H}_k\circ {\color{#1b9e77} { \mathcal{M}_{k:1}\left( {\color{#d95f02} { \hat{\pmb{x}}^\mathrm{smth}_{0|L-S} +  \mathbf{X}^\mathrm{smth}_{0|L-S} \pmb{w} } } \right)}}\parallel_{\mathbf{R}_k}^2 } }
  \end{alignat}$$

* One measures the cost as the <b>discrepancy from the observations</b> with the nonlinear evolution of the perturbation to the ensemble mean,
  
  $$\begin{align}
   \hat{\pmb{x}}^\mathrm{smth}_{0|L-S} +  \mathbf{X}^\mathrm{smth}_{0|L-S} \pmb{w}
  \end{align}$$
  combined with the <strong>size of the perturbation relative to the ensemble spread</strong>.

* The key is again, how the gradient is computed for the above cost function.


========================================================
### Hybrid EnVAR in the EnKF analysis

* The <b>gradient of the ensemble-based cost function</b> is given by,
  
  $$\begin{align}
  {\color{#d95f02} {\nabla_{\pmb{w}} \widetilde{\mathcal{J}} } }:= {\color{#d95f02} {\pmb{w}}} - {\color{#7570b3} {\sum_{k=L-S+1}^L \widetilde{\mathbf{Y}}_k^\top \mathbf{R}^{-1}_k\left[\pmb{y}_k - \mathcal{H}_k \circ {\color{#1b9e77} { \mathcal{M}_{k:1}\left({\color{#d95f02} {\hat{\pmb{x}}_{0|L-S}^\mathrm{smth} + \mathbf{X}_{0|L-S}^\mathrm{smth} \pmb{w}} }\right) } } \right]}},
  \end{align}$$

* where ${\color{#7570b3} { \widetilde{\mathbf{Y}}_k } }$ represents a <strong>directional derivative</strong> of the observation and state models,

  $$\begin{align}
  {\color{#7570b3} { \widetilde{\mathbf{Y}}_k } }:= {\color{#d95f02} {\nabla\vert_{\hat{\pmb{x}}^\mathrm{smth}_{0|L-S}} } } {\color{#7570b3} {\left[\mathcal{H}_k \circ {\color{#1b9e77} {\mathcal{M}_{k:1} } } \right] } } {\color{#d95f02} {\mathbf{X}^\mathrm{smth}_{0|L-S}} }.
  \end{align}$$

* In order to <b>avoid the construction of the tangent-linear and adjoint models</b>, the "bundle" version makes an explicit <strong>approximation of finite differences with the ensemble</strong>

  $$\begin{align}
  {\color{#7570b3} { \widetilde{\mathbf{Y}}_k } }\approx& {\color{#7570b3} { \frac{1}{\epsilon} \mathcal{H}_k \circ  {\color{#1b9e77} {\mathcal{M}_{k:1} \left( {\color{#d95f02} { \pmb{x}_{0|L-S}^\mathrm{smth} \pmb{1}^\top  + \epsilon \mathbf{X}_{0|L-S}^\mathrm{smth} } }\right) } } \left(\mathbf{I}_{N_e} - \pmb{1}\pmb{1}^\top / N_e \right)} },
  \end{align}$$
  for a small constant $\epsilon$.

* The scheme produces an <b>iterative estimate</b> using a <strong>Gauss-Newton</strong>- or, e.g., <strong>Levenberg-Marquardt</strong>-based optimization.

* A similar scheme used more commonly in reservoir modeling is the <b>ensemble randomized maximum likelihood estimator (EnRML)</b>.


========================================================

## The single iteration ensemble transform Kalman smoother (SIEnKS)


* While accuracy increases with <b>iterations in the 4D-MAP estimate</b>, every iteration comes at the <b style="color:#1b9e77">cost of the model forecast</b> ${\color{#1b9e77} { \mathcal{M}_{L:1} } }$.

* In synoptic meteorology the <b>linear-Gaussian approximation</b> of the evolution of the densities is actually an <strong>adequate approximation</strong>;

  * <b style="color:#1b9e77">iterating over the nonlinear dynamics</b> <strong>may not be justified by the improvement in the forecast statistics</strong>.
  
* However, the <b>iterative optimization over a nonlinear observation operator $\mathcal{H}_k$ or hyper-parameters in the filtering step</b> of the classical EnKS can be run <strong>without the additional cost of model forecasts</strong>.

  * This can be performed similarly to the IEnKS with the <b>maximum likelihood ensemble filter (MELF)</b> analysis.

* Subsequently, the <b style="color:#d95f02">retrospective analysis</b> in the form of the filtering right-transform can be applied to <b style="color:#d95f02">condition the initial ensemble</b>
 
 $$\begin{align}
 \mathbf{E}^\mathrm{smth}_{0|L} = \mathbf{E}_{0:L-1}^\mathrm{smth} \boldsymbol{\Psi}_L
 \end{align}$$

* As with the 4D cost function, one can <b>initialize the next DA cycle in terms of the retrospective analysis</b>, and gain the benefit of the improved initial estimate.

* This scheme, is the <b>single-iteration ensemble Kalman smoother (SIEnKS)</b>.

  
========================================================

### The single iteration ensemble Kalman smoother (SIEnKS)


<div style="float:left; width:100%">
<ul>
  <li>Compared to the classical EnKS, this <strong>adds an outer loop</strong> to the <b style="color:#d95f02">filtering cycle</b> to produce the <b style="color:#d95f02">posterior analysis</b>.</li>
</ul>
</div>
<div style="float:left; width:100%; text-align:center;" class="fragment">
<img style="width:65%", src="single_iteration_diagram.png" alt="Diagram of the filter observation-analysis-forecast cycle."/>
</div>
<div style="float:left; width:100%">
<ul>
  <li>The <b style="color:#7570b3">information flows</b> from the filtered state back in time from the <b style="color:#d95f02">retrospective analysis</b>.</li>
  <li>This re-analyzed state becomes the <b style="color:#1b9e77">initialization for the next cycle</b> over the shifted DAW, carrying this information forward.</li>
  <li>The iterative cost function is only solved in the <b>filtering estimate</b> for the <b style="color:#7570b3">new observations entering the DAW</b>.</li>
  <ul>
    <li>Combined with the retrospective analysis, this comes <b style="color:#1b9e77">without the cost of iterating the model forecast over the DAW</b>.</li>
  </ul>
  <li>When the <b>tangent-linear approximation is adequate</b>, this is shown to be an <strong>accurate and highly efficient approach to sequential DA</strong>.</li>
</ul>
</div>
