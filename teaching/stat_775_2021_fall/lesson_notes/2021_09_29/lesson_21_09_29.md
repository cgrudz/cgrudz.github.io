<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

<!-- foo 
Highlighting
bold
strong
orange <b style="color:#d95f02"> text </b>
green  <b style="color:#1b9e77"> text </b>
purple <b style="color:#d24693"> text </b>
red    <b style="color:#FF0000"> text </b>
blue   <b style="color:#0000FF"> text </b>
-->

Minimum variance and maximum likelihood estimation Part I
========================================================
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>

========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:
  * Minimum variance estimation in a simple example
  * Gauss-Markov theorem

  
========================================================
## Motivation

* In the first part of this course, we developed a variety of <b>tools for how one models a random variable</b> as it <strong>evolves in time with respect to a Markov model</strong>.

* Our prototypical example is the <b>discrete Gauss-Markov model</b>;

  $$\begin{align}
  \pmb{x}_k := \mathbf{M}_k \pmb{x}_{k-1} + \pmb{w}_k
  \end{align}$$

* When the <b style="color:#d95f02">governing process law is linear</b>, as above, and the distributions are Gaussian, we have a <b>full solution for the evolution</b> in terms of the <strong>joint Gaussian density of the forecast</strong>.

* When we consider a <b style="color:#1b9e77">nonlinear initial value problem</b>, if we have a Gaussian prior on the initial data defining the problem, 

  * the <b>tangent-linear model</b> gives an <strong>approximation for the evolution by a Gauss-Markov model on the space of perturbations</strong> (the tangent space).
  
* We can furthermore consider <b style="color:#d24693">SDEs to define a random flow map</b>, when we believe that the <b>shocks to the mechanistic model</b> should be <strong>represented continuously in time</strong>.

   * if the <b>SDEs are linear</b>, this <strong>directly defines a Gauss-Markov model</strong>.
   
   * When the <b>system of SDEs is nonlinear</b>, we can <strong>still use the tangent-linear approximation</strong>, though the details become more complicated.

* What we have not introduced, yet, is how we update such a forecast model when new information becomes available.

========================================================
### Motivation

* Let's add an equation now, supposing that there are <b>observations given sequentially in time</b>:

  $$\begin{align}
  \pmb{x}_k& = \mathbf{M}_k \pmb{x}_{k-1} + \pmb{w}_k\\
  \pmb{y}_k &= \mathbf{H}_k \pmb{x}_k + \pmb{v}_k
  \end{align}$$
  where in the above:
  <ol>
   <li>$\pmb{y}_k\in \mathbb{R}^{N_y}$ is an <strong>observed random variable</strong>; connected to the modeled state $\pmb{x}_k$ by
   <li>the <strong>observation mapping</strong> $\mathbf{H}_k$ which takes the modeled states to the observed variables, which may be different entirely and often $N_y \ll N_x$; plus
   <li><strong>observation noise</strong> $\pmb{v}_k \sim N(\pmb{0}, \mathbf{R}_k)$, modeling a random, unbiased measurement error.</li>
  </ol>
  
* This is to say that,

  $$\begin{align}
  \mathbb{E}\left[\pmb{y}_k\right] = \mathbb{E}\left[\mathbf{H}_k \pmb{x}_k + \pmb{v}_k\right] =\mathbf{H}_k \overline{\pmb{x}}_k
  \end{align}$$
  
* In this case, $\mathbf{M}_k$ <strong>represents the (discretized) time evolution between observation times</strong> in which we receive noisy information about the modeled state $\pmb{x}_k$ in the observed variables $\mathbb{R}^{N_y}$.

* The question then is,

  <blockquote>
  What do we do with the information $\pmb{y}_k$ to produce a better estimate of the modeled state $\pmb{x}_k$ and in <strong>what sense do we mean a "better" estimate</strong>?
  </blockquote>
  
* The next two lectures will develop the statistical tools that allow us to describe the <strong>"optimal" estimation algorithm for Gauss-Markov models</strong>, i.e., the <b>Kalman filter</b>.

========================================================
## Minimum variance estimation

* To begin our discussion of estimation, we will consider a <b>simpler case</b> in which the <strong>modeled variable $\pmb{x}$ does not depend on time</strong>.

* We will also begin with a <strong>scalar form of the estimation</strong> for simplicity, where we wish to <b>estimate the temperature</b> of some system.
  
* Suppose we have two <b>independent observations</b> $T_1, T_2$ of an <strong>unknown, scalar temperature</strong> defined as $T_t$.</li>

* We assume (for the moment) that the <b>temperature is deterministic</b> (and unknown), but the observations will be random, i.e.,
      
   $$\begin{align}
   T_1 &= T_t + \epsilon_1 \\
   T_2 &= T_t + \epsilon_2
   \end{align}$$
      
* We will assume furthermore that 
  
  $$\begin{align}
   \epsilon_1 &\sim N\left(0, \sigma_1^2\right)\\
   \epsilon_2 &\sim N\left(0, \sigma_2^2\right)
    \end{align}$$


========================================================
### Minimum variance estimation

* Suppose that we will estimate the true temperature by a <b>linear combination</b> of the two measurements.

* That is, we will define an "analyzed" temperature as $T_a$ where,
   
   $$\begin{align}
    T_a := a_1 T_1 + a_2 T_2
    \end{align}$$
    
*  We will <strong>require that the analysis is unbiased</strong>
    
    $$\begin{align}
    \mathbb{E}[T_a] = T_t &\Leftrightarrow a_1 + a_2 = 1
    \end{align}$$
   
   * i.e., with $\mathbb{E}[\epsilon_i]=0$, we see that $\mathbb{E}[T_a] = (a_1 + a_2)T_t$.
   
* We choose $a_1$ and $a_2$ to be <b>"optimal"</b> in the sense that they <strong>minimize the mean-square-error of the analysis</strong>, defined as
    
  $$\begin{align}
  \sigma_a^2 &= \mathbb{E}\left[\left(T_a - T_t\right)^2\right] \\
   &=\mathbb{E}\left[ \left( a_1\left\{T_1 - T_t\right\} + a_2\left\{T_2 - T_t\right\}\right)^2 \right].
  \end{align}$$
    
* Substituting the relationship $a_2 = 1 - a_1$, 

  $$\begin{align}
  \sigma^2_a &= \mathbb{E}\left[\left(a_1 \epsilon_1 + \left\{1-a_1\right\}\epsilon_2\right)^2\right] \\
   & =a_1^2 \sigma_1^2 + \left(1 - a_1\right)^2 \sigma_2^2
  \end{align}$$
  as $\epsilon_1$ and $\epsilon_2$ are uncorrelated.
    


========================================================
### Minimum variance estimation

* From the relationship on the last slide,
  
  $$\begin{align}
  \sigma^2_a = a_1^2 \sigma_1^2 + \left(1 - a_1\right)^2 \sigma_2^2
  \end{align}$$
 we can <b>compute the derivative</b> of the variance of the analysis solution above with respect to $a_1$.    

* This equation is <b>convex</b> with respect to $a_1$, so that this achieves a <strong>global minimum exactly at the critical value</strong>.

* Therefore, setting the derivative of $\partial_{a_1} \sigma_a^2 = 0$ we recover,
  
  $$\begin{align}
  & 0= 2 a_1\sigma_1^2 -  2 \sigma_2^2 + 2 a_1 \sigma_2^2 \\
 \Leftrightarrow & a_1 = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}
  \end{align}$$

* The value for $a_2$ can be derived symmetrically in the index.

* We can thus derive the <b>choice of weights</b> that <strong>minimizes the analysis variance</strong> as
 
 $$\begin{align}
 a_1 = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2} & & a_2 = \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} .
 \end{align}$$


========================================================
### Minimum variance estimation

* Notice that 
   
  $$\begin{align}
   a_1 = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2} & & \Leftrightarrow & & a_1 = \frac{\frac{1}{\sigma_1^2}}{\frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2}}
  \end{align}$$
        
* The <strong>inverse of the variance</strong> of the observation is known as the <b>precision</b> of the observation.

* This tells us that we <b>weight the observations</b> in the optimal solution <strong>proportionately to their precision</strong>. 

* Equivalently, we can say we weight the observations <strong>inverse-proportionately to their uncertainty</strong>.

* This simple example is a case of a <b>minimum variance estimator</b>.

<blockquote>
<b>Unbiased minimum variance estimator</b><br>
Let $\hat{\Theta}$ be an unbiased estimator of an unknown fixed parameter $\theta$.  Then, $\hat{\Theta}$ is the <strong>unbiased minimum variance estimator</strong> of $\theta$ if for any other unbiased point estimator $\tilde{\Theta}$
$$\begin{align}
\mathbb{E}\left[(\theta - \hat{\Theta})^2 \right] \leq \mathbb{E}\left[\left(\theta - \tilde{\Theta}\right)^2 \right].
\end{align}$$
</blockquote>

* In the above, we are referring to the expectation over all possible realizations for the point estimator, i.e., in the way it depends on the observed data used to infer $\theta$.

* This is to say, the <b>sampling distribution</b> for $\hat{\Theta}$ has the <strong>least spread about the true value</strong> compared to any other sampling distribution centered at $\theta$.

========================================================
### Minimum variance estimation

* Note, in this simple example, we were considering the <b>frequentist</b> case where $T_t$ is <strong>a fixed constant, not a random variable</strong>.

  * Likewise, in the definition, we described $\theta$ as a <strong>fixed but unknown deterministic parameter</strong>.
  
* This isn't entirely applicable to our case of interest, where we will generically take $\pmb{x}$ to be a <b>random vector</b> as <strong>defined by uncertain initial data</strong> (and possibly an uncertain evolution in time).

* A wider class of problems can be handled in a <b>multiple regression</b> setting by considering, e.g., a <strong>conditional Gaussian model</strong> for both the observations and the modeled state.

* If we assume that $\pmb{x}$ and $\pmb{y}$ are <b>jointly Gaussian distributed</b>, we can construct a correlation model for $\pmb{x}$ given an outcome of $\pmb{y}$ by using the conditional Gaussian.

* In particular, we can then similarly define a loss function

  $$\begin{align}
  \mathcal{J}\left(\hat{\pmb{x}}\right):=\mathbb{E}\left[ \parallel\pmb{x} - \hat{\pmb{x}}\parallel^2 \right]
  \end{align}$$
  which measures the <strong>expected square-difference</strong> of our predicted quantity $\hat{\pmb{x}}$ from the true realization.
  
* <b>Linear regression</b> posits that there is a <strong>linear relationship</strong> that can be defined between observed data and the estimator $\hat{\pmb{x}}$.
  
* This random estimator $\hat{\pmb{x}}$ will also be denoted a <b>linear unbiased minimum variance estimator</b> for the random variable $\pmb{x}$. 

========================================================
## Gauss-Markov theorem

* The <strong>existence and uniqueness of such an optimal solution</strong> is given by the <b>Gauss-Markov theorem</b>.

* We will suppose we have two vectors $\pmb{x}:= \overline{\pmb{x}} + \pmb{\delta}_{\pmb{x}}$ and $\pmb{y}:= \overline{\pmb{y}} + \pmb{\delta}_{\pmb{y}}$; where

* we assume that $\mathbb{E}[\pmb{\delta}_{\pmb{x}}]=\mathbb{E}[\pmb{\delta}_{\pmb{y}}]=\pmb{0}$, i.e., these are vectors of <b>anomalies</b> from the means.

* We will assume that there is some <strong>linear relationship</strong> between $\pmb{\delta}_{\pmb{y}}$ and $\pmb{\delta}_{\pmb{x}}$ that is represented by a matrix of weights $\mathbf{W}$,
  
  $$\begin{align}
  \pmb{\delta}_{\pmb{x}} = \mathbf{W} \pmb{\delta}_{\pmb{y}} + \boldsymbol{\epsilon}
  \end{align}$$
  where $\mathbb{E}\left[\boldsymbol{\epsilon}\right]=\pmb{0}$ and $\mathrm{cov}(\pmb{\epsilon}) = \mathbf{I}$.
  
* As a multiple regression, we will write the estimated value for this relationship by,

  $$\begin{align}
  \hat{\pmb{\delta}}_{\pmb{x}} = \widehat{\mathbf{W}} \pmb{\delta}_{\pmb{y}}
  \end{align}$$
  
  
* such that 
  
  $$\begin{align}
  \pmb{x} - \hat{\pmb{x}} &:= \overline{\pmb{x}} + \pmb{\delta}_{\pmb{x}} - \overline{\pmb{x}} - \hat{\pmb{\delta}}_{\pmb{x}}\\
  &=\pmb{\delta}_{\pmb{x}} - \hat{\pmb{\delta}}_{\pmb{x}} \\
  &=\pmb{\delta}_{\pmb{x}} - \widehat{\mathbf{W}} \pmb{\delta}_{\pmb{y}} \\
                      &= \hat{\pmb{\epsilon}}
  \end{align}$$
  where $\hat{\pmb{\epsilon}}$ is known as the residual error.
  
========================================================
### Gauss-Markov theorem

* The <b>Gauss-Markov theorem</b> loosely states that the weights $\widehat{\mathbf{W}}$ found by minimizing the expected residual sum of squares

  $$\begin{align}
  \text{RSS} =  \hat{\pmb{\epsilon}}^\top \hat{\pmb{\epsilon}},
  \end{align}$$
  is the <strong>Best-Linear-Unbiased-Estimator of the true relationship</strong> $\mathbf{W}$.
  
* This is commonly known as the <strong>BLUE, and this choice of weights is unique</strong>.
  
* The estimator $\widehat{\mathbf{W}}$ is <b>"best" in the sense that this is the linear unbiased minimum variance estimator</b>.

* This equation is also convex in the matrix argument $\mathbf{W}$; to find the minimizing $\widehat{\mathbf{W}}$, we differentiate the expected RSS with respect to the weight matrix
   
  $$\begin{align}   
  \partial_{\mathbf{W}}\mathbb{E}\left[ \hat{\pmb{\epsilon}}^\top \pmb{\epsilon}\right] &:=\mathbb{E}\left\{\partial_{\mathbf{W}}\left[\pmb{\delta}_{\pmb{x}}^\top \pmb{\delta}_{\pmb{x}} - \pmb{\delta}_{\pmb{x}}^\top\left(\mathbf{W} \pmb{\delta}_{\pmb{y}} \right)- \left(\mathbf{W} \pmb{\delta}_{\pmb{y}} \right)^\top\pmb{\delta}_{\pmb{x}} + \left(\mathbf{W} \pmb{\delta}_{\pmb{y}} \right)^\top \left(\mathbf{W} \pmb{\delta}_{\pmb{y}} \right)  \right] \right\}\\
  &=\mathbb{E}\left\{ 2\left[ \left(\mathbf{W}\pmb{\delta}_{\pmb{y}}\right)\pmb{\delta}_{\pmb{y}}^\top-  \pmb{\delta}_{\pmb{x}}\pmb{\delta}_{\pmb{y}}^\top\right]  \right\}
  \end{align}$$

* Setting the equation to zero for $\widehat{\mathbf{W}}$, we obtain the normal equation
  
  $$\begin{align}
  & &\widehat{\mathbf{W}}\mathbb{E}\left[\pmb{\delta}_{\pmb{y}}\pmb{\delta}_{\pmb{y}}^\top\right] - \mathbb{E}\left[ \pmb{\delta}_{\pmb{x}}\pmb{\delta}_{\pmb{y}}^\top\right]&= \pmb{0}\\
 \Leftrightarrow & & \widehat{\mathbf{W}}\mathrm{cov}(\pmb{y}) - \mathrm{cov}(\pmb{x},\pmb{y}) &= \pmb{0}\\
 \Leftrightarrow & & \widehat{\mathbf{W}}&=  \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1}.
  \end{align}$$

  
========================================================
### Gauss-Markov theorem

* Consider, we estimate $\hat{\pmb{\delta}}_{\pmb{x}}=  \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1}\pmb{\delta}_{\pmb{y}}$, where 

  * this is the regression of the deviation from the mean of $\pmb{x}$;
  
  * in terms of the deviation from the mean of $\pmb{y}$.
  
* Recalling that
 
  $$\begin{align}
  \pmb{\delta}_{\pmb{x}}&:= \pmb{x} - \overline{\pmb{x}}\\
  \pmb{\delta}_{\pmb{y}}&:= \pmb{y} - \overline{\pmb{y}}
  \end{align}$$
  we find
  
  $$\begin{align}
  &\hat{\pmb{\delta}}_{\pmb{x}}=  \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1}\pmb{\delta}_{\pmb{y}}\\
  \Leftrightarrow & \hat{\pmb{x}} =  \overline{\pmb{x}} +  \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1}\left(\pmb{y} - \overline{\pmb{y}} \right)
  \end{align}$$

* Suppose that we <strong>compute the covariance</strong> of this estimate -- i.e., the multi-dimensional spread of the minimum variance estimate.


  * From the above, it is obvious that this is an unbiased estimator
  
  $$\begin{align}
  \mathbb{E}\left[\hat{\pmb{x}}\right] &=  \mathbb{E}\left[\overline{\pmb{x}} +  \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1}\left(\pmb{y} - \overline{\pmb{y}} \right) \right] \\
  &= \overline{\pmb{x}},
  \end{align}$$
  such that
  $$\begin{align}
  \mathbb{E}\left[\hat{\pmb{x}} - \pmb{x} \right] = \pmb{0}.
  \end{align}$$
  

========================================================
### Gauss-Markov theorem


* From the last slide, we recover that,

  $$\begin{align}
  \mathrm{cov}\left(\pmb{x}-\hat{\pmb{x}} \right)&= \mathbb{E}\left[\left(\pmb{x} - \overline{\pmb{x}} - \hat{\pmb{x}} + \overline{\pmb{x}}\right)\left(\pmb{x} - \overline{\pmb{x}} - \hat{\pmb{x}} + \overline{\pmb{x}}\right)^\top \right]\\
  &= \mathbb{E}\left[\left(\pmb{\delta}_{\pmb{x}}-  \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1}\pmb{\delta}_{\pmb{y}}\right)\left(  \pmb{\delta}_{\pmb{x}} -  \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1}\pmb{\delta}_{\pmb{y}}\right)^\top \right]\\
  &= \mathbb{E}\left[\pmb{\delta}_{\pmb{x}}\pmb{\delta}_{\pmb{x}}^\top \right] - \mathbb{E}\left[\pmb{\delta}_{\pmb{x}}\pmb{\delta}_{\pmb{y}}^\top \boldsymbol{\Sigma}_{\pmb{y}}^{-1} \boldsymbol{\Sigma}_{\pmb{y},\pmb{x}} \right] - \mathbb{E}\left[\boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1} \pmb{\delta}_{\pmb{y}}\pmb{\delta}_{\pmb{x}}^\top \right] + \mathbb{E}\left[\boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1} \pmb{\delta}_{\pmb{y}} \pmb{\delta}_{\pmb{y}}^\top \boldsymbol{\Sigma}_{\pmb{y}}^{-1}\boldsymbol{\Sigma}_{\pmb{y},\pmb{x}} \right]\\
  &=\boldsymbol{\Sigma}_{x} -  \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1} \boldsymbol{\Sigma}_{\pmb{y},\pmb{x}}
  \end{align}$$

* Recall the conditional Gaussian as previously seen, where we assume $\pmb{x},\pmb{y}$ are jointly Gaussian distributed,

  $$\begin{align}
  &\pmb{x}| \pmb{y} \sim N\left(\overline{\pmb{x}} + \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}}\boldsymbol{\Sigma}_{\pmb{y}}^{-1}\left(\pmb{y} -  \overline{\pmb{y}}\right), \boldsymbol{\Sigma}_{\pmb{x}} - \boldsymbol{\Sigma}_{\pmb{x},\pmb{y}} \boldsymbol{\Sigma}_{\pmb{y}}^{-1} \boldsymbol{\Sigma}_{\pmb{y},\pmb{x}}\right)\\
  \Leftrightarrow & \pmb{x}| \pmb{y} \sim N\left(\hat{\pmb{x}}, \mathrm{cov}\left(\pmb{x} - \hat{\pmb{x}}\right)\right).
  \end{align}$$

* This tells us that, for <b>jointly Gaussian distributed variables</b>,

  * the <strong>conditional Gaussian mean is precisely the BLUE</strong>.
  
* Furthermore, the <b>BLUE and its covariance parameterizes the conditional Gaussian</b> distribution for $\pmb{x}|\pmb{y}$.
  
* The <b>Gauss-Markov theorem</b> <strong>does not require that the underlying distributions are actually Gaussian</strong>, however.

  * Without the Gaussian assumption, we can still construct the BLUE and its covariance as discussed already, though it will not generally parameterize the conditional distribution for $\pmb{x}|\pmb{y}$, which may be non-Gaussian.


  
