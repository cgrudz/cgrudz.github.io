<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

<!-- foo 
Highlighting
bold
strong
orange <b style="color:#d95f02"> text </b>
green  <b style="color:#1b9e77"> text </b>
purple <b style="color:#d24693"> text </b>
red    <b style="color:#FF0000"> text </b>
blue   <b style="color:#0000FF"> text </b>
-->

Continuous-time models and stochastic calculus Part I
========================================================
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>

========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:
  * The stochastic integral
  * Modes of convergence
  * The It&ocirc; Taylor expansion
  * It&ocirc; versus Stratononvich calculus

========================================================
## The stochastic integral

* We have now introduced the context for a <b>discrete Gauss-Markov model</b>, as <strong>generated by a sequence of linear transformations and random shocks</strong>.

* However, in practice, we will often consider a <b>model that evolves in continuous time</b> in between <strong>discrete observations of the system</strong>.

* In order to discuss such a conditional inference problem, we will extend the Gauss-Markov model to continuous time with the notion of a <b>stochastic differential equation</b>.

* The mathematics behind solutions to stochastic differential equations is quite complex, and instead we will focus on an intuitive development of the big picture.

* We will start by discussing the <strong>analogy and the difference between a deterministic and a stochastic integral</strong>.

========================================================
### The stochastic integral

* Consider a smooth function 
   
   $$\begin{align}
   h : [0, T ] \rightarrow \mathbb{R}
   \end{align}$$ 
   for which the derivative $\frac{\mathrm{d}}{ \mathrm{d} t } h$ is bounded on $[0, T ]$. 

* To define the  <b style="color:#d95f02">Riemann integral</b> of $h$, the interval $[0, T ]$ is partitioned into
subintervals 

   $$\begin{align}
   0 = t_0 < t_1 < \cdots < t_{N−1} < t_N = T .
   \end{align}$$ 
   
* The  <b style="color:#d95f02">Riemann integral</b> of $h$ is then given by

  $$\begin{align}
  \int_{0}^T h(t) \mathrm{d}t := \lim_{N\rightarrow\infty} \sum_{j=0}^{N-1} h(t_j)\left(t_{j+1} - t_j\right)
  \end{align}$$

* The stochastic integral can be defined in a similar way, and two forms exist, the <b style="color:#1b9e77">It&ocirc;
form</b> and the <b style="color:#d24693">Stratonovich form</b>.

* Both forms of the stochastic integral are used in practice for different applications. 

  * Notably <b style="color:#1b9e77">It&ocirc;</b> is typically preferred in applications such as finance, due to the fact that it preserves the <b style="color:#1b9e77">martingale property</b>:
  
  $$\begin{align}
  \mathbb{E}\left[\vert X_n\vert \right] < \infty & & \mathbb{E}\left[ X_n |X_{n-1:0}\right] = X_{n-1};
  \end{align}$$
  
  * on the other hand, the <b style="color:#d24693">Stratonovich</b> formulation is often the preferred choice in physical models, due to its connection to <b style="color:#d24693">coarse-grained simulation of a higher-resolution physical process</b>.

========================================================
### The stochastic integral

* We will start by recalling the definition of the Wiener process as

<blockquote>
<b>Wiener process</b><br>
A continuous-time stochastic process is denoted a <strong>Wiener process</strong> $W_{t}$ if it has the following properties:
<ul>
   <li>$W_0:= 0$,</li>
   <li>$W$ has independent increments;</li>
   <li>The increments $W_{t+s} - W_{t} \sim N(0,s)$; and</li>
   <li>$W_t$ is continuous in $t$.</li>
</ul>
</blockquote>

* In analogy to the deterministic integral, the <b style="color:#1b9e77">It&ocirc; integral</b> is given like the <b style="color:#d95f02">Riemann integral</b>, but <strong>integrating "against" a Wiener process</strong>

  $$\begin{align}
  \int_{0}^T h \mathrm{d}W_t:=\lim_{N\rightarrow \infty} \sum_{j=0}^{N-1} h(t_j) \left(W_{t_{j+1}} - W_{t_{j}} \right).
  \end{align}$$

* In the above, note that we use the <b style="color:#d95f02">left-endpoint</b> of the partition of the interval, just like in the <b style="color:#d95f02">Riemann integral</b>.

* The <b style="color:#d24693">Stratonovich integral</b> is defined only as a slight variation where, instead of the left end-point, we use the mid-point rule:

  $$\begin{align}
  \int_0^T h \circ\mathrm{d}W_t := \lim_{N\rightarrow \infty}\sum_{j=0}^{N-1} h\left(\frac{t_{j+1} - t_{j}}{2}\right) \left(W_{t_{j+1}} - W_{t_{j}} \right).
  \end{align}$$
  
========================================================
## Modes of convergence

* In the last slide, the two definitions are <b>deceivingly simple</b>, because we haven't specified in <strong>what way the limit and convergence is actually defined</strong>.

* It turns out, that for random variables, there are several ways we can consider convergence:

<blockquote>
<b>Convergence in probability</b><br>
Let $X_n$ , $n = 1, 2, \cdots ,$ be a sequence of random variables. We say that xn
<strong>converges in probability</strong> to some random variable $X$ if, for every real number $\epsilon> 0$,
$$\begin{align}
\lim_{n\rightarrow \infty} \mathcal{P}\left(\vert X_n - X\vert > \epsilon \right) = 0.
\end{align}$$
</blockquote>

* This is a definition that is basically the same as the previous "continuity in probability" seen earlier;
  
  * i.e., the probability of observing a non-zero jump  between the sequence limit and the random variable $X$, becomes increasingly small, limiting to zero.
  
  * In particular, if the CDF of $X_n$ is given by $P_n$, and $X\sim P$, then the above implies that $P_n\rightarrow P$.


========================================================
### Modes of convergence

* A stronger notion of convergence, that is similar to deterministic convergence, is the following:

<blockquote>
<b>Almost sure convergence </b><br>
A random sequence, $X_n$, is said to <strong>converge almost surely, or with
probability 1</strong>, to a random variable $X$ if 
$$\begin{align}
\lim_{n\rightarrow \infty} \vert X_n - X\vert = 0 
\end{align}$$
except on a set $A_0$ of probability zero, i.e., $\mathcal{P}(A_0)=0$.
</blockquote>

* This explicitly requires that, with probability one, the limiting random variable attains the identical realization of $X$.

   * This is qualitatively different than saying, the probability of seeing a discrepancy shrinks in the limit.

* These two above definitions are useful to introduce now, as they are similar to the notions we will use in numerical simulation of SDEs later of "weak" and "strong" convergence.

* Finally, the mode of convergence we will use when considering the It&ocirc; and Stratonovich integrals defined earlier is the following, mean-square convergence.

<blockquote>
<b>Mean-square convergence</b><br>
Let $X_k$ be a random sequence such that $\mathbb{E}\left[X_k^2 \right] < \infty$ and let $X$ be a random variable such that $\mathbb{E}\left[X^2\right] <\infty$. The sequence, $X_k$, is said to <strong>converge in the mean-square</strong> to $X$ if
$$\begin{align}
\lim_{k\rightarrow \infty}\mathbb{E}\left[\left(X_k - X\right)^2 \right] = 0.
\end{align}$$
</blockquote>

* This is actually a <b>stronger notion of convergence than convergence in probability</b>, but <strong>weaker than almost sure convergence</strong>.

 * That is, mean-square convergence implies convergence in probability, but not almost-sure convergence.

========================================================
## An example of the It&ocirc; integral of a Wiener process

* Let's consider an example case of the It&ocirc; integral for which $h(t):=W_t$, i.e., the It&ocirc; integral of a Wiener process.

* For this case, the It&ocirc; integral can be evaluated analytically because
  
  $$\begin{align}
  &\sum_{j=0}^{N-1} W_{t_j}\left(W_{t_{j+1}} - W_{t_j}\right)\\
  =&\sum_{j=1}^{N-1} \frac{1}{2} \left[W_{t_{j+1}}^2 - W_{t_j}^2 -\left(W_{t_{j+1}} - W_{t_{j}} \right)^2 \right]\\
  =&\frac{1}{2} \left(W_T^2 - W_0^2 \right) - \frac{1}{2}\sum_{j=0}^{N-1} \left(W_{t_{j+1}} - W_{t_j} \right)^2
  \end{align}$$

* Assume that the time-discretization is uniform such that $\mathrm{d}t := t_{j+1} - t_{j}$ for all $j$.

* Then, for $\mathrm{d}W_j := W_{t_{j+1}} - W_{t_j}$ we have that 

  $$\begin{align}
  \mathbb{E}\left[\left(\mathrm{d}W_j\right)^2 \right] = \mathrm{d}t
  \end{align}$$
  by definition of the Wiener process.
  
* In the <b>"mean-square algebra"</b>, we then write identically $\left(\mathrm{d}W_j\right)^2 \equiv \mathrm{d}t$, though the formal mathematics is suppressed here.

 
========================================================
### An example of the It&ocirc; integral of a Wiener process

* From the last slide, we thus obtain

  $$\begin{align}
  \sum_{j=0}^{N-1} \left(W_{t_{j+1}} - W_{t_j}\right)^2  =  \sum_{j=0}^{N-1} \left(\mathrm{d}W_j\right)^2= \sum_{j=0}^{N-1} \mathrm{d}t = T
  \end{align}$$

* Noting that, by definition, $W_0 \equiv 0$, we thus obtain

  $$\begin{align}
  \int_{0}^T W_t \mathrm{d}W_t = \frac{1}{2} \left( W_T^2 - T\right).
  \end{align}$$
  
* In the above, it is important to remember that this is an equality in the mean-square sense, such that both sides represent random variables for which
  * on the <b>left-hand-side</b>, the <strong>limit over the partition gives a sequence</strong> such that;
  * the <strong>expected value of the square difference</strong> with the <b>right-hand-side</b> <strong>equals zero</strong>.
  
* Therefore, we say that the It&ocirc; integral on the left hand size converges in the mean-square sense to 0.5 times the square of a Gaussian random variable, with mean zero and variance $T$, plus the constant $T$.

* This convergence of the integral to a random variable shows some of the subtlety of working with SDEs.


========================================================
## It&ocirc; Taylor expansion
* Recall that in deterministic calculus, the fundamental theorem of calculus tells us that

  $$\begin{align}
  f(b) - f(a) = \int_{a}^b \frac{\mathrm{d}}{\mathrm{d}t}f(t) \mathrm{d}t
  \end{align}$$
  provided such a derivative exists over the interval.
  
* For a perturbation in time $\mathrm{d}t$, we will write $\mathrm{d}W_t := W_{t+\mathrm{d}t} - W_t$, such that we obtain a second-order approximation

  $$\begin{align}
  f(W_t - \mathrm{d}W_t) - f(W_t) = f'(W_t)\mathrm{d}W_t + \frac{1}{2} f''(W_t)\mathrm{d}W_t^2  + \mathcal{O}\left( W_t^3\right)
  \end{align}$$

* If we integrate the above over the interval $[0,T]$, using the mean-square algebra, we obtain the first It&ocirc; lemma as

  $$\begin{align}
  f(W_T) - f(W_0) = \int_{0}^T f'(W_t)\mathrm{d}W_t + \frac{1}{2}\int_{0}^T f''(W_t) \mathrm{d}t.
  \end{align}$$
  
* An important difference between <b style="color:#d95f02">deterministic calculus</b> and <b style="color:#1b9e77">It&ocirc; calculus</b> is thus given in the above; 
   
   * the difference of the function $f$ evaluated at the two times of the Wiener process is thus given by the <b style="color:#1b9e77">It&ocirc; stochastic integral</b> of $f'(W_t)$ over the interval; and
   
   * added to an additional <b style="color:#d95f02">Riemann integral in the second derivative</b> of $f$.

* The mathematics of this are, again, quite complicated but we often will use these equations as identities while suppressing the details.

========================================================
### It&ocirc; Taylor expansion

* Consider the It&ocirc; formula from the last slide

  $$\begin{align}
  f(W_T) - f(W_0) = \int_{0}^T f'(W_t)\mathrm{d}W_t +\frac{1}{2} \int_{0}^T f''(W_t) \mathrm{d}t.
  \end{align}$$
  
* If we set $f(t):= t^2$, then $f'(t) =2t$ and $f''(t)=2$, so that

  $$\begin{align}
  & W_T^2 - W_0^2 = 2 \int_{0}^T W_t\mathrm{d}W_t + \int_{0}^T\mathrm{d}t\\
  \Leftrightarrow & \int_{0}^T W_t \mathrm{d}W_t = \frac{1}{2}\left(W_T^2 - T\right).
  \end{align}$$
  as discussed earlier.
  
* This shows how, in part, we can formally manipulate stochastic integrals with It&ocirc;'s formula, even when we suppress the details.

* Additional extensions of the It&ocirc;-Taylor expansion provide a number of rules to formally work with SDEs.

* The details of these expansions and results can be found in a more formal course on the subject.

========================================================
## It&ocirc; versus Stratonovich

* There is a direct relation between the <b style="color:#1b9e77">It&ocirc;</b> and <b style="color:#d24693">Stratonovich</b> integrals of a smooth
function $f$, which is given by

  $$\begin{align}
  \int_0^T f(W_t)\circ \mathrm{d}W_t = \int_0^T f(W_t)\mathrm{d}W_t + \frac{1}{2}\int_0^Tf'(W_t)\mathrm{d}t.
  \end{align}$$

* This is to say that the <b style="color:#d24693">Stratonovich</b> is given by the <b style="color:#1b9e77">It&ocirc;</b> integral, plus a term of a <b style="color:#d95f02">Riemann</b> integral on the right.

* Using this relationship, we can show that the Stratonovich integral is actually formally more similar to the deterministic integral than the It&ocirc; integral.

* If we define $f(t)=g'(t)$, then the It&ocirc;-Taylor expansion similarly gives

  $$\begin{align}
  &g(W_T) - g(W_0) = \int_{0}^T g'(W_t)\mathrm{d}W_t +\frac{1}{2} \int_{0}^T g''(W_t) \mathrm{d}t\\
  \Leftrightarrow & g(W_T) - g(W_0) = \int_0^T f(W_t) \mathrm{d}W_t + \frac{1}{2}\int_0^T f'(W_t)\mathrm{d}t\\
  \Leftrightarrow &g(W_T) - g(W_0) = \int_0^T g'(W_t)\circ \mathrm{d}W_t 
  \end{align}$$
  so that this looks like the deterministic fundamental theorem of calculus.

*  However, <b style="color:#d24693">Stratonovich calculus</b> is also subtle to work with, as the <b style="color:#d24693">midpoint rule</b> that defines the integral <strong>implicitly relies on future information</strong> for the value of the function, unlike the  <b style="color:#1b9e77">It&ocirc;</b> formulation.
  
  
