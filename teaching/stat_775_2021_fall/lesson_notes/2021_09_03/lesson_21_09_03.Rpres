<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>


A review of vector calculus and concepts in optimization
========================================================
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

<ul>
  <li>The following topics will be covered in this lecture:</li>
  <ul>
    <li>Concepts in analytical and numerical differentiation</li>
    <li>Newton's method in one variable</li>
    <li>Tangent vectors, tangent spaces and vector fields</li>
    <li>The Jacobian, the inverse function theorem and Newton's method in multiple variables</li>
    <li>Gradients, Hessians, and concepts in optimization</li>
  </ul>
</ul>


========================================================
### Concepts in analytical differentiation

<div style="float:left; width:45%">
<img style="width:100%", src="tangent.png" alt="Tangent line approximation by derivative."/>
<p style="text-align:center;">
Courtesy of   <a href="https://commons.wikimedia.org/wiki/File:Tangent-calculus.svg">Pbroks13</a>, <a href="http://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a>, via Wikimedia Commons
</p>
</div>
<div style="float:left; width:55%">
<ul>
  <li>The derivative represents the slope of a tangent line to a curve.</li>
  <li>In the figure to the left, we see the function <b style="color:#008080">$f$ represented by the blue curve</b>.</li>
  <li>The <b>derivative $f'(x)$</b> at a given point gives the infinitesimal rate of change at that point with respect to small changes in $x$, denoted $\delta_x$.</li>
  <li>Suppose we have a point $x_0$, a nearby point that differs by only a small amount in $x$
  $$x_1 = x_0+\delta_{x_1},$$</li>
  <li> The function 
  $$f(x_1) \approx f(x_0) + f'(x_0)\delta_{x_1}$$
  is what is known as the <b style="color:#663300">tangent line approximation</b> to the function $f$.</li>
  <li>Such an approximation exists when <strong>$f$ is sufficiently smooth</strong> and is <strong>accurate when $\delta_{x_1}$ is small</strong>, so that the difference of $x_1$ from the fixed value $x_0$ is small.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
   <li>We can see graphically how the approximation becomes worse as we take $\delta_{x_1}$ too large.</li>
</ul>
</div>

========================================================
### Concepts in analytical differentiation

* More generally, the tangent line approximation is one kind of <b>general Taylor approximation</b>.

* Suppose we have a point $x_0$ fixed, and define $x_1$ as a small perturbation
  $$x_1 = x_0+\delta_{x_1},$$

* If a function <strong>$f$ has $k$ continuous derivatives</strong> we can write
 $$f(x_1) =  f(x_0) + f'(x_0)\delta_{x_1} + \frac{f''(x_0)}{2!}\delta_{x_1}^2 + \cdots + \frac{f^{(k)}(x_0)}{k!} \delta_{x_1}^k + \mathcal{O}\left(\delta_{x_1}^{k+1}\right)$$
 
* The $\mathcal{O}\left(\delta_{x_1}^{k+1}\right)$ refers to terms in <b>the remainder</b>, that <strong>grows or shrinks like the size of the perturbation to the power $k+1$</strong>.
 
  * This is why this approximation works well when $\delta_{x_1}$ is a small perturbation.

* Another important practical example of using this Taylor approximation, when the function $f$ has two continuous derivatives, is
   $$f(x_0 + \delta_{x_1}) \approx f(x_0) + f'(x_0)\delta_{x_1} + f''(x_0) \frac{\delta_{x_1}^2}{2}$$
  which will be used shortly for obtaining solutions to several kinds of equations. 

* Particularly, this is strongly related to our <strong>second derivative test from univariate calculus</strong>.

========================================================
## An approach to numerical derivation

* At the moment, we consider how <b>Taylor's expansion</b> can be used at first order again to <strong>approximate the derivative</strong>.

* Recall, we write

  $$\begin{align}
  f(x_1) &= f(x_0) + f'(x_0) \delta_{x_1} + \mathcal{O}\left( \delta_{x_1}^2\right) \\
  \Leftrightarrow  \frac{f(x_1) -  f(x_0)}{ \delta_{x_1}} &= f'(x_0) + \mathcal{O}\left( \delta_{x_1}\right)
  \end{align}$$
  
* This says that for a small value of $\delta_{x_1}$, we can obtain the <b>numerical approximation of $f'(x_0)$</b> <strong>proportional to the accuracy of the largest decimal place of $\delta_{x_1}$</strong> by the difference on the left hand side.

* This gives a <b>forward finite difference equation</b> approximation to the derivative.

* We can similarly define a <b>backward finite difference equation</b> with $\pmb{x}_1 := \pmb{x}_0 -\pmb{\delta}_{\pmb{x}_1}$.

* In each case, we <b>use the perturbation</b> to <strong>parameterize the tangent-line approximation</strong>.


========================================================
## Newton's method in one variable

* We have seen earlier the basic linear inverse problem,

  $$\begin{align}
  \mathbf{A}\pmb{x} = \pmb{b}
  \end{align}$$
  where $\pmb{b}$ is an observed quantity and $\pmb{x}$ are the unknown variables related to $\pmb{b}$ by the relationships in $\mathbf{A}$.
  
  * We observed that a <b>unique solution exists</b> when all the relationships expressed by the columns are unique, <strong>corresponding to all non-zero eigenvalues</strong>.
  
* A similar problem exists when the <strong>relationship between $\pmb{x}$ and $\pmb{b}$ is non-linear</strong>, but we still wish to find some such $\pmb{x}$.

<blockquote>
<b>Nonlinear inverse problem (scalar case)</b><br>
Suppose we know the <strong>nonlinear, scalar function</strong> $f$ that gives a relationship
  $$\begin{align}
  f(x^\ast) = b
  \end{align}$$
  for an <strong>observed $b$ but an unknown $x^\ast$</strong>.  Finding a value of $x^\ast$ that satisfies $f(x^\ast)=b$ is known as a <strong>nonlinear inverse problem</strong>.
</blockquote>
  
* Define a function
  $$\begin{align}
  \tilde{f}(x) = f(x)-b.
  \end{align}$$

* Thus solving the <b>nonlinear inverse problem</b> in one variable is equivalent to finding the appropriate $x^\ast$ for which
  $$\begin{align}
  \tilde{f}(x^\ast)= 0 .
  \end{align}$$
  
* Finding a zero of a function, or <b>root finding</b>, is thus <strong>equivalent to a nonlinear inverse problem</strong>.

* The <b>Newton-Raphson method</b> is one classical approach which has inspired many modern techniques.

========================================================
### Newton's method in one variable

<div style="float:left; width:55%">
<ul>
  <li>We are <strong>searching for the point $x^\ast\in \mathbb{R}$</strong> for which the modified equation <b>$\tilde{f}\left(x^\ast\right) = 0$</b>, and we suppose we have a good initial guess $x_0$.</li>
  <li> We define the tangent approximation as,
  $$t(\delta_x) = \tilde{f}(x_0) + \tilde{f}'(x_0) \delta_x$$
  for some small perturbation value of $\delta_x$.</li>
  <li> Recall, $\tilde{f}'(x_0)$ refers to the value of the derivative of $\tilde{f}$ at the point $x_0$ -- suppose this value is nonzero.</li>
  <li> In this case, we will <strong>examine where the tangent line intersects zero</strong> to find a better approximation of $x^\ast$.</li>
  <li> Suppose that for $\delta_{x_0}$ we have
  $$\begin{matrix}
  t(\delta_{x_0}) = 0  & 
  \Leftrightarrow & 0= \tilde{f}(x_0) + \tilde{f}'(x_0) \delta_{x_0}  &
  \Leftrightarrow  &\delta_{x_0} = \frac{-\tilde{f}(x_0)}{\tilde{f}'(x_0)}
  \end{matrix}$$
  </li>
  <li>The above solution makes sense <b>as long as $f'(x_0)$ is not equal to zero</b>;</li>
  <ul>
    <li>if not, this says that the <strong>tangent line intersects zero</strong> at
  $x_1 = x_0 + \delta_{x_0}$, giving a new approximation of $x^\ast$.</li>
  </ul>
</ul>
</div>
<div style="float:right; width:40%" class="fragment">
<img style="width:100%", src="NewtonIteration_Ani.gif" alt="Animation of Newton iterations."/>
<p style="text-align:center;">
Courtesy of  <a href="https://commons.wikimedia.org/wiki/File:NewtonIteration_Ani.gif">Ralf Pfeifer</a>, <a href="http://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a>, via Wikimedia Commons
</p>
</div>
<div style="float:left; width:100%">
<ul>
  <li>The process of recursively <b>solving for a better approximation of $x^\ast$ terminates</b> when we reach a certain <strong>tolerated level of error in the solution or the process times out</strong>, failing to converge.</li>
  <li>This method has a direct analog in multiple variables, for which we will need to <strong>extend our notion of the derivative and Taylor's theorem to multiple dimensions</strong>.</li>
</ul>
</div>

========================================================
### Newton's method -- example

* As a quick example, let's consider the Newton algorithm built-in to Scipy.

  * Scipy is another standard library like Numpy, but which contains various scientific methods and solvers rather than general linear algebra.

* Specifically, we will import the built-in `newton` function from the `optimize` sub-module  of `scipy`.

```{python}
from scipy.optimize import newton
```

* In the following, we define the cubic function $f(x):=x^3$, but we are interested in the value $x^\ast$ for which $f\left(x^\ast\right)=1$

  * The augmented function $\tilde{f}(x):= x^3 - 1$ defines the root-finding problem from the nonlinear inverse problem:

```{python}
def f(x): return (x**3 - 1)
```

* The `newton` function can be supplied an analytical derivative, if this can be computed, to improve the accuracy versus, e.g., a finite-differences approximation.

  * In the below, we supply this as a simple lambda function in the arguments of `newton`:

```{python}
root = newton(f, 1.5, fprime=lambda x: 3 * x**2)
root
```


========================================================
## Tangent vectors

* To expand our discussion to <b>multiple variables</b>, we will review some fundamental concepts of vector calculus.

* Suppose we have a <b>vector valued function</b>, with a single argument:

  $$\begin{align}
  \pmb{x}:&\mathbb{R} \rightarrow \mathbb{R}^{N};\\ 
  \pmb{x}(t) :=& \begin{pmatrix} x_1(t) & \cdots & x_{N}(t)\end{pmatrix}^\top;
  \end{align}$$
  
  * prototypically, we will think of $\pmb{x}(t)$ as a curve in state-space, with its position at each time $t\in\mathbb{R}$ defined by the equation above.
  
<blockquote>
<b>Tangent vector</b><br>
Suppose $\pmb{x}(t)$ is defined as above and that each of the component functions $x_i(t)$ are differentiable.  The <strong>tangent vector</strong> to the state trajectory $\pmb{x}$ is defined as
  $$\vec{x}:= \frac{\mathrm{d}}{\mathrm{d}t} \pmb{x}:= \begin{pmatrix}\frac{\mathrm{d}}{\mathrm{d}t}  x_1(t) & \cdots & \frac{\mathrm{d}}{\mathrm{d}t}  x_{N}(t)\end{pmatrix}^\top$$
</blockquote>

* In the above, the interpretation of the <strong>derivative defining a tangent line</strong> is extended into multiple variables;

  * in this case, the <b>tangent line is embedded in a higher-dimensional space</b> of multiple variables.

========================================================
### Tangent spaces

<div style="float:left; width:60%">
<ul>
  <li>An important extension of the tangent vector is the notion of the <b>tangent space</b>;</li>   
  <ul>
    <li>this can be defined in terms of <strong>all differential perturbations generated at a point</strong>:</li>
  </ul>
  <blockquote>
  <b>Tangent spaces</b><br>
  Let $\pmb{x}\in\mathbb{R}^{N}$ and $\gamma(t)$ be an arbitrary differentiable curve $\pmb{\gamma}:\mathbb{R}\rightarrow \mathbb{R}^{N}$ such that $\pmb{\gamma}(0)= \pmb{x}$ with a tangent vector defined as $\vec{\gamma}(0):= \frac{\mathrm{d}}{\mathrm{d}t}|_0 \pmb{\gamma}$.  The <strong>tangent space</strong> at $T_{\pmb{x}}$ is defined by the linear span of all tangent vectors as such through $\pmb{x}$.
  </blockquote>
  <li>In the above, we consider only the simplest definition of the tangent space;</li>
  <ul>
    <li>in this case the tangent space, $T_{\pmb{x}} \equiv \mathbb{R}^{N}$, is simply the space of all perturbations to the point $\pmb{x}$.</li>
    <li>However, this idea is extended into far greater generality:</li>
  </ul>
</ul>
</div>
<div style="float:left; width:40%; text-align:center" class="fragment">
<img src="Tangentialvektor.png" style="width:100%" alt="Tangent plane representation">
<p>Courtesy of TN, Public domain, via <a href="https://commons.wikimedia.org/wiki/File:Tangentialvektor.svg">Wikimedia Commons</a></p>
</div>
<div style="float:left; width:100%">
<ul>
  <li>A <b>"differentiable manifold"</b> is a space that looks <strong>"locally" like $\mathbb{R}^{N}$</strong>, including, e.g., curved hyper-surfaces.</li>
  <li>The notion of the tangent plane thus is extended by using calculus as usual on $\mathbb{R}^{N}$ <b>locally on a differentiable manifold</b>.</li>
  <li>We can imagine as in the figure above, the tangent plane being defined by <strong>all tangent vectors for differentiable curves that are defined in the manifold</strong>.</li>
  <li> The tangent plane thus gives a <b>general linear approximation</b> to what the manifold looks like (in all directions) up to small perturbations.</li>
</ul>
</div>

========================================================
### Vector fields

<div style="float:left; width:70%">
<ul>
  <li>The tangent space construction gives us the ability to define arbitrary vector fields.</li>   
  <blockquote>
  <b>Vector fields</b><br>
  For a smooth manifold $M$, a <strong>vector field (or flow field)</strong> is a mapping from $M$ to the tangent space $TM$ such that for each point $\pmb{x}\in M$, this defines a the tangent vector $\vec{\gamma}$ for a curve $\pmb{\gamma}$ that passes through $\pmb{x}$.
  </blockquote>
  <li>In the above, we again consider a simple, intuitive version of this concept;</li>
  <ul>
    <li>this idea can again be extended into far greater generality.</li>
  </ul>
  <li>However, this gives a good picture of how we will use vector fields to <strong>define equations of motion and state space trajectories</strong>.</li>
  <li>A vector field can for instance <b>define the velocity vector</b> (instantaneous rate of change and direction) for a particle of fluid in the atmosphere.</li>
  <li>The figure to the right where we can imagine that the sphere represents the surface of the Earth with the <strong>vector field defining the equations of motion</strong>:</li>
</ul>
</div>
<div style="float:left; width:30%; text-align:center" class="fragment">
<img src="Vector_sphere.png" style="width:100%" alt="Vector field on a sphere">
<p>Courtesy of I, Cronholm144, <a href="http://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:Vector_sphere.svg">Wikimedia Commons</a></p>
</div>
<div style="float:left; width:100%">
<ul>
  <li>A curve $\gamma(t)$ can be defined in time as the <strong>evolution of the particle</strong> where at any given point, its tangent vector $\vec{\gamma}(t)$ will be equal to the vector field mapping.</li>
  <li>More abstractly, we can consider a curve $\pmb{x}(t):\mathbb{R}\rightarrow \mathbb{R}^{N}$ to <strong>represent all time-evolving states in a dynamical model</strong>;</li>
  <ul>
    <li>this can represent, e.g., the temperature, pressure and humidity in the atmosphere at all grid points defined with a discretized sphere.</li>
  </ul>
  <li>As we move forward, we will be interested in <strong>how to "track" the trajectory in the state space</strong>, knowing the principles of its evolution, a probability density on its current / past values and having random observations.</li>
</ul>
</div>

========================================================
### The Jacobian

*  Not all functions in multiple variables have a single input -- we can instead write
  
  $$\begin{align}
  \pmb{f} : \mathbb{R}^N & \rightarrow \mathbb{R}^M\\
  \pmb{x} &\rightarrow \begin{pmatrix} f_1\left(\pmb{x}\right) \\ \vdots \\ f_M(\pmb{x})\end{pmatrix}
  \end{align}$$
  
* In the above, each of the component-functions $f_i$ for $i=1,\cdots , M$ is a function defined
  
  $$\begin{align}
  f_i :\mathbb{R}^N &\rightarrow \mathbb{R} \\
  \pmb{x} &\rightarrow f_i (\pmb{x})
  \end{align}$$

<blockquote>
<b>The Jacobian</b><br>
For a vector-valued, continuously differentiable function $\pmb{f}$ as above, the <strong>Jacobian</strong> is defined as the matrix of first partial derivatives
  $$\begin{align}
  \nabla \pmb{f} &=  
  \begin{pmatrix}
  \partial_{x_1} f_1 & \partial_{x_2} f_1 & \cdots & \partial_{x_n} f_1 \\
  \partial_{x_1} f_2 & \partial_{x_2} f_2 & \cdots & \partial_{x_n} f_2 \\
  \vdots &  \vdots & \ddots & \vdots \\
  \partial_{x_1} f_m & \partial_{x_2} f_m & \cdots & \partial_{x_n} f_m
  \end{pmatrix}\in\mathbb{R}^{m\times n}
  \end{align}$$
  where the partials are with respect to the components of $\pmb{x}$.
</blockquote>

========================================================
### The Jacobian

* The Jacobian is also a <strong>tangent-linear approximation</strong>, taking into account perturbations in all directions of $\pmb{x}$.

* This also gives a version of <b>Taylor's theorem</b> where we can write a <strong>tangent-linear approximation</strong> of a mapping.

<blockquote>
<b>Tangent-linear approximation</b><br>
Let $\pmb{f}:\mathbb{R}^N \rightarrow \mathbb{R}^M$ in be a continuously differentiable mapping and $\pmb{x}_1 = \pmb{x}_0 + \pmb{\delta}_{\pmb{x}_1}$ be a purturbation within a sufficiently small neighborhood.  Then, the tangent-linear approximation for $\pmb{f}(\pmb{x}_1)$ is given as
  $$\begin{align}
  \pmb{f} \left(\pmb{x}_1\right) 
  & = \pmb{f} \left(\pmb{x}_0 + \pmb{\delta}_{\pmb{x}_1}\right) \\
  & \approx \pmb{f}(\pmb{x}_0) + \nabla \pmb{f}|_{\pmb{x}_0} \pmb{\delta}_{\pmb{x}_1}.
  \end{align}$$
</blockquote>

* The <b>tangent-linear approximation</b> above gives an approximation of the image of $\pmb{f}$ with the <strong>tangent space based at $\pmb{f}(\pmb{x}_0)$</strong>, parameterized by the perturbation $\pmb{\delta}_{\pmb{x}_1}$.

* In particular, the Jacobian is a mapping defined

  $$\begin{align}
  \nabla \pmb{f}|_{\pmb{x}} :T_{\pmb{x}} \rightarrow T_{\pmb{f}(\pmb{x})}
  \end{align}$$
  
  between the tangent space at the input and the tangent space at the output.
  
* The <b>tangent space</b> is also a <strong>linear space</strong> by construction, thus giving the "linear" approximation.

* This is also generalized in a local sense with mappings between differential manifolds and their tangent spaces.



========================================================
## Inverse function theorem

* The Jacobian furthermore gives the extension of the nonlinear inverse problem to multiple dimensions.

<blockquote>
<b>Nonlinear inverse problem (multivariate case)</b><br>
Suppose we know the <strong>nonlinear, multivariate function</strong> $\pmb{f}$ that gives a relationship
  $$\begin{align}
  \pmb{f}(\pmb{x}^\ast) = \pmb{b}
  \end{align}$$
  for an <strong>observed $\pmb{b}$ but an unknown $\pmb{x}^\ast$</strong>.  Finding a value of $\pmb{x}^\ast$ that satisfies $\pmb{f}(\pmb{x}^\ast)=\pmb{b}$ is known as a <strong>nonlinear inverse problem</strong>.
</blockquote>

* An extremely important result from vector calculus establishes a local notion of invertibility for nonlinear inverse problems as above:

<blockquote>
<b>The inverse function theorem</b><br>
Let $\pmb{f}:\mathbb{R}^N \rightarrow \mathbb{R}^N$ be a nonlinear function such that for $\pmb{x}^\ast$, $\nabla \pmb{f}|_{\pmb{x}^\ast}\in\mathbb{R}^{N \times N}$ exists and is an invertible matrix, and it is a continuous function at $\pmb{x}^\ast$.  Then there exists a neighborhood $\mathcal{N}$ containing the image $\pmb{f}\left(\pmb{x}^\ast\right)$ for which any $\pmb{p}\in\mathcal{N}$ has a <strong>unique inverse value</strong> $\pmb{q}$ where
$$\begin{align}
\pmb{p}:= \pmb{f}(\pmb{q}).
\end{align}$$
I.e., $\pmb{f}^{-1}$ exists on $\mathcal{N}$ such that $\pmb{f}^{-1}\circ \pmb{f}= \mathbf{I}_N$ within the properly defined domain.
</blockquote>

* In particular, the inverse function theorem motivates the extension of Newton's method to multiple variables. 


========================================================
### Multivariate Newton

* The <b>Newton-Raphson</b> method can now be restated in terms of <strong>multiple variables</strong> as follows.

* Suppose that we have a <b>nonlinear inverse problem</b> stated as follows:

$$\begin{align}
\pmb{f} :\mathbb{R}^N &\rightarrow \mathbb{R}^N  \\
          \pmb{x} & \rightarrow \pmb{f}(\pmb{x}) \\
          \pmb{f}\left(\pmb{x}^\ast\right)& = \pmb{b}
\end{align}$$

* We redefine this in terms of the <b>adjusted function</b> $\tilde{\pmb{f}}\left(\pmb{x}^\ast\right) = \pmb{f}\left(\pmb{x}^\ast\right) - \pmb{b} = \pmb{0}$, and we wish to make the same first order approximation as before.

* Supposing we have a <b>good initial guess</b> $\pmb{x}_0$ for $\pmb{x}^\ast$, we look for the point where the <strong>tangent-linear approximation equals zero</strong>, i.e.,
$$\begin{align}
 0 &= \pmb{f}\left(\pmb{x}_0\right) + \nabla \pmb{f}\left(\pmb{x}_0\right) \pmb{\delta}_{\pmb{x}_1} \\
\Leftrightarrow \pmb{\delta}_{\pmb{x}_1} &= -\left(\nabla \pmb{f}\left(\pmb{x}_0\right) \right)^{-1} \pmb{f}\left(\pmb{x}_0\right). 
\end{align}$$

* The above makes sense as an approximation as long as $\left(\nabla \pmb{f}\left(\pmb{x}_0\right) \right)^{-1}$ exists, i.e., as long as <strong>the Jacobian has no zero eigenvalues</strong>.

* We can thus once again <b>update the approximation recursively</b> so that $\pmb{x}_1 = \pmb{x}_0 + \pmb{\delta}_{\pmb{x}_1}$, <strong>as long as the inverse exists</strong>.

* This update continues until an error tolerance is reached or the optimization times out.


========================================================
## Concepts in optimization

* A related notion to the inverse problem is <strong>the maximization or minimization (optimization) of functions</strong>.

* <b>Optimization problems</b>, contain two components:
  1. an <b style="color:#d95f02">objective function $f(\pmb{x})$</b>; and
  2. <b style="color:#1b9e77">constraints $g(\pmb{x})$</b>.
  
* E.g., we may wish to <b style="color:#d95f02">optimize factory output $f(x)$</b> as a function of hours $x$ in a week, with a measure of our <b style="color:#1b9e77">active machine-hours $g(x)$ not exceeding a pre-specified limitation $g(x)\leq C$</b>.
  
* Optimization problems can thus be classified into two categories:
  
  * If there are <b style="color:#1b9e77">constraints $g(\pmb{x})$</b> affiliated with the objective function $f(\pmb{x})$, then it is a <b style="color:#1b9e77">constrained optimization problem</b>, <strong>otherwise, it is a unconstrained optimization problem</strong>.
  
* We will focus on the simpler <strong>unconstrained optimization</strong>; this is formulated as the following problem,

  $$\begin{align}
  f: \mathbb{R}^N \rightarrow \mathbb{R}& &
     \pmb{x} \rightarrow  f(\pmb{x}) & &
  f(\pmb{x}^\ast) = \mathrm{max}_{\pmb{x} \in \mathcal{D}} f
  \end{align}$$


* We note that the above problem is equivalent to a minimization problem by a substitution of $\tilde{f} = -f$, i.e.,

  $$\begin{align}
  \tilde{f}: \mathbb{R}^N \rightarrow \mathbb{R}& &
     \pmb{x} \rightarrow  -f(\pmb{x})& &
  f(\pmb{x}^\ast) = \mathrm{max}_{\pmb{x} \in \mathcal{D}} f  & & \tilde{f}(\pmb{x}^\ast)= \mathrm{min}_{\pmb{x}\in \mathcal{D}} \tilde{f}
  \end{align}$$

========================================================
### Concepts in optimization

<div style="float:left; width:60%">
<ul>
  <li> Because these problems are equivalent, we <strong>focus on the minimization of functions</strong> as they are traditionally phrased in optimization.</li>
  <li>The same techniques will apply for maximization by a simple change of variables.</li>
  <li> We will need to identify a few key concepts: <b style="color:#1b9e77">global</b> and <b style="color:#d95f02">local</b> minimizers.</li>
  <li> Suppose we are trying to minimize an <b>objective function</b> $f$.</li>
  <li>We would ideally find a <b style="color:#1b9e77">global minimizer of $f$</b>, a point where the function attains its least value over all possible values under consideration:
<blockquote>
<b>Global minimizer</b><br>
A point $\pmb{x}^\ast$ is a <strong>global minimizer</strong> if $f(\pmb{x}^\ast) \leq f(\pmb{x})$ for all other possible $\pmb{x}$ in the domain of consideration $D\subset \mathbb{R}^n$.
</blockquote></li>
  <li> A <b style="color:#1b9e77">global minimizer can be difficult to find</b>, because our <b style="color:#d95f02">knowledge of $f$ is usually only local</b>;</li>
</ul>
</div>
<div style="float:left; width:40%" class="fragment">
<img style="width:100%", src="difficult_minimization.png" alt="A difficult global minimization."/><p style="text-align:center"> Courtesy of: J. Nocedal and S. Wright. <i>Numerical optimization</i>. Springer Science & Business Media, 2006.</p>
</div>
<div style="float:left; width:100%">
<ul>
  <ul>
    <li>i.e., we only can approximate the behavior of the function $f$ within <b style="color:#d95f02">small perturbations $\pmb{\delta}_{x}$ of values $\pmb{x}$</b> where we already know $f(\pmb{x})$.</li>
  </ul>
  <li>Since our algorithm <strong>hopefully does not need to compute $f$ over many points</strong>, we usually do not have a good picture of the overall shape of $f$,</li>
  <ul>
    <li>generally, we can never be sure that the function does not take a sharp dip in some region that has not been sampled by the algorithm.</li> 
  </ul>
</ul>
</div>

========================================================

### Local minima and convexity

<div style="float:left; width:65%">
<ul>
  <li>The difficulty of finding a global minimum means that we will generally need to handle local minima:</li>
  <blockquote>
  <b>Local minimizer</b><br>
  A point $\pmb{x}^\ast$ is a <strong>local minimizer</strong> if there exists some neighborhood $\mathcal{N}\subset D$ containing $\pmb{x}^\ast$ such that $f(\pmb{x}^\ast) \leq f(\pmb{x})$ for all other possible $\pmb{x} \in \mathcal{N}$.
  </blockquote>
  <li>Throughout mathematics, the notion of convexity is a powerful tool, often used in optimization for understanding local minima.</li>
  <li>A <b>function is convex</b> if and only if <strong>the region above its graph is a convex set</strong>.</li>
</div>
<div style="float:left; width:25%; text-align:center" class="fragment">
<img style="width:75%", src="convex_super_graph.png" alt="Image of the upper region enclosed by a convex function."/><p style="text-align:center"> Courtesy of: Oleg Alexandrov. Public domain, via <a href="https://commons.wikimedia.org/wiki/File:Convex_supergraph.svg" target="blank">Wikimedia Commons</a>.</p>
</div>
<div style="float:left; width:100%">
  <ul>
    <li>The <b>convexity of the full epigraph</b> set means that the function attains a <b style="color:#1b9e77">global minimum over its entire domain</b>.</li>
  </ul>
  <li>In <b style="color:#d95f02">non-convex functions</b>, we can have regions that are also <b style="color:#d95f02">locally convex</b> in the graph of the function.</li>
  <li>For such regions, we can find <b style="color:#d95f02">local minimizers</b> as defined above.</li>
  <li>In a single variable, this is phrased in terms of the <b>second derivative test</b>.</li>
<blockquote>
<b>Second derivative test</b><br>
  For the function of one variable $f(x)$ we say that $x^\ast$ is a local minimizer if $f'(x^\ast)=0$ and $f''(x^\ast)&gt; 0$.
  </blockquote>
  <li>There is a direct analogy for a function of multiple variables, but this need to be rephrased slightly.</li>
  <li>We introduce the tools as follows.</li>
</ul>
</div>





========================================================
### The gradient

* We suppose that the objective function takes multivariate inputs and gives a scalar output:
 
 $$\begin{align}
 f:&\mathbb{R}^N \rightarrow \mathbb{R}\\
   &\pmb{x}\mapsto f(\pmb{x})
 \end{align}$$

* Formally we will write the <b>gradient</b> as follows, using the same $\nabla$ notation

<blockquote>
<b>The gradient</b><br>
Suppose $f$ is a contiuously differentiable objective function defined as above, then the gradient is given as
  $$\begin{align}
  \nabla f = \begin{pmatrix} \partial_{x_1}f  & \partial_{x_2} f  & \cdots & \partial_{x_N} f \end{pmatrix}^\top \in \mathbb{R}^N
  \end{align}$$
  where the above partial derivatives are with respect to the components of $\pmb{x}\in\mathbb{R}^N$.
</blockquote>
  
* An important property of the <b>gradient</b> is that it gives the tangent vector for a curve in the <strong>direction and velocity of greatest accent</strong> for the output of $f$.

* We also use the gradient similar to previous cases as a linear approximation for the multivariate-input, scalar-output function $f(\pmb{x})$.
  
* Let $\pmb{x}_0\in \mathbb{R}^n$ be some vector and $\pmb{x}_1 = \pmb{x}_0 + \pmb{\delta}_{x_1}$, where $\pmb{\delta}_{x_1}$ is now a vector of small perturbations.

* At <b>first order, the Taylor series</b> is given as
  
  $$f(\pmb{x}_1) = f(\pmb{x}_0) + \left(\nabla f(\pmb{x}_0)\right)^\top \pmb{\delta}_{x_1} + \mathcal{O}\left(\parallel \pmb{\delta}_{x_1}\parallel^2\right);$$
  
  * note, the scalar output is given by the inner product of the gradient with the perturbation.


========================================================
### The Hessian

* A similar <strong>second order approximation</strong> to the one we developed for univariate function $f(x)$,
  $$f(x_1) \approx f(x_0) + f'(x_0)\delta_{x_1}  + f''(x_0)\frac{\delta_{x_1}^2}{2},$$
  can be developed for the multivariate-input case.
  
* In order to do so, we will need to introduce the idea of the <b>Hessian as the second multivariate derivate of $f$</b>.

<blockquote>
<b>The Hessian</b><br>
Suppose that $f$ is a scalar-output function of multiple variables,
  $$\begin{align}
  f: \mathbb{R}^n & \rightarrow \mathbb{R} \\
  \pmb{x} &\rightarrow f(\pmb{x})
  \end{align}$$ 
with continuous second derivatives.  The <strong>Hessian matrix</strong> for the function $f$ is defined as $\mathbf{H}_f$
  $$\begin{align}
  \mathbf{H}_{f} = 
  \begin{pmatrix}
  \partial_{x_1}^2 f & \partial_{x_1}\partial_{x_2} f & \cdots & \partial_{x_1}\partial_{x_n}f \\
  \partial_{x_2}\partial_{x_1} f & \partial_{x_2}^2 f & \cdots & \partial_{x_2} \partial_{x_n}f \\
  \vdots & \vdots & \ddots & \vdots \\
  \partial_{x_n}\partial_{x_1} f & \partial_{x_n}\partial_{x_2} & \cdots & \partial_{x_n}^2 f
  \end{pmatrix}.
  \end{align}$$
</blockquote>

* For short, this is often written as $\mathbf{H}_f = \left\{ \partial_{x_i}\partial_{x_j} f\right\}_{i,j=1}^n$ where this refers to the $i$-th row and $j$-th column.

* $\mathbf{H}_f$ can be evaluated at a particular point $\pmb{x}_0$, and notice that $\mathbf{H}_f$ is <strong>always symmetric</strong>.

========================================================
### The Hessian

* As before, let $\pmb{x}_1 = \pmb{x}_0 + \pmb{\delta}_{\pmb{x}_1}$ be given as a <strong>small perturbation</strong> of $\pmb{x}_0$.

* Using the Hessian as defined on the last slide, if $f$ has continuous second order partial derivatives, the <b>Taylor series is given at second order</b> as
  $$\begin{align}
  f(\pmb{x}_1) = f(\pmb{x}_0) + \left(\nabla f(\pmb{x}_0)\right)^\top \pmb{\delta}_{x_1} + \frac{1}{2} \pmb{\delta}_{x_1}^\top\mathbf{H}_f (\pmb{x}_0) \pmb{\delta}_{x_1} + \mathcal{O}\left(\parallel \pmb{\delta}_{x_1}\parallel^3\right)
  \end{align}$$

* Similarly, our second order approximation is defined as follows.

<blockquote>
<b>Second order objective function approximation</b><br>
Let $\pmb{f}$ be a multi-input, scalar-output function with second order continuous derivatives.  We define the second order approximation as
$$\begin{align}
  f(\pmb{x}_1) \approx f(\pmb{x}_0) + \left(\nabla f(\pmb{x}_0)\right)^\mathrm{T} \pmb{\delta}_{x_1}+ \frac{1}{2} \pmb{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\pmb{x}_0) \pmb{\delta}_{x_1}
  \end{align}$$
</blockquote>

*   This approximation above is accurate when the size of the perturbation $\parallel \pmb{\delta}_{x_1}\parallel$ is small.



========================================================

### The second derivative test with the Hessian

<div style="float:left; width:40%">
<ul>
  <li>For a real-valued function of multiple variables,
  $$f:\mathbb{R}^n \rightarrow \mathbb{R},$$
  we define the <strong>second derivative test in terms of the Hessian of $f$</strong>,
  $$\begin{align}
  \mathbf{H}_{f} = 
  \begin{pmatrix}
  \partial_{x_1}^2 f & \cdots & \partial_{x_1}\partial_{x_n}f \\
  \vdots &\ddots & \vdots \\
  \partial_{x_n}\partial_{x_1} f  & \cdots & \partial_{x_n}^2 f
  \end{pmatrix}
  \end{align}$$
  </li>
  <li>Particularly, the <b>spectral theorem</b> says the <strong>Hessian is diagonalizable</strong>. by an orthogonal change of basis</li>
</ul>
</div>
<div style="float:right; width:55%" class="fragment">
<img style="width:100%", src="Paraboloids.png" alt="Different critical points for different Hessian spectrum."/><p style="text-align:center"> Courtesy of: <a href="https://commons.wikimedia.org/wiki/File:Parabol-el-zy-hy-s.svg">Ag2gaeh</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons.</p>
</div>
<div style="float:left; width:100%">
<ul>
  <li>The different combinations of <b>eigenvalues of the Hessian</b> will determine the <strong>local curvature of the graph of $f$</strong> with examples in two dimensions pictured above:</li>
  <ol>
    <li><b>Left:</b> we see a <strong>convex equation</strong> for the simple paraboloid -- around this critical point, the eigen values of $\mathbf{H}_f$ will all be <strong>strictly positive</strong>.</li>
    <li><b>Middle:</b> we see an equation with <strong>no global minimum, but infinitely local minima</strong> -- all points $(x,y)$ with $x=0$ are critical points, but this is convex in only in the $x$ direction and the $y$ direction will correspond to a <strong>zero eigen value</strong> of $\mathbf{H}_f$.</li>
    <li><b>Right:</b> there is a <strong>critical saddle point</strong> at $(0,0)$ but this is <strong>not even a local mimizer</strong> -- here the Hessian $\mathbf{H}_f$ will have <strong>one positive and one negative eigen value</strong>.</li>
  </ol>
  <li>These examples extend into higher dimensions, and generally we say that the <b>function is locally convex</b> at $\pmb{x}^\ast$ when the <strong>Hessian has only positive eigenvalues</strong> at $\pmb{x}^\ast$.</li>
</ul>
</div>

========================================================

### The second derivative test with the Hessian

* We can understand the second derivative test with the Hessian using our <b>second order Taylor approximation</b> as follows.

* Let $\pmb{x}^\ast$ be a critical point and $\pmb{x}_1 = \pmb{x}^\ast + \pmb{\delta}_{x_1}$ be a perturbation of this in the neighborhood $\mathcal{N}$.

* For any <b>critical point</b>, the <strong>gradient is zero like the regular first derivative</strong>.

* Suppose that the <strong>Hessian has only positive eigenvalues</strong> at $\pmb{x}^\ast$, then our <b>second order approximation</b>
  $$\begin{align}
  f(\pmb{x}_1) &\approx f(\pmb{x}^\ast) + \left(\nabla f(\pmb{x}^\ast)\right)^\mathrm{T} \pmb{\delta}_{x_1}+ \frac{1}{2} \pmb{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\pmb{x}^\ast) \pmb{\delta}_{x_1} \\
  &= f(\pmb{x}^\ast) + \frac{1}{2} \pmb{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\pmb{x}^\ast) \pmb{\delta}_{x_1}
  \end{align}$$
  
* Provided $\mathcal{N}$ is a small enough neighborhood, the $\mathcal{O}\left(\parallel \pmb{\delta}_{x_1}\parallel^3\right)$ will remain very small.

* However, $\pmb{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\pmb{x}^\ast) \pmb{\delta}_{x_1}$ <strong>must be positive by the positive eigenvalues of the Hessian</strong>.

* This says for a radius sufficiently small, $\parallel \pmb{\delta}_{x_1}\parallel$, and any perturbation of the point $\pmb{x}^\ast$ defined as $\pmb{x}_1 = \pmb{x}^\ast + \pmb{\delta}_{x_1}$, we have
  $$f(\pmb{x}_1) \geq f(\pmb{x}^\ast).$$

* Therefore, we can identify a <b>local minimizer</b> whenever the <strong>gradient is zero and the Hessian has positive eigenvalues</strong>, due to the local convexity.


========================================================

## Gradient descent vs Newton's descent

<div style="float:left; width:55%">
<ul>
  <li> We noted before that the <b>gradient $\nabla f$</b> is the <strong>direction and the velocity of the greatest rate of increase of the function $f$</strong>.</li>
  <li> There are some good reasons to consider thus following the direction $-\nabla f$ to find a local minimizer.
  <li>However, it is possible that we will overshoot the local minimum if we take too long of a step along this direction.</li>
  <li>We may not know a <strong>good choice for what length of step to use</strong>, for example consider the following figure.</li>
</ul>
</div>
<div style="float:right; width:40%" class="fragment">
<img style="width:100%", src="gradient_descent_edit.png" alt="Gradient descent."/><p style="text-align:center"> Courtesy of: J. Nocedal and S. Wright. <i>Numerical optimization</i>. Springer Science & Business Media, 2006.</p>
</div>
<div sytle="float:left; width:55%">
<ul>
  <li>The contours represent fixed values for the function $f$; i.e.,</li> 
  <li>if a contour is defined $\mathcal{C}$, then for all $\mathbf{c}_0 \in \mathcal{C}$, 
  $$f(\mathbf{c}_0 ) = C$$ 
  for a fixed value $C$.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>For $t &gt; 0$ we define a <b>perturbation along the gradient vector</b> by $\pmb{x}_k - t \nabla f$. </li>
  <li>For most of this direction,
  $$f(\pmb{x}_k - t \nabla f) \leq f(\pmb{x}_k)$$
  as this moves to the inner contours around $\pmb{x}^\ast$.</li>
  <li>However, for $t$ large enough, this is not true and <strong>we do not know by default what size $t$ is appropriate</strong>, or if $f$ is extremely sensitive in this way.</li>
</ul>
</div>

========================================================

### Gradient descent vs Newton's descent


<div style="float:left; width:55%">
<ul>
  <li>Let's recall <b>Newton's method</b> to find the appropriate direction and step length:</li>
  <ul>
    <li> When we were looking for the zero of a function, we used the <strong>first order approximation and found where this function takes the value zero</strong>.</li>
  </ul>
  <li> We will consider a similar idea using the <strong>second order approximation</strong>, 
  $$\begin{align}
  m(\pmb{\delta}_{x_1}) = f(\pmb{x}_0) + \left(\nabla f(\pmb{x}_0)\right)^\mathrm{T} \pmb{\delta}_{x_1}+\frac{1}{2} \pmb{\delta}_{x_1}^\mathrm{T}\mathbf{H}_f (\pmb{x}_0) \pmb{\delta}_{x_1}.
  \end{align}$$</li>
  <li>By setting the derivative of $m$ with respect to the perturbation $\pmb{\delta}_x$ equal to zero for some $\pmb{\delta}_{x_1}$:
  $$\begin{align}
  & 0 =   \nabla f(\pmb{x}_0)  + \mathbf{H}_f(\pmb{x}_0) \pmb{\delta}_{x_1} \\
  \Leftrightarrow & \pmb{\delta}_{x_1} = -\left(\mathbf{H}_f(\pmb{x}_0)\right)^{-1} \nabla f(\pmb{x}_0)
  \end{align}$$</li>
  <li>If $\mathbf{H}_f$ has an <strong>inverse</strong> at $\pmb{x}_0$, and if $\mathbf{H}_f(\pmb{x}_0)$ has <strong>positive eigenvalues</strong>, this gives a <b>descent direction</b> in $f$.</li>
</ul>
</div>
<div style="float:right; width:35%" class="fragment">
<img style="width:100%", src="newton_descent_edit.png" alt="Newton descent."/><p style="text-align:center"> Courtesy of: J. Nocedal and S. Wright. <i>Numerical optimization</i>. Springer Science & Business Media, 2006.</p>
</div>
<div style="float:left; width:55%">
<ul>
  <li>Particularly, our new choice for the estimated minimum will be given by $\pmb{x}_1 = \pmb{x}_0 + \pmb{\delta}_{x_1}$ for which $f(\pmb{x}_0)\geq f(\pmb{x}_1)$.</li>
  <li>Moreover, the <b>second order approximation</b> with Taylor's expansion gives a <strong>second order rate of convergence</strong> to the minimum.</li>
</ul>
</div>


========================================================

### Newton's descent

* In <b>Newton's descent</b>, for each $k$ we repeat the step to define the $k+1$ approximation,
  $$\begin{align}
  & 0 =   \nabla f(\pmb{x}_k)  + \mathbf{H}_f(\pmb{x}_k) \pmb{\delta}_{x_{k+1}} \\
  \Leftrightarrow & \pmb{\delta}_{x_{k+1}} = -\left(\mathbf{H}_f(\pmb{x}_k)\right)^{-1} \nabla f(\pmb{x}_k)\\
  &\pmb{x}_{k+1} = \pmb{x}_k + \pmb{\delta}_{x_k}.
  \end{align}$$


* This process will continue until:
  1. <strong>approximation reaches an error tolerance</strong> (when we have a good initial guess $\pmb{x}_0$); or
  2. it <strong>terminates after timing out</strong>, failing to converge if we are not in an appropriate neighborhood $\mathcal{N}$ of a minimizer $\pmb{x}^\ast$.

* When we are in a <b style="color:#d95f02">locally convex neighborhood $\mathcal{N}$ containing $\pmb{x}^\ast$</b>, we can be assured that $\mathbf{H}_f$ will be invertible, and that it will have strictly positive eigenvalues, making the initial choice very important in producing a result.

* Unlike the gradient vector alone, this <strong>gives a step choice derived by the local geometry</strong>, and this <b>converges at second order</b> as long as the initial choice $\pmb{x}_0$ is in the neighborhood $\mathcal{N}$.

* However, this method does not know if there is a better minimizing solution $\pmb{x}_0^\ast$ that lies in a different neighborhood $\mathcal{N}_0$.

* The biggest issue with Newton's method is that <strong>calculating the Hessian may not be realistic</strong> for a <b>large number of inputs</b>.

* If $N$ is large, then the Hessian has $N^2$ entries, and there may not be any exact expression for $\mathbf{H}_f$ at all.

* <b>Newton's descent</b> is therefore a <strong>basis for a wide class of "quasi-Newton" methods</strong> which typically <strong>approximate the Hessian</strong> in some form.
