========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080
  
========================================================

<h1>Review from last time...</h1>

<h2> Diagnostics Checking error assumptions</h2>

* If we wish to check the assumptions on the error or variation in the signal $\boldsymbol{\epsilon}$, we need to consider, $\boldsymbol{\epsilon}$ itself is not observable.

* Therefore, we will typically use the residuals as a proxy for the true errors to validate our assumptions.


* Assuming that the error $\boldsymbol{\epsilon} \sim N\left(0, \mathbf{I}\sigma^2\right)$, we can derive that
$$\begin{align}
\hat{\boldsymbol{\epsilon}} & \sim N\left( 0, \sigma^2\left(\mathbf{I} - \mathbf{H}\right)\right)
\end{align}$$

  * where $\mathbf{H} = \mathbf{X}\left(\mathbf{X}^\mathrm{T} \mathbf{X}\right)^{-1} \mathbf{X}^\mathrm{T}$

* Typically, we will plot the values of the residuals in the vertical axis (y-axis) $\hat{\boldsymbol{\epsilon}}$, equal to $\mathbf{Y} - \hat{\mathbf{Y}}$

* The horizontal (x-axis) of the plot will be the corresponding fitted value $\hat{\mathbf{Y}}$, <b>or</b> the values of some explanatory variable for the response.

* This will help us determine the existence of a linear/ nonlinear relationship in the error.

* Particularly, when $\boldsymbol{\epsilon}$ doesn't have constant variances, our uncertainty quantification will be flawed and we cannot accurately perform hypothesis tests or confidence intervals.


========================================================

<div style="float:left; width:45%">
<img style="width:100%", src="hetero.png"/>
</div>

<div style="float:left; width:44%">
<img style="width:100%", src="nonlinear.png"/>
</div>


========================================================

<h2>An example of non-constant variance</h2>

* We considered the "savings" dataset, listing the average savings rate, age demographics and per capita incomes of fifty countries avveraged over 1960 -1970.

```{r}
library("faraway")
head(savings)
```

* <b>sr</b> - is the savings rate, calculated as personal savings divided by disposable income;

* <b>pop15</b> - is the percent of the countrys' populations under age 15;
 
* <b>pop75</b> - is the percent of the countrys' populations over age 75;
 
* <b>dip</b> - is the per-capita disposable income in dollars;

* <b>ddpi</b> - is the percent growth rate of dpi.

```{r}
lmod_savings <- lm(sr ~ pop15+pop75+dpi+ddpi,savings)
```



========================================================

* We plotted the residual versus the explanatory variables of the model:

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(savings$pop15,residuals(lmod_savings), xlab="Population under 15", ylab="Residuals", cex=3, cex.lab=3, cex.axis=1.5)
abline(h=0)
```

* We noted that there are two distinct groups illustrated in the population below 15, and we made a comparison of the two populations for equal variance with an F-test.

========================================================
<h2>The F-test for variance</h2>

*  Suppose we have two independent samples from two Gaussian distributions of unknown variances respectively, i.e.,

  $$\begin{align}
x &\sim N(\mu_x ,  \sigma^2_x)  & &
z \sim N(\mu_z ,  \sigma^2_z)  
\end{align}$$

  * for $m,n>1$, we have samples of $x$ and $z$ respectively as $\left\{x_i\right\}_{i=1}^n$ and $\left\{z_i\right\}_{i=1}^m$.

* Then, we may compute the <b>sample-based means</b> of the two distributions above as,

$$\begin{align}
\tilde{x} &\triangleq \frac{1}{n}\sum_{i=1}^n x_i & &
\tilde{z} \triangleq \frac{1}{m}\sum_{i=1}^m z_i
\end{align}$$

* And likewise, we can compute the <b>sample-based variances</b> as,

$$\begin{align}
S_x^2 \triangleq &  \frac{1}{n-1} \sum_{i=1}^n\left(x_i - \tilde{x}\right)^2 & &
S_z^2 \triangleq \frac{1}{m-1}\sum_{i=1}^m \left(z_i - \tilde{z}\right)^2
\end{align}$$


========================================================

<h2> F-test continued...</h2>



 <div style="float:left; width:50%">
<img style="width:100%", src="F-distribution_pdf.png"/>
Courtesy of IkamusumeFan <a href="https://creativecommons.org/licenses/by-sa/4.0" target="blank">CC BY-SA 4.0</a>
</div>
<div style="float:right; width:50%">

<ul>
  <li> Consider the following null and alternative hypothesis:
  $$\begin{align}
  H_0 : &\sigma^2_x = \sigma^2_z \\
  H_1 : & \sigma^2_x \neq \sigma^2_z
  \end{align}$$
  </li>
<li> Under the previous assumptions, and the null hypothesis $H_0$, the <b>ratio</b> of the two sample based variances is an F-statistic

  $$\begin{align}
  \frac{S_x^2}{S_z^2}\sim F_{(n-1,m-1)}
\end{align}$$
</li>
<li> If the p-value of the above ratio is greater than the pre-specified significance ($5\%$ typically) then we fail to reject the null hypothesis, i.e., $\sigma^2_x = \sigma^2_z$.
</ul>
</div>

========================================================

<h2> Returning to the regression analysis...</h2>
<div style="float:left; width:40%">
<img style="width:100%", src="residual_comparison.png"/>
</div>
<div style="float:left; width:60%">
<ul>
 <li> We determined if there was non-constant variance in the residuals with respect to the values of the explanatory variable "pop15".</li>
 <li> As in the F-test described earlier, we take
   <ul>
    <li> $\left\{x_i \right\}_{i=1}^n$ will be the residuals, only for the observations which correspond to a country with the percent of population under 15, <b>less</b> than $35\%$;</li>
    <li> $\left\{z_i \right\}_{i=1}^m$ will be the residuals, only for the observations which correspond to a country with the percent of population under 15, <b>greater</b> than $35\%$.</li> 
    </ul>
 </li>
 <li> Then assuming that both sample populations are normally distributed with unknown variance, we will use an F-test to determine if the variances are equal.</li>
</ul>
</div>

========================================================

* We perform the above F-test in the following code:

```{r}
var.test(residuals(lmod_savings)[savings$pop15>35], residuals(lmod_savings)[savings$pop15<35])
```

* We see two dual measures for the F-test of variances above:

  * Firstly, the p-value is less than $2\%$ so we reject the null hypothesis (equal variances) with $5\%$ significance.
  
  * Secondly, the $95\%$ confidence interval <b>for the ratio of variances</b> excludes one, such that with $95\%$ confidence we exclude the possibility  that their variances are equal.
  
========================================================

<h2>General considerations for the change of scale of the response</h2>

* We supposed that we could identify some change of scale for the response that would make the variation constant in each direction.

* Using some loose approximations, we identified a rationale for the following two transformations:

 * suppose that $var(Y) = var(\boldsymbol{\epsilon}) \propto \left(\mathbb{E}(Y)\right)^2$, then
   $$\begin{align}
   h(y) \approx \int \frac{dy}{y}  = log(y);
   \end{align}$$

 * suppose that $var(Y) = var(\boldsymbol{\epsilon}) \propto \mathbb{E}(Y)$, then
  $$\begin{align}
   h(y) \approx \int \frac{dy}{\sqrt{y}}  = y^{1/2}.
   \end{align}$$

* Note: neither of the above transformations make sense when $Y$ has non-positive values, due to invertibility issues.

========================================================
<h3> A summary of evaluating the constant variance assumption</h3>

* As with many of these techniques, there isn't generally an exact way to derive a transformation of scale, and our approach ealier was baed on very loose approximations.

* Generally, we must perform the residual versus fitted plots to:

  1. look for non-constant variance;
  2. nonlinear behaviors;
  3. possible suggestions for nonlinear changes of scale, based on the standard deviation as above.
  4. potentially, other variables of interest for the model, including the plot of residuals versus other variables not included as explanatory variables on the response.
  
* When trying a change of scale, we should repeat the above steps to determine if the change of scale has had a remedial effect, or if we should try to use a different change of scale again.

* If we cannot transform the problem into one suitable for our simple least squares approach, we may consider more adavanced techniques like weighted least squares (to be disucssed later).

========================================================


<h2> Tests for normality</h2>


<div style="float:left; width:40%">
<img style="width:100%", src="cdf.png"/>
Courtesy of Inductiveload via <a href="https://upload.wikimedia.org/wikipedia/commons/c/ca/Normal_Distribution_CDF.svg" target="blank">Wikimedia Commons</a> 
</div>
<div style="float:left; width:60%">
<ul>
  <li>We recall, the hypothesis testing and confidence interval procedures we have learned are based upon assumptions of normal errors.</li>
  <li>We can assess the normality of the error with a quantile-quantile <b>(Q-Q)</b> plot.</li>
  <li>To define a Q-Q plot, we will consider the <b>theoretical</b> versus the <b>empirical</b> values for such a plot.</li>
  <li>Suppose we have two probability measures $P_1$ and $P_2$.</li>
  <li>Let each distribution have a corresponding CDF given by $C_1$ and $C_2$ respectively. </li>
  <li>Recall, the CDF of a probability measure $P$ is defined,
  $$\begin{align}
  C(x) \triangleq& P(X \leq x )\\
   \equiv& \int_{-\infty}^x f(t) dt,
  \end{align}$$
  whenever $P$ has a continuous probability density function $f(t)$
  </li>
  <li> On the left, we illustrate this with the CDF of the normal distribution with several different parameters.</li>
  
</ul>
</div>


========================================================

* Given the two probability measurs $P_1,P_2$, and their respective CDF's, we can define their theoretical Q-Q plot as the graph of the $\mathbb{R}^2$ valued function,
  
  $$\begin{align}
  G:&[0,1]  &\mapsto &\mathbb{R}^2& \\
    &p      &\mapsto &(C_1^{-1}(p), C_2^{-1}(p)) & 
  \end{align}$$

  *  Suppose $p$ is a particular quantile, e.g., $p=50\%$.  Then the map $G$ takes $p$ to the point in the plane, $(x_1, x_2)$, for which
    $$\begin{align}
    P_1(X \leq x_1) &= 50\% \\
    P_2(X \leq x_2) &= 50\%
    \end{align}$$

  * <b>Q:</b> what does the point $(x_1,x_2)$ correspond to?

  * <b>A:</b> The point $(x_1,x_2)$ is just the plot of the medians of the two distributions in the plane.
  
* <b>Q:</b> What kind of shape do we expect for the plot when the two CDFs are equal? 

* <b>A:</b> If the two CDFs are equal, $F_1 = F_2$, the Q-Q plot of $G$ is just the straight line with slope one through the origin.

========================================================


* Suppose that $P_1,P_2$ represent the same family of probability measures, such that their CDFs differ only by location and shape, i.e.,     
   * suppose $C_1(x) = C_2\left(\frac{x - \mu}{\sigma}\right)$.  
   * Then $C_1^{-1}(p) \equiv \mu +  \sigma C_2^{-1}(p)$, and there is a linear relationship between the quantiles of the two measures.
   
* Therefore, if two distributions differ only in location and scale,
  
  * e.g., $f_1 = N(0,1)$ and $f_2= N(\mu, \sigma^2)$,
  
* the Q-Q plot is just a straight line with slope $\sigma$ and intercept $\mu$.

* For this reason, when making a Q-Q plot, the CDFs (and/or the data) are typically standardized to be mean zero and variance one.

* By doing so, when measuring two distributions in the same family (as above), the Q-Q plot will just be the central diagonal of the plane.

========================================================

* Typically, we will use the Q-Q plot to make a visual inspection of the empirical quantiles of our data versus the theoretical quantiles of a target distribution.

* Let's suppose that we have samples $\left\{X_i\right\}_{i=1}^n$ of some random variable $X\sim f_2$ where $f_2$ is an unknown distribution.

* Suppose that we <em>believe</em> $f_2 = f_1$ for some known, theoretical distribution $f_1$.  That is,

  $$\begin{align}
  H_0: f_1 = f_2 \\
  H_1: f_1 \neq f_2
  \end{align}$$
  
* We will make a hypothesis test as above where, 
  
  1. if the Q-Q plot of the empirical versus theoretical CDF is "almost" the central diagonal, 
  
  2. we will <em>tentatively</em> accept the null hypothesis.

* Recall, that the inverse of the CDF applied to $50\%$, $C^{-1}\left(50\%\right)$, is just the median.  

* <b>Q:</b> Then, how do we compute the inverse of the empirical CDF based on the samples $\left\{X_i\right\}_{i=1}^n$?

* <b>A:</b> For the samples $\left\{X_i\right\}_{i=1}^n$, the inverse of the empirical CDF is simply the re-ordered values such that $X_i \leq X_{i+1}$ for all $i=1,\cdots, n-1$.


========================================================

  
* With $n$ samples, we can make $n$ plotting positions for the inverse of the empirical and theoretical CDF, i.e., we can plot the points,

  $$\begin{align}
  \left\{\left(C_1^{-1}\left(\frac{i}{n+1}\right), X_i\right) \in \mathbb{R}^2:  i = 1,\cdots,n \right\}
  \end{align}$$

  * <b>Q:</b> Notice, we do not compute $C_1^{-1}\left(1\right)$, what would be the possible issue with this plot?
  * <b>A:</b>  If, for instance, $f_1 = N(\mu, \sigma^2)$, then $C_1^{-1}(1)= \infty$.
  
  * Generally, some adjustment must be made in the above equation including, e.g., the $n+1$ as the denominator. 
  * Several different approximations can be made, but some adjustment is unavoidable.
  * For a sufficiently large number of independent, identically distributed samples $n$, the approximation won't make a very large difference, but will keep the plot finite. 

========================================================

<h2> An example of Q-Q plots</h2>

* We return to the savings data, with the savings rate defined as the response variable, while the demographic and income values as explanatory variables.

```{r}
lmod <- lm(sr ~ pop15+pop75+dpi+ddpi,savings)
```

* We want to test the residuals of this model for Gaussianity, so we will:

  1. order the values of the residuals in ascendingly such that $\hat{\boldsymbol{\epsilon}}_i \leq \hat{\boldsymbol{\epsilon}}_{i+1}$ for each $i = 1,\cdots,n$; and
  2. plot these values as the y-coordinate versus the corresponding inverse value of the CDF for the standard normal in the x-coordinates.

========================================================


* The function "qqnorm" performs the previous steps in a Q-Q plot with respect to a Gaussian (normal) target distribution automatically;

* the function "qqline" plots a line joining the first and third quartiles of the empirical distribution:

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
qqnorm(residuals(lmod),ylab="Residuals", main="",  cex=3, cex.lab=3, cex.axis=1.5)
qqline(residuals(lmod))
```

* <b>Q:</b> can you conjecture if we would accept the residuals as following a Gaussian distribution based on the above plot?

========================================================

* In the following is a histogram of the same residuals as plotted in the Q-Q plot.

* <b>Q:</b> can we determine the normality of the data from this plot?

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
hist(residuals(lmod),xlab="Residuals",main="", cex=3, cex.lab=3, cex.axis=1.5)
```

========================================================

* A primary issue of histograms for testing for normality is that the bin-size is not unique.

* There are various selection algorithms to determine a good estimate of how many bins and of what size to aid us, but histograms can be unreliable compared to a Q-Q plot.

* In this context, as a test for normality, a histogram should only be used as exploratory analysis and shouldn't be relied upon for our conclusions.

* We can numerically verify the Gaussianity of the residuals with the Shapiro-Wilk test, validating our Q-Q plot.  Here the null hypothesis is that the samples come from a normal distribution:

```{r}
shapiro.test(residuals(lmod))
```

* However, without performing a Q-Q plot before hand, we can't diagnose what possible issues may exist by the p-value alone.


========================================================

<div style="float:left; width:70%">
<img style="width:100%", src="qq_plots.png"/>
</div>

<div>
<ul>
  <li>We illustrate three examples of how the Q-Q plot can diagnose non-Gaussianity:</li>
  <ol>
    <li> Left, there is an example of a highly skewed data, drawn from a log-normal distribution.</li>
    <li> Middle, there is an example of heavy-tailed data, drawn from a Cauchy distribution.</li>
    <li> Right, there is an example of short-tailed data, drawn from a uniform distribution.</li>
  </ol>
<li>Note: Q-Q plots are powerful, but are sensitive to the tails of distributions. </li>
<li>It can be hard at times to tell at times if the tails that lie off the diagonal are trully from a heavy-tailed distribution, or if these are simply outliers in otherwise well-behaved data.</li>
</ul>
</div>

========================================================

* We note that Gaussianity of the error wasn't required by the Gauss-Markov theorem, rather this gauranteed that least-squares was the <emph>maximum likelihood estimator</em>.

* Without Gaussianity of the errors, least-squares is still the best linear unbiased estimator, but we may find that a linear model in itself is not appropriate.

* Additionally, our confidence intervals and hypothesis tests utilized the Gaussian assumption on the error.

* However, we can sometimes make due "OK" with slightly innacurate uncertainty quantification, if the <b>sample sizes are sufficiently large</b>...
  
* Particularly, the hypothesis testing and confidence intervals we have developed can be understood as good approximations due to the central limit theorem.


========================================================

<h2> Central limit theorem</h2>

* Suppose we have a sequence of random variables, $\left\{X_i \right\}_{i=1}^\infty$ which are independent, identically disributed <emph>from any distribution</emph>, such that for all $i$
<ol>
  <li> $\mathbb{E}[X_i] = \mu$; and</li>
  <li> $var(X_i) = \sigma^2$, for some finite $\sigma$.</li>
</ol>

* For each $n\geq 1$ define the sample based mean of the sequence $\left\{X_i \right\}_{i=1}^n$

  $$\begin{align}
  \tilde{X}_n \triangleq \frac{1}{n} \sum_{i=1}^n X_i
  \end{align}$$

* Then, as the number of random variables $n\rightarrow \infty$, the sample-based means
  
  $$\begin{align}
  \sqrt{n}\left(\tilde{X}_n - \mu\right) {\xrightarrow {d}} N(0, \sigma^2)
  \end{align}$$
  where the convergence is in distribution.
  
* Put another way, for $n$ sufficiently large, $\tilde{X}_n$ has <b>approximately</b> a $N\left(\mu, \frac{\sigma^2}{\sqrt{n}}\right)$ distribution.

* Therefore, <b>for large sample sizes</b>, we can produce confidence intervals for the mean of the unbiased estimator (the true $\boldsymbol{\beta}$) from the sample based estimate (the least squares solution $\hat{\boldsymbol{\beta}}$) as usual, which will <b>be a good approximation</b> if not strictly accurate.

========================================================

<h3> A summary of issues with non-Gaussianity</h3>

* Generally, when facing a non-Gaussian error distribution the solution will depend on the types of issues detected.

* For a large number of observations, we can usually ignore issues of non-Gaussianity for uncertainty quantification due to the Central Limit Theorem.

* This is also often the case for short-tailed disributions.

* For skewed distributions, a nonlinear tranformation of the response may alleviate the issuse, 
  
  * e.g., log transformation or square-root.

* For long-tailed distributions, we may need to use more robust methods, e.g. bootstrapping (not covered in lecture but discussed in the text).
