========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080

========================================================

<h1> Review -- hypothesis testing</h1>

* Recall we have seen two ways to test the significance of our models:

  * Suppose we have a large model $\boldsymbol{\Omega}$, which abstractly refers to the set of all linear models possible by choices of $p$ parameters in $\beta$, and their respective uncertainties, over certain variables $x_1, x_2, \cdots, x_{p-1}$ (with an intercept term).

  * Let $q < p$, and suppose that abstractly $\boldsymbol{\omega}$ represents a "smaller model", as found by a strictly smaller set of explanatory variables, $x_1, x_2, \cdots, x_{q-1}$ (with an intercept term).

  * We will say that we favor the model $\boldsymbol{\omega}$ unless $\boldsymbol{\Omega}$ provides appreciably better results.
 
* The variable $F$ defined as follows
  
   $$\begin{align}
   F &\triangleq \frac{ \left( RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}\right)/    (p-q)}{RSS_\boldsymbol{\Omega}/(n-p)} 
 \end{align}$$
is an $F$ statistic, with $F$ distribution under the null hypothesis.

  * <b>Q:</b> What is the null hypothesis $H_0$ and the alternative hypothesis $H_1$ in the above scenario?

  * <b>A:</b> $H_0: \boldsymbol{\beta}_i = 0$ for $i=q, \cdots, p-1$ and $H_1: \boldsymbol{\beta}_i \neq 0$ for $i =0, 1, \cdots , q -1, q, \cdots , p-1$.
  
 
========================================================

* If the null hypothesis holds, then $F \sim F_{(p-q),(n-p)}$

* The <b>significance level</b> is a value $\alpha$ we specify a priori, typically at $\alpha = 5\%, 1\%, .01\%$ etc...

* For any particular choice of $\alpha$, there exists some $F^\alpha_{(p-q),(n-p)}$ such that: 

 * the probability of $F \sim F_{(p-q),(n-p)}$  <b> and </b> 
 
 * $F > F^\alpha_{(p-q),(n-p)}$  
 
 * is less than or equal to $\alpha$.

* We will decide to reject the null hypothesis (accept the larger model) if the probability of obtaining a value of $F \sim F_{(p-q),(n-p)}$ is less than the significance level $\alpha$.
 
 * The probability of $F$ given $F \sim F_{(p-q),(n-p)}$ is called the <b> p-value</b>.
 
* <b>Exercise:</b> Suppose the p-value of $F$ is given as $\theta$, and we have a priori selected some $\alpha$ significance level. 
  
  * If we find $\theta < \alpha$, what do we conclude?
  
  * If we find $\theta > \alpha$, what do we conclude?
  
========================================================

* We saw that the test of <b>one variable</b> with respect to the larger model can be equivalently derived via the F-test and the t-test with the student t distribution.

* This is to say, we have the space of small models $\boldsymbol{\omega}$ over the variables $x_1, \cdots, x_{p-2}$ and the space of large models $\boldsymbol{\Omega}$ over the variables $x_1, \cdots, x_{p-1}$

 * <b>Note:</b> Up to re-ordering the indices, this problem can always be written this way.
 
* <b>Exercise:</b> What is the null hypothesis $H_0$ and the alternative hypothesis $H_1$ in this case?
 * <b>Answer:</b> 
   * $H_0: \boldsymbol{\beta}_{p-1} = 0$, and
   *  $H_1: \boldsymbol{\beta}_i \neq 0$ for $i=0, \cdots, p-1$.
 
* As before, we can use the F-test to test the null hypothesis, at some significance level $\alpha$.
 
========================================================

* On the other hand, the value
$$\begin{align}
t_{p-1} = \frac{\hat{\boldsymbol{\beta}}_{p-1}}{se(\hat{\boldsymbol{\beta}}_{p-1})}
\end{align}$$
 has t-distribution in $(n-p)$ degrees of freedom under the null hypothesis,

 * where we recall, 
 $$\begin{align}
se(\beta_{p-1}) \triangleq \hat{\sigma}\sqrt{(\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}_{pp}}
\end{align}$$
 and $\hat{\sigma}^2 = \frac{RSS}{n-p}$

 
  * That is, 
     * we can compute the significance of any individual variable $x_i$ and associated parameter, $\boldsymbol{\beta}_i$,
     * with respect to the big model space including the variables $x_1, \cdots, x_{p-1}$
     * from using the two-sided t-test on the associated $t_{i}$.
     
========================================================

* <b>Exercise:</b> State the null and alterantive hypothesis for each t-test and F-test below.  Suppose $\alpha=5\%$.  Where do we reject and where do we fail to reject the null hypothesis?


```r
library('faraway')
summary(lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala))
```

```

Call:
lm(formula = Species ~ Area + Elevation + Nearest + Scruz + Adjacent, 
    data = gala)

Residuals:
     Min       1Q   Median       3Q      Max 
-111.679  -34.898   -7.862   33.460  182.584 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.068221  19.154198   0.369 0.715351    
Area        -0.023938   0.022422  -1.068 0.296318    
Elevation    0.319465   0.053663   5.953 3.82e-06 ***
Nearest      0.009144   1.054136   0.009 0.993151    
Scruz       -0.240524   0.215402  -1.117 0.275208    
Adjacent    -0.074805   0.017700  -4.226 0.000297 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 60.98 on 24 degrees of freedom
Multiple R-squared:  0.7658,	Adjusted R-squared:  0.7171 
F-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07
```


========================================================

* <b>NOTE:</b> The null hypothesis will have a different interpretation if it is stated with respect to different alternative hypotheses.  


```r
summary(lm(Species ~ Area, gala))
```

```

Call:
lm(formula = Species ~ Area, data = gala)

Residuals:
    Min      1Q  Median      3Q     Max 
-99.495 -53.431 -29.045   3.423 306.137 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 63.78286   17.52442   3.640 0.001094 ** 
Area         0.08196    0.01971   4.158 0.000275 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 91.73 on 28 degrees of freedom
Multiple R-squared:  0.3817,	Adjusted R-squared:  0.3596 
F-statistic: 17.29 on 1 and 28 DF,  p-value: 0.0002748
```

 * <b> Q:</b> What is important to notice about the p-value for the F-statistic and the p-value for the t-statistic for "area"?
   * <b> A:</b> These correspond to the <b>same</b> test of the null versus alternative hypothesis in this case, i.e., leaving out "Area" is the same null hypothesis as the data is expressed only as random variation around the mean $\overline{\mathbf{Y}}$.  
   * The p-value for the t-test on "Area" is not the same here as it was on the last slide. 
   
========================================================

* We have seen how to perform a hypothesis test on a pair of predictors.  Suppose we want to test the signifcance of including the variables "Area" and "Adjacent" <b> simultaneously</b>, versus the big model given as

 * "lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)"

* <b> Exercise:</b> With a neighbor, perform the following,

 1. Write the null and alternative hypothesis.
 2. Perform the appropriate test for the pair of variables simultaneously, with significance level $\alpha = 5\%$.
 3. Answer: do we reject or fail to reject the null hypothesis?
 
 4. Repeat the above, testing the significance of "Area", "Nearest" and "Scruz" simultaneously <em>against the big model</em>.

========================================================

<h2>One more F-test...</h2>

 * ... that we will cover in this class.
 
 * Suppose we think we can make a simpler model by combining two variables as some kind of linear combination, 
  * e.g., what if we want to create a new variable 
  $$\begin{align}
  \large{\tilde{x}_i \triangleq x_i + x_j}
  \end{align}$$
  corresponding to two explanatory variables $x_i,x_j$.
 
 * This still makes sense with an F-test because we are again testing a null hypothesis as an embedded <b>subspace of the space of big models</b>.
 

========================================================

* Concretely, suppose we wish to take a new variable corresponding to the sum of the area of the island itself and the area of the adjacent island.
 
* Our null hypothesis will be that $\boldsymbol{\beta}_{area} = \boldsymbol{\beta}_{adjacent}$
 


```r
lmod  <- lm(Species ~ Area + Adjacent + Elevation + Nearest + Scruz, gala)
lmods <- lm(Species ~ I(Area+Adjacent) + Elevation + Nearest + Scruz, gala)
anova(lmods,lmod)
```

```
Analysis of Variance Table

Model 1: Species ~ I(Area + Adjacent) + Elevation + Nearest + Scruz
Model 2: Species ~ Area + Adjacent + Elevation + Nearest + Scruz
  Res.Df    RSS Df Sum of Sq     F  Pr(>F)  
1     25 109591                             
2     24  89231  1     20360 5.476 0.02793 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

* In the above, notice the only difference is the I() function, which demands R not to convert the "+" expression as a formula.

* In this case, we can reject the null hypothesis at the $\alpha = 1\%$ significance level.

========================================================

* We have learned so far how to test the significance of a given parameter $\boldsymbol{\beta}_i=0$, which would reduce the model space $\boldsymbol{\Omega}$ to the subspace $\boldsymbol{\omega}$ where $x_i$ has no effect on the response.

* We may quite similarly test the significance of a parameter having a specific value (other than zero), where $\boldsymbol{\omega}$ will represent the subspace of the big model space $\boldsymbol{\Omega}$ where $x_i$ has a specified effect.

* Consider the null hypothesis $H_0: \boldsymbol{\beta}_{Elevation} = 0.5$, versus the alternative hypothesis where we consider the space of models over "Species ~ Area + Adjacent + Elevation + Nearest + Scruz"

* We use the function "offset" to fix the associated value $\boldsymbol{\beta}_{Elevation} = 0.5$


```r
lmods <- lm(Species ~ Area+ offset(0.5*Elevation) + Nearest + Scruz + Adjacent, gala)
anova(lmods, lmod)
```

```
Analysis of Variance Table

Model 1: Species ~ Area + offset(0.5 * Elevation) + Nearest + Scruz + 
    Adjacent
Model 2: Species ~ Area + Adjacent + Elevation + Nearest + Scruz
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
1     25 131312                                
2     24  89231  1     42081 11.318 0.002574 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

* In the above, we reject the null hypothesis at the $0.1\%$ significance level.

========================================================

* <b>Note:</b> there is a simpler form of the above test which relates to the t-test.

*  Suppose we want to test the following null hypothesis, 
 $$H_0: \boldsymbol{\beta}_{Elevation} = C$$
 for some constant $C$ as discussed above.
 
* If $\hat{\boldsymbol{\beta}}_i$ is the solution by least squares, we can compute<br>
$$\begin{align}
t = \frac{\left(\hat{\boldsymbol{\beta}}_i - C\right)}{se(\boldsymbol{\beta})}
\end{align}$$

* It turns out that this is also t distributed so that pulling the values from before


```r
(tstat <- (0.31946-0.5)/0.05366)
```

```
[1] -3.364517
```

* We can compute the p-value for this realization of $t$ with a two-sided t-test,


```r
 2*pt(tstat, 24)
```

```
[1] 0.002572168
```

