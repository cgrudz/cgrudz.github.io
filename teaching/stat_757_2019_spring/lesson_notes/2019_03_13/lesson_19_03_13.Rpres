========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080


========================================================

<h1> Diagnostics</h1>

<ul> 
  <li> Our analysis and methodology for:</li>

  <ol>
    <li> fitting a linear model; </li>
    <li> forecasting predictions from a linear model; and</li>
    <li> explaning the relationships between variables in a linear model</li>
  </ol>
</ul>
* all rely on several assumptions, e.g., the conditions for the Gauss Markov theorem.

* Methods for checking and validating these assumptions are known as <b>diagnostics</b>.

* Typically, we will start with one model as a best first guess.

* Performing diagnostics will reveal issues in the  model, and suggest ways for improvement.

* Building a model is thus usually an interactive, iterative process, where we will create and perform diagnostics over a succession of models.

========================================================

<ul> 
  <li> In the rest of the class, we will consider 3 categories of potential issues with the model:</li>
  <ol>
    <li> Issues with the error/ variation.  Particularly, we have assumed that $\boldsymbol{\epsilon}\sim N(0, \sigma^2 \mathbf{I})$ such that the errors are normally distributed, independent and of constant variance.</li>
    <li> Issues with the systematic part of the model.  Particularly, we have assumed that there is an actual signal in the data of the form
    $$\begin{align}
    \mathbb{E}[\mathbf{Y}] = \mathbf{X} \boldsymbol{\beta},
    \end{align}$$
    which may not be valid.</li>
    <li> Issues with unusual observations.  Some of the observations may not fit the model, and they might change the choice and the fit of the model</li>
  </ol>
</ul>

========================================================

<h2> Checking error assumptions</h2>

* If we wish to check the assumptions on the error or variation in the signal $\boldsymbol{\epsilon}$, we need to consider, $\boldsymbol{\epsilon}$ itself is not observable.

* <b>Q:</b> What proxy could we consider for the error?

  * <b>A:</b> The residuals are related to the error functionally, but have slightly different properties.
  
* Recall, the definition of $\hat{\mathbf{Y}}$
 
 $$\begin{align}
 \hat{\mathbf{Y}} \triangleq& \mathbf{X}\left(\mathbf{X}^\mathrm{T} \mathbf{X}\right)^{-1} \mathbf{X}^\mathrm{T} \mathbf{Y} \\
  & =\mathbf{H} \mathbf{Y}
 \end{align}$$

* Therefore, if we compute the residuals,

$$\begin{align}
\hat{\boldsymbol{\epsilon}} & = \mathbf{Y} - \hat{\mathbf{Y}} \\
 & =\left(\mathbf{I} - \mathbf{H}\right)\mathbf{Y} \\
 & =\left(\mathbf{I} - \mathbf{H}\right)\mathbf{X} \boldsymbol{\beta} + \left(\mathbf{I} - \mathbf{H}\right)\boldsymbol{\epsilon}
\end{align}$$

* <b>Exercise:</b> recalling the definition of $\mathbf{H} = \mathbf{X}\left(\mathbf{X}^\mathrm{T} \mathbf{X}\right)^{-1} \mathbf{X}^\mathrm{T}$, what does $\mathbf{H}\mathbf{X}$ equal to?

* <b>Exercise:</b> given the above, what does $\left(\mathbf{I} - \mathbf{H}\right)\mathbf{X}$ equal to?

========================================================

* From the previous two exercises, we can deduce,

  $$\begin{align}
  \hat{\boldsymbol{\epsilon}} = \left(\mathbf{I} - \mathbf{H}\right)\boldsymbol{\epsilon}
  \end{align}$$

* We take the <b>assumption</b> that $\boldsymbol{\epsilon}\sim N(0, \mathbf{I} \sigma^2)$, 
  * <b>Exercise:</b> given the above assumption, what is the mean of $\hat{\boldsymbol{\epsilon}}$?
  
  * <b>Answer:</b> 
  
  $$\begin{align}
  \mathbb{E}[\hat{\boldsymbol{\epsilon}}] &= \mathbb{E}\left[\left(\mathbf{I} - \mathbf{H}\right)\boldsymbol{\epsilon}\right] \\
  &=\left(\mathbf{I} - \mathbf{H}\right)\mathbb{E}\left[\boldsymbol{\epsilon}\right]\\
  &= 0
  \end{align}$$
  
  * <b>Exercise:</b> given the above assumption, what is the covariance of $\hat{\boldsymbol{\epsilon}}$?
  
  * <b>Answer:</b> 
  
  $$\begin{align}
  \mathbb{E}\left[\left(\hat{\boldsymbol{\epsilon}}\right) \left(\hat{\boldsymbol{\epsilon}}\right)^\mathrm{T}\right] &= \mathbb{E}\left[\left(\left(\mathbf{I} - \mathbf{H}\right)\boldsymbol{\epsilon}\right)\left(\left(\mathbf{I} - \mathbf{H}\right)\boldsymbol{\epsilon}\right)^\mathrm{T}\right] \\
  &=\mathbb{E}\left[\left(\mathbf{I} - \mathbf{H}\right)\boldsymbol{\epsilon}\boldsymbol{\epsilon}^\mathrm{T}\left(\mathbf{I} - \mathbf{H}\right)^\mathrm{T}\right]\\
  &=\left(\mathbf{I} - \mathbf{H}\right)\mathbb{E}\left[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^\mathrm{T}\right]\left(\mathbf{I} - \mathbf{H}\right)^\mathrm{T}\\
 & =\left(\mathbf{I} - \mathbf{H}\right)\left[\sigma^2 \mathbf{I}\right]\left(\mathbf{I} - \mathbf{H}\right)\\
 &=\sigma^2\left(\mathbf{I} - \mathbf{H}\right)
  \end{align}$$
  
========================================================

* <b>Q:</b> Why is the last slide relevant?

  * <b>A:</b> If the assumptions hold for $\boldsymbol{\epsilon}$, then we can compare the coviance of the <b>residuals</b> to their theoretical value $\sigma^2\left(\mathbf{I} - \mathbf{H}\right)$ for consistency.
  
* Note, even though the errors are assumed to be independent and of equal variance $\sigma^2$, the same doesn't hold for the residuals.

* In particular, the operator $\mathbf{I} - \mathbf{H}$ is not generally diagonal (such that the residuals have correlation); nor are the diagonal values equal (so that the variances don't all match).

* Nonetheless, we will use the above values of the variances to test our assumptions for consistency.

========================================================


<div style="float:left; width:40%">
<img style="width:100%", src="no_problem.png"/>
</div>

<div style="float:left; width:60%">
<ul>
  <li> To the left is a plot where:</li>
  <ol>
    <li> The horizontal (x-axis) represents the fitted value $\hat{\mathbf{Y}}$, as in some model $\hat{\mathbf{Y}}=\mathbf{X}\hat{\boldsymbol{\beta}}$.</li>
    <li> The vertical axis (y-axis) represents the corresponding residual $\hat{\boldsymbol{\epsilon}}$ value, equal to $\mathbf{Y} - \hat{\mathbf{Y}}$.
  </ol>
  <li>Suppose that the variances of the error $\boldsymbol{\epsilon}$ are <b>not fixed</b>, i.e.,</li> 
    <ul>
      <li> suppose that <b>some</b> observations have <b>more</b> variation and <b>some</b> observations have <b>less</b> variation around the signal, $\mathbf{X}\boldsymbol{\beta}$. </li>
    </ul>
  <li>In this case, there is <b>dependence</b> of the variation (or error $\boldsymbol{\epsilon}$) on the <b>observation</b>.</li>
  <li> To the left, this is actuall a <b>well behaved</b> situation.  In particular, the residuals show variation that doesn't seem to depend on the value of the fitted value/ observation.</li>
  <li> The mean of the residuals appears to be zero as well, because there isn't a clear preference for positive or negative residuals</li>
  <li> The situation to the left is denoted <b> homoscedasticity.</b>
  <li><b>Q:</b> on the other hand, if there are non-constant variances (as discussed above), do we expect this to show a pattern in the plot to the left?</li> 
  <li><b>A:</b> yes, and it may express in several different ways...</li>
</ul>
</div>

========================================================

<div style="float:left; width:40%">
<img style="width:100%", src="hetero.png"/>
</div>

<div style="float:left; width:40%">
<ul>
  <li> The non-constant variance of $\hat{\boldsymbol{\epsilon}}$ to the left is known as <b>heteroscedasticity</b>.</li>
  <li> In this case, there is a clear dependence on the <b>variation</b> of $\hat{\boldsymbol{\epsilon}}$ on the fitted value/ the observation.</li>
  <li> Breaking the constant variance assumption, the Gauss-Markov theorem no longer applies</li>
  <li> Heteroscedasticity does not, in itself, cause ordinary least squares coefficient estimates to be biased;</li>
  <li> however, the theoretical estimates of the variance of the residuals (and therefore the standard errors) will become biased.</li>
</ul>
</div>

========================================================

<div style="float:left; width:40%">
<img style="width:100%", src="hetero.png"/>
</div>

<div style="float:left; width:60%">
<ul>
  <li> The bias in the standard errors complicates our ability to accurate quantify the uncertainty, and therefore to accurately:</li>
  <ol>
    <li> make hypothesis tests on the parameters for significance;</li>
    <li> provide confidence intervals for the parameters</li>
    <li> provide accurate prediction intervals and confidence intervals for the  mean response;</li>
    <li> provide explanatory power in the relationship between the response and the explanatory variables.</li>
  </ol>
</ul>
</div>

========================================================


<div style="float:left; width:40%">
<img style="width:100%", src="accuracy_precision.png"/> Courtesy of
Pekaje at the English language Wikipedia <a href="http://www.gnu.org/copyleft/fdl.html" target="blank">GFDL</a>
</div>

<div style="float:left; width:60%">
<ul>
  <li>Why does the accuracy of the uncertainty estimates matter?</li>
  <ul>
  <li>Consider the intiutive accuracy versus precision graphic.</li>
  <li>In the left, we suppose we model the reference value (vertical line) with a Gaussian.</li>
  <li>Based on our observations of the response variable, our prediction will be some random draw from the Gaussian.</li>
  <ul>
    <li> We can treat the Gaussian as the distribution of outcomes for the model we create, given different independent clinical trials.</li>
  </ul>
  <li>The accuracy of the prediction is described in terms of the difference between the mean of our predictions (over many possible observations) and the true value.</li>
  <li> The precision is the variation of the estimates about the mean value. </li>
  <li> To say that the estimated parameters of the least-squares fit are unbiased is to say the following:
  <ol>
    <li> if the true parameters are the reference line, then the distribution of the parameters will have a mean that matches this line;</li>
    <li> in particular, over many independent observations of the response and explanatory variables, we will recover the true parameters in expectation.</li>
  </ol>
</ul>
</div>

========================================================


<div style="float:left; width:40%">
<img style="width:100%", src="accuracy_precision.png"/> Courtesy of
Pekaje at the English language Wikipedia <a href="http://www.gnu.org/copyleft/fdl.html" target="blank">GFDL</a>
</div>


<div style="float:left; width:60%">
<ul>
  <li> However, we typically won't have many independent trials and we rely on the uncertainty estimates to determine the <b>precision</b> of our estimates.</li>
  <li> That is to say, without accurate estimates of the precision, we won't know if our solution by least squares "lives within an inch or a mile" of the true parameters.</li>
  <li> Our estimates (prediction/ explantion) based on our solution to least-squares fit will be similarly impacted.</li>
  </ul>
</div>

========================================================

<div style="float:left; width:40%">
<img style="width:100%", src="nonlinear.png"/>
</div>

<div style="float:left; width:60%">
<ul>
<li> Using the same plot as before, we can likewise determine if there is actually nonlinearity in the residuals.</li>
<li> The plot at the left exhibits a nonlinear dependence of the residual on the fitted/ observed values</li>
<li> The same technique can also be used replacing the fitted values $\hat{\mathbf{Y}}$ in the horizontal axis with any other variable in the model $\mathbf{X}_i$, to determine dependence of the residual on the explanatory variables</li>
<li> Additionally, we may consider how the residual varies across variables $\mathbf{X}_i$ that we have data for, but have not included as explanatory variables on the response.</li>
<li> In particular, if there is a dependence structure for the residuals on the variable that was left out of the model, it suggests that we should consider its impact on the response and/or if it is a variable that is tightly correlated with our existing model variables.</li>
<li> We will return to this on Friday</li>
</ul>
