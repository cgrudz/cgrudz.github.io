<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>
<!-- foo 
Highlighting
bold
strong
orange <b style="color:#d95f02"> text </b>
green  <b style="color:#1b9e77"> text </b>
purple <b style="color:#d24693"> text </b>
red    <b style="color:#FF0000"> text </b>
blue   <b style="color:#0000FF"> text </b>
-->

Mean and variance of a random variable
========================================================
date: 03/01/2021
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:

  * Mean, median and mode
  * Weighted mean
  * Mean of a frequency distribution
  * Mean of a probability distribution
  * Standard deviation
  * Variance

<!-- 3.2 -->

========================================================

## Motivation

* Our goal in this course is to use <b style="color:#d95f02">statistics</b> from a <b style="color:#d95f02">small, representative sample</b> to say something <b style="color:#1b9e77">general</b> about the <b style="color:#1b9e77">larger, unobservable population or phenomena</b>.</li>
  
* Recall, the measures of the <b style="color:#1b9e77">population</b> are what we referred to as <b style="color:#1b9e77">parameters</b>.

* <b style="color:#1b9e77">Parameters</b> are generally <strong>unknown and unknowable</strong>.</li>
   
   * For example, the <b style="color:#1b9e77">mean age of every adult</b> living in the United States is a <b style="color:#1b9e77">parameter</b> for the adult population of the USA.
    
   * We <strong>cannot possibly know this value exactly</strong> as there are people who cannot be surveyed and / or don't have accurate records.
   
   * If we have a <b style="color:#d95f02">representative sample</b> we can compute the <b style="color:#d95f02">sample mean</b>.
   
   * The <b style="color:#d95f02">sample  mean</b> will almost surely <b>not equal</b> <b style="color:#1b9e77">population mean</b>, due to the natural variation <b style="color:#d95f02">(sampling error)</b> that occurs in <b style="color:#d95f02">any given sample</b>.
   * However, if we have a good <strong>probabilistic model</strong> for the ages of adults, we can use the <b style="color:#d95f02">sample statistic</b> to estimate the general, unknown <b style="color:#1b9e77">population parameter</b>.
   
* <strong>Random variables</strong> and <strong>probability distributions</strong> give us the <b>model</b> for estimating <b style="color:#1b9e77">population parameters</b>.

* <b>Note:</b> we can only <b>"find"</b> the parameters exactly in <strong>very simple examples</strong> like games of chance.

* Generally, we will have to be satisfied with <b>estimates of the parameters</b> that are uncertain, but also <strong>include measures of "how uncertain"</strong>.

========================================================

## Characteristics of data


<div style="float:left; width:40%">
<img src="Standard_deviation_diagram.png" style="width:100%"  alt="Diagram of the percent of outcomes contained within each standard deviation of the mean
for a standard normal distribution.">
<p style="text-align:center">
Courtesy of M. W. Toews <a href="https://creativecommons.org/licenses/by/2.5" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Standard_deviation_diagram.svg"> Wikimedia Commons</a>. 
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>In statistics, we try to characterize data and populations by a number of the features that they exhibits.</li>
  <li>For a single variable, the most common measures are:</li>
  <ol>
    <li> <b>Center:</b> A representative value that indicates where the <strong>middle</strong> of the data set
is located.</li>
    <li> <b>Spread:</b> A measure of the amount that the data values <strong>vary around the center</strong>.</li>
  </ol>
</ul>
</div>
<div style="float:left;width:100%">
<ul>  
  <li>We will now recall some measures of center:</li>
  <ol>
    <li><b>mean</b>;</li>
    <li><b>median</b>; and</li>
    <li><b>mode</b>.</li>
    </ol>
    <li>Each of these usually gives a different view of where the "most central point" of the data lies.</li> 
</ul>
</div>



========================================================

## Mean

* The <b>(arithmetic sample) mean</b> is usually the most important measure of center.

* Suppose we have a sample of <strong>$n$ total measurements of some random variable $X$</strong>.

  * We will denote these measurements $x_1, x_2, \cdots, x_n$
  
* Then, the <b>(arithmetic sample) mean is defined</b>
  
  $$\text{Sample mean} = \overline{x} =  \frac{x_1  +x_2 +\cdots + x_n}{n}= \frac{\sum_{i=1}^n x_i}{n} $$
  
* <b>Q:</b> is the sample mean a <b style="color:#d95f02">statistic</b> or a <b style="color:#1b9e77">parameter</b>?

  * <b>A:</b> the  <b style="color:#d95f02">sample mean</b> is computed from  a <b style="color:#d95f02">sample</b> and thus is a <b style="color:#d95f02">statistic</b>.
  
  * For this reason, if we took <b style="color:#d95f02">new measurements from a new sample</b> of the <b style="color:#1b9e77">population</b>, we could get a different value.
  
  * <strong>The random difference between the <b style="color:#d95f02">sample mean</b> and the mean of the true <b style="color:#1b9e77">population mean</b> is called sampling error</strong>.
  
* An important property of the sample mean is that it <strong>tends to vary less over re-sampling than other statistics</strong>.

  * That is, it tends to stay close to the same value.
  
*  However, <strong>the sample mean is very sensitive to outliers</strong>.

  * If outliers exist in the data, the mean can be drawn far away from the "main" cluster of data. 
  
* A statistic is called <b>resistant</b> if it doesn't change very much with respect to outlier data.


========================================================

## Median and mode

<ul>
  <li> A different notion of center is the <strong>middle of the data</strong>.</li>
  <li> For a <strong>numerical measurement</strong>, we can always <strong>order the data</strong> so that we go from low to high or high to low.</li>
  <li> <b>Median</b> -- the median is the middle of the ordered data set.</li>
  <ul>
    <li> If there are an <b>odd number of measurements</b>, <strong>the median is defined as the middle value exactly</strong>.</li>
    <li> If there are an <b>even number of measurements</b>, we split the data into the lower $50\%$ and upper $50\%$ of the measurements;</li>
    <li> then we take the <strong>median to be the mean of</strong> the:</li>
    <ol>
      <li><b>largest of the lower $50\%$</b>; and</li>
      <li><b>smallest of the upper $50\%$</b>.</li>
    </ol>
  </ul>
  <li> Another notion of the most <strong>"central point"</strong> in the data can be the value that is <strong>measured most frequently</strong>.</li>
  <li> <b>Mode</b> -- the mode is the observed value that is most frequent in the data.</li>
</ul>


========================================================

## Differences in mean, median and mode


<div style="float:left; width:60%">
<img src="mean_median_mode.png" style="width:100%"  alt="Differences between mean, median and mode with non-symmetric distributions.">
<p style="text-align:center">
Courtesy of Diva Jain <a href="https://creativecommons.org/licenses/by-sa/4.0" target="blank">CC</a> via <a href="https://commons.wikimedia.org/wiki/File:Relationship_between_mean_and_median_under_different_skewness.png"> Wikimedia Commons</a>. 
</p>
</div>
<div style="float:left; width:40%">
<ul>
  <li>Usually, the mean, median and mode tell us different characteristics of what we call the "center" of the data.</li>
  <li>In the <strong>special case when data is normal, these coincide</strong>.</li>
  <li>In the left, we see data that is all uni-modal, but with three different cases.</li>
  <li>In the left case, we have right skewness:
  <ul>
    <li>Here, the mean and median are discplaced to the right away from the mode.</li>
    <li>Additionally, the mean and median do not match.</li>
  </ul>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>In the right case, we have left skewness:</li>
  <ul>
    <li> In this case, the mean and the median are skewed to the left away from the mode.</li>
  </ul>
  <li><b>Note:</b> the precise location of the mean and median do not need to hold this way for all skew distributions -- <strong>this is only one example of how this can look</strong>  .</li>  
  <li>"Physically", the <strong>mean corresponds to the center of mass of the distribution</strong>, if each observation is weighted by the measurement value.</li>
  <li>Even if $50\%$ of values lie above and below the median, the <b>weights</b> of the observations can move the mean away from the median.</li>   
</ul>
</div>


========================================================

## Weighted means

* Let us suppose that we have a sample $x_1, x_2, \cdots, x_n$.

* Suppose each measurement is given a corresponding <b>weight</b> $w_i$ so that there are pairs of the form
 $$\begin{matrix} x_1 & w_1\\ x_2 & w_2 \\ \vdots & \vdots \\ x_n & w_n \end{matrix}$$
 
* We compute a <b>weighted mean</b> using the following formula,

  $$\frac{\sum_{i=1}^n x_i \times w_i}{\sum_i^n w_i} = \frac{x_1 \times w_1 + x_2 \times x_2 + \cdots + x_n \times w_n}{w_1 + w_2 + \cdots + w_n}$$  
 
 * i.e., <strong>this is the sum of the measurements times the weights, divided by the sum of the weights</strong> .

========================================================

### Weighted means example

* Let's suppose that we want to compute the grade point mean (GPA) for some student.

* We will suppose that the student gets letter grades as follows: $A, B, C, A, B$.

* The letters are given point values as $A=4.0, B=3.0, C=2.0, D=1.0$

* The GPA is <strong>computed as a weighted mean of the grade points, weighted by the number of credits for the class</strong>.

  $$\begin{matrix} A & 3 \text{ credits} \\ B & 2 \text{ credits}  \\ C & 2 \text{ credits} \\ A & 1 \text{ credit} \\ B & 3 \text{ credits} \end{matrix}$$

* <b>Q:</b> how do we compute the weighted mean in this case? What is the GPA? 

  * The <b>weighted mean</b> (GPA) is computed as,

 $$\frac{4.0 \times 3 + 3.0 \times 2 + 2.0 \times 2 + 4.0 \times 1 + 3.0 \times 3}{3 + 2 + 2 + 1 + 3}= \frac{35}{11} \approx 3.18$$

========================================================

## Calculating the mean from a frequency distribution

<div style="float:left; width:65%">
<ul>
 <li>Computing the mean from a <b>frequency distribution</b> for an experiment is very similar to computing a weighted mean.</li>
 <li>Let's suppose we have an experiment in which we flip two coins, and <b>$X$ represents the number of heads</b>.</li>
 <li>Then suppose, we replicate this experiment with 20 independent trials.</li>
 <ul>
  <li>In the right, we will suppose that this table represents the frequency of different outcomes of the experiment over the 20 replicated trials.</li>
</ul>
</div>
<div style="float:right; width:35%;text-align:center;">
<table class="fragment">
<tr>
  <th> <strong>Outcome </strong></th> <th> <strong>Observed value for $X=x$</strong> </th> <th><strong>Frequency</strong></th></th>
</tr>
<tr>
<td> $\{H,H\}$ </td> <td>$x=2$</td> <td> $n=6$</td></tr>
<tr><td>$\{H,T\}$</td> <td>$x=1$</td> <td>$n=3$</td></tr>
<tr><td>$\{T,H\}$</td> <td>$x=1$</td> <td>$n=7$</td></tr>
<tr><td>$\{T,T\}$</td> <td>$x=0$</td> <td>$n=4$</td></tr>
</table>
</div>
<div style="float:left; width:100%">
<ul>
  <li>The formula to compute the <b>mean of a frequency distribution</b> is as follows:
  $$\text{Mean of a frequency distribution} = \frac{\sum_\text{Possible values} \text{(Observed value)} \times \text{(Frequency of observation)}}{\sum_\text{Possible values} \text{(Frequency of observation)}}.$$</li>
  <li>In the above table, we will denote $x=\text{Observed value}$ and $n=\text{Frequency}$.</li>
  <li>The formula thus becomes
  $$\text{Mean of a frequency distribution} = \frac{\sum_\text{Possible values} x \times n}{\sum_\text{Possible values} n}.$$</li>
  <li>For this frequency distribution, we thus get the equation
  $$\text{Mean of a frequency distribution} = \frac{2\times 6 + 1 \times 3 + 1\times 7 + 0 \times 4}{6+3+7+4} =\frac{22}{20}=1.1.$$</li>
  <li>Notice, this is very close to the number of heads we would expect in two coin flips if we <strong>averaged over infinite replications</strong>...</li>
</ul>
</div>


========================================================

## The mean of probability distributions


<div style="float:left; width:65%">
<ul>
  <li><b style="color:#d95f02">Frequency distributions are derived from samples</b>, and therefore the measures of frequency distributions are <b style="color:#d95f02">statistics</b>.</li>
  <ul>
    <li>In the last example, this was equivalent to computing the <b style="color:#d95f02">sample mean $\overline{x}$</b> of the random variable $X$.</li>
  </ul>
  <li>On the other hand, a <b style="color:#1b9e77">probability distribution represents the entire population</b>, where the population may be abstract.</li>
  </ul>
</div>

<div style="float:right; width:35%;text-align:center;">
<table class="fragment">
<tr>
  <th> <strong>Outcome </strong></th> <th> <strong>Observed value for $X=x$</strong> </th> <th><strong>Probability</strong></th></th>
</tr>
<tr>
<td> $\{H,H\}$ </td> <td>$x=2$</td> <td> $f(x)=\frac{1}{4}$</td></tr>
<tr><td>$\{H,T\}, \{T,H\}$</td> <td>$x=1$</td> <td>$f(x)=\frac{2}{4}$</td></tr>
<tr><td>$\{T,T\}$</td> <td>$x=0$</td> <td>$f(x)=\frac{1}{4}$</td></tr>
</table>
</div>
<div style="float:left; width:100%">
<ul>
  <ul>
    <li>E.g., for the two coin flips, the probability distribution for $x$ represents the relative frequency of outcomes over the <b style="color:#1b9e77">population of all possible experiments or infinite replications</b>.</li> 
  <li>Therefore, if we compute the mean of $x$ for the probability distribution, we have the <b style="color:#1b9e77">population parameter $\mu$</b>.</li>
  </ul>
  <li>To compute the mean of the <b style="color:#1b9e77">probability distribution</b>, we follow a formula like the mean of a <b style="color:#d95f02">frequency disribution</b>.</li> 
  <ul>
    <li>Let $\{x_\alpha\}$ be the collection of all possible values for $x$ in its range $\mathbf{R}$.</li>
    <ul>
      <li>For a table as above, this corresponds to all row values in the middle.</li>
    </ul>
    <li>Let $\{P(X=x_\alpha)\}$ be all the associated probabilities for $x$ over its range of values $\mathbf{R}$.</li>
    <ul>
      <li>For a table as above, this corresponds to all row values in the right-hand-side.</li>
    </ul>
    <li>Then the <b style="color:#1b9e77">mean of the probability distribution</b> is given,
    $$\mu = \sum_{x_\alpha \in \mathbf{R}} x_\alpha P(X=x_\alpha) = \sum_{x_\alpha \in \mathbf{R}} x_\alpha f(x_\alpha)$$ </li>
  </ul>
</ul>
</div>

========================================================

### The mean of probability distributions continued

<ul>
  <li>Notice that the <b style="color:#1b9e77">mean of the probability distribution</b>
    $$\mu = \sum_{x_\alpha \in \mathbf{R}} x_\alpha P(X=x_\alpha) = \sum_{x_\alpha \in \mathbf{R}} x_\alpha f(x_\alpha)$$
    is really <strong>identical to the formula for the mean of a frequency distribution</strong>.</li>
    <ul>
      <li>Suppose there are $N$ total possibilities for the outcome of our experiment;</li>
      <li>suppose for each $x_\alpha$ in the range there are $n_\alpha$ total ways that $X$ can attain the value $x_\alpha$.</li>
      <li>If we look at the formula for the mean of a frequency distribution
      $$\begin{align}
       \frac{ \sum_{x_\alpha \in \mathbf{R}} x \times n_\alpha}{\sum_{x_\alpha \in \mathbf{R}} n_\alpha}=   \frac{ \sum_{x_\alpha \in \mathbf{R}} x \times n_\alpha}{N}
      =   \sum_{x_\alpha \in \mathbf{R}} x \times \frac{ n_\alpha}{N}
      = \sum_{x_\alpha \in \mathbf{R}} x_\alpha P(X=x_\alpha) = \mu
      \end{align}$$</li>
    </ul>
  <li>Therefore, the formula is really the same, but the <strong>interpretation is different</strong> because we are dealing with <b style="color:#1b9e77">population values</b>.</li>
  <li>Because of the difference in the interpretation, the mean of a probability distribution has a special name:</li>
  <ul>
    <li>For a random variable $x$ with probability distribution defined by the pairs of values $\{x_\alpha\}$ and $P(X=x_\alpha)$, the <b style="color:#1b9e77">expected value of $x$</b> is defined,
    $$\mu = \mathbb{E}\left[X\right] = \sum_{x_\alpha \in \mathbf{R}} x_\alpha P(X=x_\alpha).$$</li>
    <li>We call the <b style="color:#1b9e77">mean of the probability distribution the expected value</b>, because it can be thought of as the <strong>theoretical mean if we repeated an experiment infinitely many times or sampled the entire population</strong>;</li>
    <li>we would <b>expect this value on average</b>, <strong>relative to infinitely many experiments</strong>.</li>  
  </ul>
</ul> 


========================================================

### Example of the expected value

<div style="float:right; width:35%;text-align:center;">
<table class="fragment">
<tr>
  <th> <strong>Outcome </strong></th> <th> <strong>Observed value for $X=x$</strong> </th> <th><strong>Probability</strong></th></th>
</tr>
<tr>
<td> $\{H,H\}$ </td> <td>$x=2$</td> <td> $f(x)=\frac{1}{4}$</td></tr>
<tr><td>$\{H,T\}, \{T,H\}$</td> <td>$x=1$</td> <td>$f(x)=\frac{2}{4}$</td></tr>
<tr><td>$\{T,T\}$</td> <td>$x=0$</td> <td>$f(x)=\frac{1}{4}$</td></tr>
</table>
</div>
<div style="float:left; width:65%">
<ul>
<li>Let's consider the probability distribution for the two coin flipping experiment.</li>
<li>If we follow the calculation from the last slide
  $$\begin{align}
  \mu &= \mathbb{E}\left[X\right] \\
      &= \sum_{x_i} x_i \times P(X=x_i) \\
      &= \sum_{x_i} x_i \times f(x_i) \\
      &= 2 \times \frac{1}{4} + 1 \times \frac{2}{4} + 0 \times \frac{1}{4} \\
      &= 1
  \end{align}$$</li>
<li> Thus we say that the  <b style="color:#1b9e77">expected value</b> of two coin flips is to observe <b>one heads</b>.</li>
</div>

========================================================

## Basic concepts of variation

<div style="float:left; width:40%;text-align:center;">
<img src="waiting_times.png" style="width:100%" alt="IQ scores table, including the mean estimate computed from the table.">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 5th edition
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>In the figure to the left, we examine the frequency of wait times in seconds at a bank to see a teller.</li>
  <li> On the top, the customers were fed into a single line to wait for an open teller among multiple tellers;</li>
  <li>on the bottom, customers are fed into one of multiple lines to see one an open teller at the front of the line.</li>
  <li>We note that both frequency plots have the <strong>same mean, median and mode of 100 seconds</strong>.</li>
  <li>If we only characterize data in terms of the <b>center</b> we actually don't have a very complete picture -- indeed, <strong>we can't distinguish the two scenarios by these statistics</strong> .</li>  
  <li>Particularly, the outcomes with multiple lines have much more <b>variation</b> than the outcomes with a single line.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>In this example, the bank actually chose to combine the multiple lines into a single line.</li>
  <li><b>Note:</b> this does not actually reduce the mean waiting time for customers.</li>
  <li>However, customers were happier with this option because the <strong>variability in waiting time was reduced</strong>.</li>
  <li>In practice, <strong>our statistics for variation / spread / dispersion of the data can be the most important statistics</strong>.</li> 
  <li>We will study <b>range</b>, <b>standard deviation</b> and <b>variance</b> to quantify this.</li>
</ul>
</div>

========================================================

## Standard deviation

<ul>
  <li><b>Standard deviation</b> is probably the most important measure of the spread of data.</li>
  <li>Suppose that we have a sample $x_1,x_2,\cdots, x_n$, and let us denote the <b style="color:#d95f02">sample mean</b> as
  $$\overline{x} = \frac{\sum_{i=1}^n x_i}{n}.$$</li>
  <li>Then, the <b style="color:#d95f02">sample standard deviation</b> denoted $s$ is defined as,
  $$s = \sqrt{\frac{\sum_{i=1}^n\left(x_i - \overline{x}\right)^2}{n-1}}$$</li>
  <li>This formula can be understood as follows:</li>
  <ul>
    <li>In each term of the sum $\sum_{i=1}^n\left({\color{blue} {x_i - \overline{x}}} \right)^{\color{red}2}$, we see <b style="color:blue">how much measurement $i$ deviates from the sample mean</b>.</li>
    <li>The <b style="color:red">square</b> in this equation keeps the quantity positive.</li>
    <li>The numerator thus gives <strong>the total sum of square differences of each sample value from the sample mean</strong>.</li>
    <li>The denominator divides by the number of total sample size minus one;</li>
    <ul>
      <li>there are good mathematical reasons for this, but for now we will say that we take an average over $n-1$ because the <strong>sample mean required one calculation of a statistic of the samples already</strong>. </li>
    </ul>
    <li>Finally, we take a square root to make the standard deviation in the same units as the variable $x$;</li>
    <ul>
      <li>without this, the standard deviation would still be in the units squared.</li>  
    </ul>
  </ul>
</ul>


========================================================

### Standard deviation continued

* Note, we could have considered a different way to measure the variation than standard deviation.

* Consider, if we want to measure the total deviation we could instead write this as,
 
  $$\sum_{i=1}^n \vert x_i - \overline{x}\vert $$
  
* We could then divide this by the total number of observations, which gives

  $$\text{Mean absolute deviation} = \frac{\sum_{i=1}^n \vert x_i - \overline{x}\vert}{n-1} $$

* This is a possible choice for a similar measure of the variation, but the main issue lies in that the absolute value is not an "algebraic operation".

* If we want to make calculations or inferences based on the formula above, this will become very difficult and there are few tools that work well with this statistic.

* For this reason, using the square as in the <b style="color:#d95f02">sample standard deviation</b> 
 
 $$s = \sqrt{\frac{\sum_{i=1}^n\left(x_i - \overline{x}\right)^2}{n-1}}$$
 
 we get a similar result, but one that is mathematically easier to manipulate.

========================================================

### Standard deviation continued

<ul>
  <li><b>Note</b> -- the <b style="color:#d95f02">sample standard deviation</b>  can be computed equivalently as follows.</li>
  <li> Supposing again that we have a sample $x_1, \cdots, x_n$, we can compute
  $$s = \sqrt{\frac{n \left(\sum_{i=1}^n x_i^2 \right) - \left(\sum_{i=1}^n x_i\right)^2}{n\left(n-1\right)}}$$</li>
  <li>This formula is totally equivalent to the last one and just requires some algebraic manipulation to show that this is the case.</li>
  <li>Often, the above calculation is preferable because we do not need to pre-compute the sample mean.</li>
  <li>This is also the form that is usually preferred for computer software calculation of the sample standard deviation.</li> 
  <li>We should note, the <b style="color:#d95f02">sample standard deviation is a statistic</b> because it is computed from samples.</li>
  <li>We can also consider the <b style="color:#1b9e77">population standard deviation</b>.</li>
  <li>Suppose that there are $N$ total members in the <b style="color:#1b9e77">population</b> with corresponding measurement values $x_1 ,\cdots, x_N$.</li>
  <li>If we had access to the entire population, could compute the <b style="color:#1b9e77">population mean</b> as
  $$\text{Population mean} = \mu = \frac{\sum_{i=1}^N x_i }{N}.$$</li>
  <li>With respect to the <b style="color:#1b9e77">population mean $\mu$</b> the <b style="color:#1b9e77">population standard deviation</b> is given as,
  $$\text{Population standard deviation} = \sigma = \sqrt{\frac{\sum_{i=1}^N \left(x_i - \mu\right)^2}{N}}.$$
</ul>

========================================================

### Standard deviation continued


<ul>
  <li>Usually, we will not have access to the entire <b style="color:#1b9e77">population</b> $x_1, \cdots, x_N$.</li>
  <li>Instead, we will only have some <b style="color:#d95f02">smaller subset of values in a sample</b> $x_1, \cdots, x_n$ for $n< N$.</li>
  <li>Therefore, the formulas which we use most often are,
  $$s = \sqrt{\frac{\sum_{i=1}^n\left(x_i - \overline{x}\right)^2}{n-1}}$$</li>
  <li>or
  $$s = \sqrt{\frac{n \left(\sum_{i=1}^n x_i^2 \right) - \left(\sum_{i=1}^n x_i\right)^2}{n\left(n-1\right)}}$$</li>
  <li>but not 
  $$\sigma = \sqrt{\frac{\sum_{i=1}^N \left(x_i - \mu\right)^2}{N}}.$$
  </li>
  <li>One key difference to remember is that for the <b style="color:#d95f02">sample standard deviation</b>, we have a very different denominator than with the <b style="color:#1b9e77">population standard deviation</b>.</li>
</ul>

========================================================

### Standard deviation example

* We will not focus on calculating the sample standard deviation manually in this course;

 * however, to demonstrate the concept, we will consider it once here.
 
* Suppose we have the sample $22, 22, 26$ and  $24$.  

* We wish to compute how much deviation there is in the data from the <b style="color:#d95f02">sample mean</b>, so we will begin by computing this value

  $$\overline{x} = \frac{22 + 22 + 26 + 24 }{4} = \frac{94}{4}=23.5$$

* We now compute the <strong>raw deviation</strong> of each sample from the <b style="color:#d95f02">sample mean</b>:

  $$\begin{align}
  x_1 - \overline{x} =& 22 - 23.5 = -1.5\\
  x_2 - \overline{x} =& 22 - 23.5 = -1.5\\
  x_3 - \overline{x} =& 26 - 23.5 = 2.5\\
  x_4 - \overline{x} =& 24 - 23.5 = 0.5\\
  \end{align}$$

* Squaring each value, we obtain $2.25, 2.25, 6.25, 0.25$, so that

  $$s = \sqrt{\frac{\sum_{i=1}^4 \left(x_i - \overline{x}\right)^2}{3}} = \sqrt{\frac{11}{3}}\approx 1.9$$
  
* This shows how the <b style="color:#d95f02">sample standard deviation</b> can be computed, but we will want a few ways to interpret the value.


========================================================

## Variance


* The word <b>variance</b> also has a specific meaning in statistics and is another tool for describing the variation / dispersion / spread of the data.

* Suppose that the data has a <b style="color:#1b9e77">population standard deviation of $\sigma$</b> and a <b style="color:#d95f02">sample standard deviation of $s$</b>.

* Then, the data has a <b style="color:#1b9e77">population variance of $\sigma^2$</b>.

* Likewise, the data has a  <b style="color:#d95f02">sample variance of $s^2$</b>.

* Therefore, for either a <b style="color:#1b9e77">population parameter</b> or a <b style="color:#d95f02">sample statistic</b>, the <strong>variance is the square of the standard deviation</strong>. 

  * Because of this, the variance has units which are the <strong>square of the original units</strong>.
  
* For example, measuring the heights of students in inches, the <strong>standard deviation is in the units inches</strong>.

  * However, the <strong>variance is in the unit $\text{inches}^2$</strong>.
  

========================================================

## Important properties of standard deviation and variance

<ul>
  <li>We should introduce a few key properties of the standard deviation and the variance:</li>
  <ol>
    <li><b>Standard deviation</b> and <b>variance</b> are always <strong>non-negative by construction</strong>.
    $$\begin{align}
    s &= \sqrt{\frac{\sum_{i=1}^n \left(x_i - \overline{x}\right)^2}{n-1}} \\
    \sigma & = \sqrt{\frac{\sum_{i=1}^N \left(x_i - \mu\right )^2}{N}}
    \end{align}$$
    </li>
    <li><b>Standard deviation</b> and <b>variance</b> are only zero when all values are equal and <strong>larger values means there is more spread in the data</strong>.</li>
    <li>However, the size of the <b>standard deviation</b> and <b>variance</b> is also <strong>sensitive to outliers</strong>, and they can become large with a few outliers present.</li> 
    <li>The <b style="color:#d95f02">sample variance</b> is an <b>unbiased estimator</b> of the <b style="color:#1b9e77">population variance</b>.</li>
    <ul>  
      <li>Sampling error will mean that we <b>usually do not have a <b style="color:#d95f02">sample variance</b> that is equal to the <b style="color:#1b9e77">population variance</b></b>.</li>
      <li>However, <strong>over all possible samples this should usually be close to the true value</strong>.</li>
    </ul>
    <li>The <b style="color:#d95f02">sample standard deviation</b> is a <strong>biased estimator</strong> of the <b style="color:#1b9e77">population standard deviation</b>.</li>
    <ul>
      <li>Over repeated resampling, the different estimates for the <b style="color:#1b9e77">population standard deviation</b> via the <b style="color:#d95f02">sample standard deviation</b> <strong>tend to be too small</strong>.</li> 
      <li>However, for large sample sizes this bias of the sample standard deviation often being too small won't have much practical effect.</li>
    </ul>
  </ol>
</ul>

========================================================

## The empirical (68-95-99.7) rule


<div style="float:left; width:40%">
<img src="empirical_rule_histogram.png" style="width:100%"  alt="Diagram of the percent of outcomes contained within each standard deviation of the mean
for a normal distribution.">
<p  style="text-align:center">
Courtesy of Melikamp <a href="https://creativecommons.org/licenses/by/2.5" target="blank">CC</a> via  <a href=" https://commons.wikimedia.org/wiki/File:Empirical_rule_histogram.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>Recall now the bell curve picture that we often consider -- we will suppose we have a population that is distributed as a bell shape.</li>
  <li>We suppose that the <b style="color:#1b9e77">population mean is $\mu$</b> and <b style="color:#1b9e77">population standard deviation $\sigma$</b>.</li>
  <li>We suppose that the histogram represents the <b style="color:#d95f02">sample data</b> which is mostly bell-shaped, but is smaller than the population so it is not exact.</li>
  <li>The <b>empirical rule</b> is as follows:</li>
  <ul>
    <li>Approximately $68\%$ of the <b style="color:#d95f02">sample data</b> will lie within <b style="color:#1b9e77">one standard deviation $\sigma$ of the population mean $\mu$</b>, i.e., in
    $$[\mu - \sigma, \mu + \sigma].$$
    </li>
    <li>Approximately $95\%$ of <b style="color:#d95f02">sample data</b> will lie within <b style="color:#1b9e77">two standard deviations $2\sigma$ of the population mean $\mu$</b>, i.e., in
    $$[\mu - 2\sigma, \mu + 2\sigma].$$</li>
    <li>Approximately $99.7\%$ of <b style="color:#d95f02">sample data</b> will lie within <b style="color:#1b9e77">three standard deviations $3\sigma$ of the population mean $\mu$</b>, i.e., in
    $$[\mu - 3\sigma, \mu + 3\sigma].$$</li>  
  </ul>
</ul>
</div>
<div style="float:left;width:100%">
<ul>  
  <li>This tells us that for <b>normal data</b>, the <strong>spread can be easily interpreted from the standard deviation</strong>.</li> 
</ul>
</div>


========================================================

## The standard deviation and variance of probability distributions

<ul>
  <li>Let's recall the formula now for the <b style="color:#1b9e77">standard deviation of a population</b> with members $\{x_i\}_{i=1}^N$ 
  $$\sigma = \sqrt{\frac{\sum_{i=1}^N \left(x_i - \mu\right)^2}{N}},$$
  where we take a denominator of $N$ for the population instead of $N-1$ as in samples.</li>
  <li>Let's suppose that the <b style="color:#1b9e77">population members $x_i$</b> <strong>equal values $x_\alpha$ in the range $\mathbf{R}$ with frequencies $n_\alpha$</strong>.</li>
  <li>If we re-write the above formula in terms of $x_\alpha$ and $n_\alpha$ we can say,
  $$\begin{align}
  \sigma = \sqrt{\frac{\sum_{x_\alpha\in \mathbf{R}} n_\alpha\left(x_\alpha - \mu\right)^2}{N}} 
  = \sqrt{\sum_{x_\alpha \in \mathbf{R}} \frac{n_\alpha}{N} \left(x_\alpha - \mu\right)^2}
  = \sqrt{\sum_{x_\alpha\in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2 }.
  \end{align}$$</li>
  <li>We will denote,
  $$\sigma = \sqrt{\sum_{x_\alpha\in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2 }$$
  the <b style="color:#1b9e77">standard deviation of the probability distribution</b> associated to the random variable $x$.</li>  
  <li>That is to say that the <b style="color:#1b9e77">population standard deviation $\sigma$</b> is <b>exactly the standard deviation of the probability distribution</b>.</li> 
  <li>For infinite populations and ranges, we can use the same argument (with calculus) to show this holds in general.</li>   
</ul>

========================================================

### The standard deviation and variance of probability distributions continued

<ul>
  <li>Using the derivation from the last slide,
  $$\sigma = \sqrt{\sum_{x_\alpha\in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2 },$$</li>
  <li>we can show directly that the <b style="color:#1b9e77">variance of a probability distribution</b> is given as,
  $$\sigma^2 = \sum_{x_\alpha \in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2 .$$</li>
  <li>We can also derive the alternative forms for the <b style="color:#1b9e77">population standard deviation and variance</b> in terms of the <b style="color:#1b9e77">probability distribution</b> as
  $$\begin{align}
  \sigma &= \sqrt{\sum_{x_\alpha \in \mathbf{R}} \big( x_\alpha P(X=x_\alpha) \big)^2  -  \mu^2 }, \\ \\
  \sigma^2 &= \sum_{x_\alpha\in\mathbf{R}} \big( x_\alpha P(X=x_\alpha) \big)^2  -  \mu^2  . 
  \end{align}$$</li>
  <li>This again just amounts to some algebraic manipulation and these forms are <strong>totally equivalent to the other forms</strong>.</li>
</ul>

========================================================

### Example of the variance and standard deviation

<div style="float:right; width:35%;text-align:center;">
<table class="fragment">
<tr>
  <th> <strong>Outcome </strong></th> <th> <strong>Observed value for $X=x$</strong> </th> <th><strong>Probability</strong></th></th>
</tr>
<tr>
<td> $\{H,H\}$ </td> <td>$x=2$</td> <td> $f(x)=\frac{1}{4}$</td></tr>
<tr><td>$\{H,T\}, \{T,H\}$</td> <td>$x=1$</td> <td>$f(x)=\frac{2}{4}$</td></tr>
<tr><td>$\{T,T\}$</td> <td>$x=0$</td> <td>$f(x)=\frac{1}{4}$</td></tr>
</table>
</div>
<div style="float:left; width:65%">
<ul>
<li>Let's recall the probability distribution for the two coin flipping experiment.</li>
<li>We showed already that $\mu = \mathbb{E}\left[X\right] =1$.</li>
<li> Thus we say that the  <b style="color:#1b9e77">expected value</b> of two coin flips is to observe <b>one heads</b>.</li>
<li>Recall that our variance is computed as
$$\sigma^2 = \sum_{x_\alpha \in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2$$</li>
</div>
<div style="float:left; width:100%">
<ul>
  <li>Using the above, we can show that
  $$\begin{align}
  \sigma^2 &= \sum_{x_\alpha \in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2\\
  &= \frac{1}{4}\left(2 - 1 \right)^2 + \frac{2}{4}\left(1 - 1\right)^2 + \frac{1}{4}\left(0 - 1\right)^2 \\
  &= \frac{1}{4} + \frac{1}{4} = \frac{1}{2}
  \end{align}$$</li>
  <li>Therefore, the standard deviation has to be given by
  $$\begin{align}
  \sigma = \sqrt{\frac{1}{2}}
  \end{align}$$
</ul>
</div>

========================================================

## Binomial distribution

<ul>
  <li>A coin flipping experiment is actually a simple example of a broad category of experiments.</li>
  <ul>
    <li>For example, we can consider an experiment in which we describe two possible outcomes:</li>
    <ol>
      <li><b>(success / $S$ / $1$ / $H$)</b>; or</li>
      <li><b>(failure / $F$ / $0$ / $T$)</b>.</li>
    </ol>
    <li>We can <strong>encode the outcomes</strong> any way we like;</li>
    <ul>
      <li>it is <strong>common to encode the outcomes as $S$ or $F$</strong>, where the choice of "success" is arbitrary.</li>  
    </ul>
      <li>More generally than coin flipping, we might <strong>consider the case where the probabilities</strong>,
      $$\begin{align}
      P(\text{success}) \neq P(\text{failure})
      \end{align}$$</li>
      <li>Recall, if the experiment has only <strong>two possible outcomes</strong>, if $A=$"success"   then $\overline{A}=$"failure".</li>
      <li>Therefore,
      $$\begin{align}
      P(\text{success}) + P(\text{failure}) = 1.
      \end{align}$$
      </li>
      <li>Suppose we run the experiment a total of $n$ trials.</li>
      <ul>
        <li>Because there are only <strong>two possible outcomes for each trial</strong>, and a <strong>finite number of trials</strong>, we can <b>create a list of all possible outcomes for $n$ trials</b>.</li>
        <ul>
          <li>For example, if there are <b>$x_\alpha$ total successes</b>, there must be exactly  <strong>$n - x_\alpha$ failures in $n$ trials</strong>.</li>
        </ul>
        <li>More importantly, we can also <strong>make a list of all possible ways</strong> we can have <b>$x_\alpha$ successes</b> and <b>$n-x_\alpha$ failures</b>.</li>
        <li>Let <b>$X$ be the random variable</b> equal to the <strong>number of successful trials</strong> --  we can therefore calculate the probability,
        $$P(X = x_\alpha);$$
        however, the <b>classical model for probability</b> (equal probability of all outcomes) <strong> will no longer apply</strong>.</li>  
      </ul> 
  </ul>
</ul>


========================================================

### Binomial distribution continued

<ul>
  <li>Recall our <b>random variable $X$</b> equal to the <strong>number of successful trials</strong> in $n$ total trials.</li>
  <ul>
    <li>Unlike with coin flipping, we suppose that <strong>it is possible for</strong>
    $$\begin{align}
      P(\text{success}) \neq P(\text{failure})
      \end{align}$$</li>
  </ul>
  <li>However, there are finite trials, finite possible outcomes and, for <strong>each possible number of successes $x_\alpha$</strong>, there are a <b>finite number of ways $X=x_\alpha$</b>.</li>
  <ul>
    <li>Provided <strong>all trials are independent</strong> (like coin flipping) and the <strong>probability of success is constant</strong>, we can still make a counting argument using</li>
    <ol>
      <li>The rule of complementary probability
      $$\begin{align}
      P(\text{success}) + P(\text{failure}) = 1;
      \end{align}$$</li>
      <li>independence;</li>
      <li>the list of all possible ways we can make $x_\alpha$ successes;</li>
      <li>the list of all possible ways we can make $n-x_\alpha$ failures; and</li>
      <li>a total of $n$ trials exactly;</li>
    </ol>
    <li>to compute the probability exactly for each $x_\alpha$ where $x_\alpha$ ranges from $0, 1, \cdots, n$.</li>
    <li>The list of all possible number of successes $x_\alpha = 0, 1, \cdots, n$ and the associated probabilities $P(X= x_\alpha)$ for $x_\alpha = 0, 1, \cdots, n$ is called the <b>binomial distribution</b>.</li>
  </ul>
  <li>The argument itself is somewhat long, but it really only uses tools we already know.</li>
  <ul>
    <li>Therefore, if you can understand the principles of the points 1 - 5 above, we don't need to belabor the details.</li> 
  </ul>
</ul>


========================================================

### Binomial distribution continued

<ul>
  <li>Formally, we will now describe the <b>binomial distribution</b>.</li>
  <ul>
    <li>Suppose we run an experiment with <b>two possible outcomes</b> <strong>$S=$"success"</strong> and <strong>$F=$"failure"</strong>, where
    $$\begin{align}
    P(S) = p  && P(F) = 1 - P(S) = q.
    \end{align}$$</li>
    <li>Suppose we <b>run exactly $n$ total trials</b> of the above experiment and suppose that:</li>
    <ol>
      <li><strong>each trial is independent</strong>; and
      <li><strong>$P(S)=p$ for every trial</strong>.</li>
    </ol>
    <li>Let <b>$x$ be the random variable</b> equal to the <strong>total number of successful trials</strong>.</li>  
    <li>Let <b>$x_\alpha$</b> be one of the <strong>possible number of successful trials</strong> in the range $0, 1 ,\cdots , n$.</li>
    <ul>
      <li>Then the probability of <strong>exactly $x_\alpha$ successful trials (the event $X= x_\alpha$)</strong> is given by
      $$\begin{align}
      P(X=x_\alpha) =  \frac{n!}{\left( n  - x_\alpha\right)! x_\alpha !} p^{x_\alpha} q^{(n - x_\alpha)},
      \end{align}$$</li>
      <li>where:</li>
     <ol>
       <li>The total number of ways that we can have exactly $x_\alpha$ successes in $n$ trials is given by
        $$ \frac{n!}{\left( n  - x_\alpha\right)! x_\alpha !}. $$
        </li>
        <li>The probability of $x_\alpha$ independent successes (or $n-x_\alpha$ independent failures) is $p^{x_\alpha}$ $\big($ or $q^{(n-x_\alpha)}\big)$ respectively.
    </ul> 
  </ul>   
</ul> 


========================================================

## Binomial distribution example 

<ul>
  <li>Recall our notation:
  <ol>
    <li><b>$n$</b> - <strong>the number of trials</strong>;</li>
    <li><b>$x$</b> - <strong>the random variable</strong>;</li>
    <li><b>$x_\alpha$</b> - <strong>a specific number of successes that $X$ could possibly attain</strong>;</li>
    <li><b>$P(S)= p$</b> - <strong>the probability of an independent trial's success</strong>;</li>
    <li><b>$P(F)=q$</b> - <strong>the probability of an independent trial's failure</strong>.</li>
  </ol>
  <li>Consider that when an <strong>adult is randomly selected with replacement</strong>, there is a <b>$0.85$ probability</b> that this <strong>person knows what Twitter is</strong> (based on results from a Pew Research
Center survey from several years ago...).</li>
  <li> Suppose that we want to find the probability that exactly three of
five random adults know what Twitter is.</li>
  <li><b>Q:</b> can you identify what $n$, $X$, $x_\alpha$, $p$ and $q$ are in the above word problem?</li>
  <ul>
    <li>Here we consider the <b>random selection to be a "trial"</b> so that the number of trials is $n=5$</li>
    <li>If we consider a <b>"successful" trial</b> to be <strong>"select an adult who knows what Twitter is"</strong>, then <b>$X$</b> is <strong>"number of adults who know what Twitter is out of five"</strong>.</li>
    <li><b>$x_\alpha$</b> is the <strong>specific number of successful trials</strong> we are interested in, i.e., <b>$x_\alpha = 3$</b>.</li>
    <li><b>$p$ is the probability of an independent trial's successs</b>, i.e, $p=0.85$</li>
    <li><b>$q$ is the probability of an independent trial's failure</b>, i.e., $q=1-p = 0.15$.</li>
  </ul>
</ul>

========================================================

### Binomial distribution example continued

<ul>
  <li>Let's recall our values from the last slide,
  <ul>
   <li>Here we consider the <b>random selection to be a "trial"</b> so that the number of trials is $n=5$</li>
    <li>If we consider a <b>"successful" trial</b> to be <strong>"select an adult who knows what Twitter is"</strong>, then <b>$X$</b> is <strong>"number of adults who know what Twitter is out of five"</strong>.</li>
    <li><b>$x_\alpha$</b> is the <strong>specific number of successful trials</strong> we are interested in, i.e., <b>$x_\alpha = 3$</b>.</li>
    <li><b>$p$ is the probability of an independent trial's success</b>, i.e, $p=0.85$</li>
    <li><b>$q$ is the probability of an independent trial's failure</b>, i.e., $q=1-p = 0.15$.</li>
  </ul>
  <li>Suppose we wanted to compute the probability of one particular outcome,</li>
  <ul>
    <li>say, $S_i =$"the $i$-th particpant knows what Twitter is" and $F_i=$"the $i$-th participant does not know what twitter is", where
  $$A = S_1 \text{ and } S_2 \text{ and } S_3 \text{ and } F_4 \text{ and } F_5.$$</li>
    <li>We can use <b>independence</b> and the <b>multiplication rule</b> to show
    $$\begin{align}
    P(A) &= P(S_1)\times P(S_2)\times P(S_3)\times P(F_4)\times P(F_5) \\
    &= 0.85 \times 0.85 \times 0.85 \times 0.15 \times 0.15 \\
    &= 0.85^3 \times 0.15^2.
    \end{align}$$
  </ul>
  <li>This shows how we get one part of the binomial distribution formula.</li>
  <ul>
    <li>However, there are many combinations of $S_i$ and $F_i$ that arise in $x=3$.</li>
  </ul>
  <li>Using a counting argument, we can show that the total number of ways $x=3$ is
  $$\begin{align}
  \frac{n!}{(n- x_\alpha)! x_\alpha!} = \frac{5!}{(5 - 3)! 3!} = \frac{5!}{(2)!3!} = 10
  \end{align}$$
</ul>

========================================================

### Binomial distribution example continued

<ul>
  <li>Let's recall our values from the last slide,
  <ul>
   <li>Here we consider the <b>random selection to be a "trial"</b> so that the number of trials is $n=5$</li>
    <li>If we consider a <b>"successful" trial</b> to be <strong>"select an adult who knows what Twitter is"</strong>, then <b>$X$</b> is <strong>"number of adults who know what Twitter is out of five"</strong>.</li>
    <li><b>$x_\alpha$</b> is the <strong>specific number of successful trials</strong> we are interested in, i.e., <b>$x_\alpha = 3$</b>.</li>
    <li><b>$p$ is the probability of an independent trial's successs</b>, i.e, $p=0.85$</li>
    <li><b>$q$ is the probability of an independent trial's failure</b>, i.e., $q=1-p = 0.15$.</li>
    <li>The <b>total number of ways $x=3$</b> is
  $$\begin{align}
  \frac{n!}{(n- x_\alpha)! x_\alpha!} = \frac{5!}{(5 - 3)! 3!} = \frac{5!}{(2)!3!} = 10
  \end{align}$$</li>
  </ul>
  <li>The binomial distribution formula can then be read as,</li>
  <ul>
    <li>The probability of finding exactly $3$ out of $5$ independently, randomly selected adults who know what Twitter is, is equal to
    $$\begin{align}
    \frac{n!}{(n- x_\alpha)! x_\alpha!} p^{x_\alpha} q^{n- x_\alpha} = 10 \times 0.85^3 \times 0.15^2 \approx 0.138,
    \end{align}$$</li>
  <li>or in plain English,
  <blockquote>
  the probability of three independent successful trials, times the probability of two indepdendent failure trials, times all possible ways we can have exactly $3$ successful trials out of five.
  </blockquote></li>
  <li>Again, the counting argument follows directly from the material we studied before the midterm.</li>
</ul> 

========================================================

### Binomial distribution example continued

* Let's now take a graphical look at the last problem in R.

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(c(0:5), dbinom(c(0:5), size=5, prob=0.85),  xlab="Probability", ylab="Number of Successses", cex=3, cex.lab=3, cex.axis=2.5)
```

* In the above, the `dbinom` is the function for the probability mass function for the binomial distribution.

* We are setting a range of values to plot `c(0:5)` and the size of the trial `size=5` and the probability of succcess `prob=0.85`.

========================================================

### Binomial distribution example continued

<ul>
  <li>We should remark the following on the last calculation.</li>
  <ul>
    <li>Technically, we could only make use of the binomial distribution because we <b>sampled with replacement</b> to enforce <strong>independent trials</strong>.</li>
    <li>If sampled our population <b>without replacement</b>, we know</li>
    <ol>
      <li>that <strong>the trials are dependent</strong>; and</li>
      <li>that the <strong>probability of success changes at each trial</strong>.</li> 
    </ol>
    <li>These conditions make it so the <strong>binomial distribution does not apply</strong> to the random variable $x$ <b>when we do not replace samples</b>.</li>
    <li>However it is common to <b>approximate sampling without replacement as independent</b> when the <strong><b style="color:#d95f02">sample size</b> is less than $5\%$ of the <b style="color:#1b9e77">population</b></strong>.</li>
    <li>In practice for polls of, e.g., all US adults, this approximation will often be used.</li>
  </ul>
</ul>

========================================================

## Parameters of the binomial distribution

<div style="float:left; width:65%;text-align:center;">
<img src="sample_space.png" style="width:100%" alt="Random variables are the numerical measure of the outcome of a random process.">
<p style="text-align:center">
Courtesy of Ania Panorska  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a>
</div>  
<div style="float:left; width:35%">
<ul>
  <li>We saw earlier the following definitions for the mean and the standard deviation of a probability distribution:
  <ul>
  <li>Suppose we have a random variable $x$ which assigns a numerical value to each outcome in the sample space $\mathbf{S}$.</li>
  <li>Suppose all values that $x$ can attain are given by a collection $\{x_\alpha\}$ in the range $\mathbf{R}$ of $x$.</li>
</ul>
</div>
<div style="float:left;width:100%">
<ul>
  <li>Then the <b style="color:#1b9e77">mean (or expected value) of the probability distribution</b> is given,
    $$\mu = \sum_{x_\alpha \in \mathbf{R}} x_\alpha P(x=x_\alpha)$$ </li>
  <li>The <b style="color:#1b9e77">standard deviation of the probability distribution</b> is given
    $$\sigma = \sqrt{\sum_{x_\alpha\in \mathbf{R}} P(x=x_\alpha) \left(x_\alpha - \mu\right)^2 }$$</li>
    <li>These formulas hold for all probability distributions (with a slight modification when the variable is continuous by using calculus).</li>
  </ul>
</ul>

========================================================

### Parameters of the binomial distribution continued

<div style="float:left; width:45%;text-align:center;">
<img src="binomial_distribution.png" style="width:100%" alt="Random variables are the numerical measure of the outcome of a random process.">

Public domain via  
        <a href="https://commons.wikimedia.org/wiki/File:Binomial_distribution_pmf.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:55%">
<ul>
  <li>The binomial distribution has a very nice structure so that the <b style="color:#1b9e77">parameters</b> have a nice form.</li>
  <li>For the <b>binomial distribution</b> the <b style="color:#1b9e77">mean</b> is given as,
  $$\mu = n \times p .$$</li>
    <li>For the <b>binomial distribution</b> the <b style="color:#1b9e77">variance</b> is given as,
  $$\sigma^2 = n \times p \times q.$$</li>
      <li>For the <b>binomial distribution</b> the <b style="color:#1b9e77">standard deviation</b> is given as,
  $$\sigma = \sqrt{ n \times p \times q}.$$</li> 
  <li><b>Q:</b> what is $\mu$ and $\sigma$ for the binomial distribution for $20$ trials and probability of success $p=0.5$?</li>
  <ul>
    <li>Notice that these are given as,
    $$\begin{align} \mu = 20 \times 0.5 = 10 & & \sigma = \sqrt{20 \times 0.5 \times 0.5} = \sqrt{ 5}\end{align}$$</li>
  </ul>
</ul> 
</div>
<div style="width:100%; float:left">
<ul>
  <li><b>Q:</b> what is $\mu$ and $\sigma$ for the binomial distribution for $40$ trials and probability of success $p=0.5$?</li>
  <ul>
    <li>Notice that these are given as,
    $$\begin{align} \mu = 40 \times 0.5 = 20 & & \sigma = \sqrt{40 \times 0.5 \times 0.5} = \sqrt{ 10}\end{align}$$</li>
  </ul>
    <li><b>Q:</b> what is $\mu$ and $\sigma$ for the binomial distribution for $20$ trials and probability of success $p=0.7$?</li>
  <ul>
    <li>Notice that these are given as,
    $$\begin{align} \mu = 20 \times 0.7 = 14 & & \sigma = \sqrt{20 \times 0.7 \times 0.3} = \sqrt{ 4.2}\end{align}$$</li>
  </ul>
</ul>
</div>

========================================================

## Review of the binomial distribution

<ul>
  <li>The binomial distribution is a key distribution that gives us a way to model a wide range of experiments probabilistically.</li>
  <li>This applies when we run an experiment with <b>two possible outcomes</b> <strong>$S=$"success"</strong> and <strong>$F=$"failure"</strong>, where
    $$\begin{align}
    P(S) = p  && P(F) = 1 - P(S) = q.
    \end{align}$$</li>
    <li>When we <b>run exactly $n$ total trials</b> of the above experiment, assuming that:</li>
    <ol>
      <li><strong>each trial is independent</strong>; and
      <li><strong>$P(S)=p$ for every trial</strong>.</li>
    </ol>
  <li>We can model the probability of a particular number of successes $x_\alpha$ like a (possibly) non-fair coin flipping experiment.</li>
  <li>We model the probability of exactly $x_\alpha$ successful trials as 
  $$\begin{align}
  P(x=x_\alpha) =  \underbrace{\frac{n!}{\left( n  - x_\alpha\right)! x_\alpha !}}_{(1) } \times \underbrace{ p^{x_\alpha}}_{(2)} \times  \underbrace{q^{(n - x_\alpha)}}_{(3)}
  \end{align}$$  
  where:
  </li> 
  <ol type="A">
    <li>Total number of ways to find exactly <b>$x_\alpha$ successful trials</b> out of <strong>$n$ total trials</strong>;</li>
    <li><b>Probability</b> of <strong>$x_\alpha$ independent succesful trials</strong>;</li>
    <li><b>Probability</b> of <strong>$n-x_\alpha$ independent failure trials</strong>;</li>
  </ol> 
  <li>The special structure of this distribution also allows us to compute the mean and standard deviation directly as
  $$\begin{align} \mu = n \times p  & & \sigma = \sqrt{n \times p\times q} \end{align}$$
</ul>

