<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>
<!-- foo 
Highlighting
bold
strong
orange <b style="color:#d95f02"> text </b>
green  <b style="color:#1b9e77"> text </b>
purple <b style="color:#d24693"> text </b>
red    <b style="color:#FF0000"> text </b>
blue   <b style="color:#0000FF"> text </b>
-->

Continuous probability distributions and their parameters
========================================================
date: 03/08/2021
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:

  * Continuous probability distributions
  * The probability density function
  * The cumulative distribution function
  * The expected value of a continuous random variable
  * The expected value of a function of a continuous random variable
  * The standard deviation and variance of a continuous random variable

<!-- 3.2 -->

========================================================

## Motivation

<div style="float:left; width:50%;text-align:center;">
<img src="sample_space.png" style="width:100%" alt="Random variables are the numerical measure of the outcome of a random process.">
<p style="text-align:center">
Courtesy of Ania Panorska  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a>
</div>  
<div style="float:right;width:50%">
<ul>
  <li>Recall, that we have a separate kind of model for random experiments in which the units of measurement can be arbitrarily sub-divided.</li>
  <li>A good example to think of is if $X$ is the daily high temperature in Reno in degrees Celsius.</li>
  <li>If we had a sufficiently accurate thermometer, we could <b>measure $X$ to an arbitrary decimal place</b> and it would make sense.</li>
  <li>$X$ thus takes today's weather from the outcome space and <strong>gives us a number in a continuous unit of measurement</strong>.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <ul>
    <li style="list-style-type:none"><blockquote><b>Continuous random variable</b> is a random variable with an interval (either finite or infinite) of real numbers for its range.</blockquote></li>
    <li>The range of such a random variable is <strong>uncountably infinite</strong>.</li>
    <li>This is to say that if $X$ is a <b style="color:#d95f02">continuous random variable</b>, there is no possible index set $\mathcal{I}\subset \mathbb{Z}$ which can enumerate the possible values $X$ can attain.</li>
    <li> For <b style="color:#1b9e77">discrete random variables</b>, we could perform this with a possibly <b style="color:#1b9e77">infinite index set</b>, $\{x_j\}_{j=1}^\infty$</li>
    <li> This has to do with how the <b style="color:#d95f02">infinity of the continuum $\mathbb{R}$</b> is actually <strong>larger than</strong> the <b style="color:#1b9e77">infinity of the counting numbers, $\aleph_0$</b>;</li>
    <li> in the <b style="color:#d95f02">continuum</b> you can <b style="color:#d95f02">arbitrarily sub-divide the units of measurement</b>.</li>
    <li>The fact that a continuous sample space has an uncountably infinite number of outcomes eliminates the option
of assigning a probability to each point as we did in the discrete case with the mass function.</li> 
  </ul>
</ul>
</div>

========================================================

### Motivation Continued


<div style="float:left; width:50%">
<ul>
  <li>We will instead look empirically at how we can construct a continuous probability as a <b>density</b>.</li>
  <li>Suppose an electronic surveillance monitor is turned on briefly at the beginning
of every hour and has a $0.905$ probability of working properly, regardless of how long
it has remained in service. </li>
  <li>Let the random variable $X$ denote the hour at which the monitor <b>first fails</b> and $A_k$ represent the event that the monitor fails in the $k$-th hour.</li> 
  <li>Then the probability mass  $f(k)$ is the product of $k$ individual probabilities:
  $$\begin{align}
  f(k) &= P(X= k)\\
       &= P\left( A_k  \cap_{i=1}^{k-1}\overline{A}_i \right)\\
       &=(0.905)^{k-1} \times (0.095)
  \end{align}$$
  for any given $k=1, 2, \cdots$</li>
  <li>We plot the histogram for the probability mass function to the right</li>
</ul>
</div>

<div style="float:right; width:50%;text-align:center;" class="fragment">
<img src="exponential_hist.png" style="width:100%" alt="Geometrically decaying histogram.">
<p style="text-align:center">
Courtesy of Larsen &amp; Marx. <em>An Introduction to Mathematical Statistics and Its Applications</em>. 6th Edition.
</p>
</div>


========================================================

### Motivation Continued


<div style="float:left; width:50%">
<ul>
  <li>Now we superimpose the exponential curve $y = 0.1e^{−0.1x}$ onto the histogram.</li> 
</ul>
</div>
<div style="float:right; width:50%;text-align:center;" class="fragment">
<img src="exponential_curve.png" style="width:100%" alt="Geometrically decaying histogram.">
<p style="text-align:center">
Courtesy of Larsen &amp; Marx. <em>An Introduction to Mathematical Statistics and Its Applications</em>. 6th Edition.
</p>
</div>
<div style="float:left; width:50%">
<ul>
<li>Notice how closely the area under the curve approximates the area of the bars.</li> 
  <li>It follows that the probability that $X$ lies in some given interval will be numerically similar to the integral of the exponential curve above that same interval.</li>
  <li>For example, the probability that the monitor fails sometime during the first four
hours would be the sum
  $$\begin{align}
  P(1\leq X \leq 4) &= \sum_{k=1}^4 f(k) \\
  &= (0.905)^{k-1} \times (0.095)\\
  &\approx 0.3297
  \end{align}$$</li>
</ul>
</div>
<div style="float:left; width:100%">
  <li>To four decimal places, the corresponding area under the exponential curve representing continuous time between $X=0$ and $X=4$ is the same:
  $$\begin{align}
  \int_{0}^4 0.1 e^{-0.1 x}\mathrm{d}x \approx 0.3297
  \end{align}$$
</ul>
</div>




========================================================

### Motivation Continued

<div style="float:left;width:45%">
<ul>
  <li>We can generally use the idea of fitting a continuous probability function to approximate an integer-
valued discrete probability model.</li>
  <li>The "trick" is to replace the spikes that define $f(x)$ with rectangles whose heights are $f(x)$ and whose widths are $1$.</li> 
  <li>Doing that makes the sum of the areas of the rectangles corresponding to $f(x)$ equal to $1$;</li>
  <ul>
    <li>this is the same as the total area under the approximating continuous probability function.</li> 
  </ul>
  <li>Because of the equality of those two areas, it makes sense to superimpose (and compare) the "histogram" of
$f(x)$ and the continuous probability function on the same set of axes.</li>
  <li>Imagine that we are forming a <strong>frequency histogram</strong> for the measurements from a random experiment with a <b>continuous random variable</b> $Y$.</li>
  <li>Suppose we have measurements $y_1,\cdots, y_n$ which we will bin into rectangles over the range for $Y$.</li>
</ul>
</div>
<div style="float:right; width:50%;text-align:center;" class="fragment">
<img src="uniform_table.png" style="width:100%" alt="Table of uniformly distributed measurements.">
</div>

<div style="float:left; width:45%">
<ul>
  <li>Grouping those $y_i$’s into five classes, each of width $10$, produces the distribution and histogram
pictured below.</li>
</ul>
</div>

<div style="float:right; width:55%;text-align:center;" class="fragment">
<img src="uniform_frequency.png" style="width:100%" alt="Table of uniformly distributed measurements.">
<p style="text-align:center">
Courtesy of Larsen &amp; Marx. <em>An Introduction to Mathematical Statistics and Its Applications</em>. 6th Edition.
</p>
</div>


========================================================

### Motivation Continued

* Suppose we think that the probability of a measurement lying in one of the five classes

  $$\begin{align}
  \{[20,30), [30,40), [40, 50), [50, 60), [60, 70]\}= \cup_{i=1}^5 A_i
  \end{align}$$
  is equally likely.
  
* Then we could assign a probability mass function as a discrete random variable 
  
  $$\begin{align}
  f(X \text{ in } A_i )= \frac{1}{70 - 20} = \frac{1}{50} & & \text{for }i=1,\cdots, 5
  \end{align}$$

* Note, however, that $f(X)$ and the histogram are not compatible in the sense that the area under $f(x)$ should be $1$ but the sum of the areas of the bars of the histogram is $400$:

  $$\begin{align}
  \text{histogram area} = \sum_{i=1}^5 \text{width of }A_i \times \text{height of }A_i
  = 10(7) + 10(6) + 10(9) + 10(8) + 10(10)
  = 400
  \end{align}$$

* Nevertheless, we can make the total area of the five bars to match the area under $f(x)$ by redefining the scale of the vertical axis on the histogram...

========================================================

### Motivation Continued
<div>
  <ul>
    <li>Specifically, <strong>frequency needs to be replaced</strong> with the analog of <b>probability density</b>;</li>
    <ul>  
      <li> intuitively, the density associated with, e.g., the interval $[20, 30)$ would be defined as the quotient
      $$\begin{align}
      \text{Density of a class}& = \frac{\text{Frequency of measurement}}{\text{Total number of all measurements }\times\text{Class width}} \\
      &= \frac{7 }{40 \times 10}
      \end{align}$$
      </li>
    </ul>
    <li>Applying this argument to each class and computing the associated density values, we plot this in the below:</li>
  </ul>
</div>
<div style="float:left; width:80%" class="fragment">
<img src="uniform_density.png" style="width:100%" alt="Table of uniformly distributed measurements.">
<p style="text-align:center">
Courtesy of Larsen &amp; Marx. <em>An Introduction to Mathematical Statistics and Its Applications</em>. 6th Edition.
</p>
</div>
<div style="float:left; width:100%">
<ul>
  <li>In this graph above, the sum of the areas under the density bars and the curve $f(x)$ both equal $1$, corresponding to an <b>empirical probability distribution</b>.</li>
</ul>
</div>


========================================================

### Motivation Continued

<div style="float:left; width:60%">
<ul>
  <li>Now imagine that we are modeling an unknown probability distribution of a continuous random variable.</li>
  <li>Suppose for a generic sample of size $N$, $x_1, \cdots, x_N$, that we construct a density histogram as in the last example;</li>
  <ul>
    <li>let's construct classes of equal width $\frac{1}{\sqrt{N}}$ and the frequency of measurement in the class $i$ given by $n_i$.</li>
  </ul>
  <li>The associated density of the class $i$ would be given as
  $$\begin{align}
  f_N(X \text{ in class } i) = \frac{n_i}{N \times \frac{1}{\sqrt{N}}} = \frac{n_i}{\sqrt{N}}.
  \end{align}$$</li>
  <li>The sum of the area over all classes would also equal
  $$\begin{align}
  \text{Area of density histogram} &= \sum_{i} f_N(X \text{ in class } i)\times \frac{1}{\sqrt{N}} \\
  &=\sum_i \frac{n_i}{\sqrt{N}}\times \frac{1}{\sqrt{N}}\\
  & =  \sum_i \frac{n_i}{N} = 1
  \end{align}$$</li>
  <li>We can imagine then taking the sample size $N\rightarrow \infty$ and refining this argument with smaller and smaller class sizes $\frac{1}{\sqrt{N}}$, corresponding to <strong>arbitrarily sub-dividing the units of measurement</strong>:</li>
</ul>
</div>

<div style="float:right; width:40%;text-align:center;" class="fragment">
<img src="Riemann_sum_(middlebox).gif" style="width:100%" alt="Random variables are the numerical measure of the outcome of a random process.">
<p style="text-align:center">
Courtesy of <a href="https://commons.wikimedia.org/wiki/File:Riemann_sum_(middlebox).gif">09glasgow09</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>, via Wikimedia Commons</p>
</div>  
<div style="float:right; width:40%">
<ul>
  <li>This heuristic argument would give us something similar to a Riemann sum as above, though the precise technical details are more complicated.</li>
</ul>
</div>

========================================================

## Probability density functions

<ul>
  <li> Intuitively, however, this gives the picture of how to represent the <strong>probability of a continuous random variable</strong>.</li>
  <li style="list-style-type:none;"><blockquote>For a continuous random variable $X$, a <b>probability density function</b> is a function $f$ such that
  <ol>
    <li>$f(x)\geq 0$ for all $x\in \mathbb{R}$.</li>
    <li>$\int_{-\infty}^\infty f(x)\mathrm{d}x = 1$</li>
    <li>$P(a \leq X \leq b) = \int_{a}^b f(x)\mathrm{d}x = \text{The area under the density curve }f(x)\text{ for any }a\leq b$</li>
  </ol>
</ul>
</blockquote></li>
  <li>The notion of the density curve thus directly extends the idea of the probability mass function for discrete random variables.</li>
  <li>However, a key difference is that
  $$\begin{align}
  \int_{a}^a f(x) \mathrm{d}x = P(X=a)=0
  \end{align}$$
  for any value $a$.</li>
  <ul>
    <li>I.e., <strong>the probability of any single point measurement is always zero</strong>.</li>
    <li>Particularly, we can only compute non-zero probability in ranges for continuous random variables.</li>
  </ul>
</ul>

========================================================

### Probability density functions continued

<ul>
  <li>Based on this last result, it might appear that our model of a continuous random variable is useless.</li>
  <li>However, in practice, when a particular current measurement, such as $14.47$ milliamperes,
is observed, this result can be interpreted as the <strong>rounded value of a current measurement</strong>
that is actually in a range such as $14.465\leq x \leq 14.475$.</li> 
  <li>Therefore, the probability that the rounded value $14.47$ is observed as the value for X is the probability that X assumes a value in the interval 
  $$[14.465, 14.475],$$ 
  which is not zero.</li>
  <li>Similarly, because each point has zero probability, one need not distinguish between inequalities such as $<$ or $\leq$ for continuous random variables.</li>
    <li style="list-style-type:none;"><blockquote>For a continuous random variable $X$, for any $x_1< x_2$
    $$P(x_1 \leq X \leq x_2 ) = P(x_1 < X \leq x_2) = P(x_1 \leq X < x_2) = P(x_1 < X < x_2).$$
</blockquote></li>
</ul>

========================================================

### Probability density function example

<div style="float:left; width:65%">
<ul>
  <li>Let the continuous random variable $X$ denote the current measured in a thin copper wire in milliamperes.</li> 
  <li>Assume that the range of $X$ is $[4.9, 5.1]$ mA, and assume that the probability density function of $X$ is $f(x) = 5$ for $4.9 ≤ x ≤ 5.1$.</li>
  <li>Notice that the width of the total range is
  $$5.1-4.9 = 0.2$$
  such that
  $$\text{Area under the density curve} = 5 \times 0.2 = 1.$$</li>
  <li>Suppose we want to calculate the probability that a current measurement is less than 5 milliamperes.</li>
  <li>The probability density function is shown to the right, with the probability the shaded area under the curve.</li>
</ul>
</div>
<div style="float:right; width:35%; text-align:center" class="fragment">
<img src="probability_density_1.png" style="width:100%" alt="Uniform density">
<p style="text-align:center">
Courtesy of Montgomery & Runger, <em>Applied Statistics and Probability for Engineers</em>, 7th edition
</p>
</div>
<div style="float:left; width:100%">
<ul>
  <li>It is assumed that $f(x) = 0$ wherever it is not specifically defined.</li>
  <li>We can thus make the computation for the area directly as
  $$\begin{align}
  \int_{4.9}^{5.0} f(x) \mathrm{d}x &= \int_{4.9}^{5.0} 5.0\mathrm{d}x \\
                                    & = 5.0 x \vert_{4.9}^{5.0} \\
                                    &= 5.0(0.1) = 0.5
  \end{align}$$ </li>
  <li>The above is precisely equivalent to calculation half of the nonzero area under the density.</li>
</ul>
</div>


========================================================

## Cumulative distribution function

<ul>
  <li>Recall how we defined the cumulative distribution function for a discrete random variable.</li>
  <li>We will suppose that in the first place, the possible-to-measure values are given by $\{x_i\}$; then
  $$F(x) = P(X \leq x_i) = \sum_{j\leq i} f(x_j)$$
  </li>
  <li>If we follow the analogy above, and recall our definition of the <b>probability density function</b>, the next definition is a straight-forward extension.</li>
  <li style="list-style-type:none;"><blockquote>For a continuous random variable $X$, a <b>cumulative distribution function</b> is a function $F$ such that
  <ol>
    <li>$F(x) = P(X\leq x) = \int_{-\infty}^x f(u)\mathrm{d}u$  given a density function $f$.</li>
    <li>$f(x) = \frac{\mathrm{d}}{\mathrm{d}x} F(x)$ given that $F(x)$ is differentiable.</li>
  </ol>
  </blockquote></li>
  <li>Given the two statements above, it follows easily from the <strong>fundamental theorem of calculus</strong> that we can compute the probability of a range equivalently by the following:
  $$P(a \leq X\leq b)= \int_{a}^b f(x)\mathrm{d}x = \int_a^b \frac{\mathrm{d}}{\mathrm{d}x} F(x)\mathrm{d}x = F(b) - F(a) = P(X\leq b) -P(X\leq a)$$
</ul>

========================================================

### Cumulative distribution function example

<div style="float:left; width:60%">
<ul>
  <li>For the copper current measurement in the previous example, let's calculate the cumulative distribution function.
  <li>We recall that the random variable $X$ has a probability density defined as
  $$\begin{align}
  f(x) = \begin{cases}
  5.0 & x\in[4.9, 5.1]\\
  0.0 & \text{else}
  \end{cases}
  \end{align}$$</li>
  <li>Thus we can use the previous relationships to derive the cumulative distribution function as
  $$\begin{align}
  F(x) &= \int_{-\infty}^x \frac{\mathrm{d}}{\mathrm{d}{u}} F(u)\mathrm{d}u \\
       &= \int_{4.9}^x f(u)\mathrm{d}u
  \end{align}$$</li>
  <li>Using the defintion of where $f$ is nonzero, we find
  $$\begin{align}
  F(x)&= \begin{cases}
       0.0 & x \in(-\infty, 4.9)\\
       5.0\left(x - 4.9\right) & x \in[4.9, 5.0]\\
       1.0 & x \in (5.0,\infty)
       \end{cases}
  \end{align}$$</li>
  <li>The resulting cumulative distribution function is pictured to the right.</li>
</div>
<div style="float:right; width:35%; text-align:center" class="fragment">
<img src="cumulative_distribution.png" style="width:100%" alt="Uniform cumulative distribution.">
<p style="text-align:center">
Courtesy of Montgomery & Runger, <em>Applied Statistics and Probability for Engineers</em>, 7th edition
</p>
</div>
<div style="float:right; width:35%">
<ul>
  <li>Notice that the shape of the cumulative distribution starts at zero and increases until it reaches one.</li>
  <li>This is a general property due to the facts that,
  $$\begin{align}
  f(x)&\geq 0 \\
  \int_{-\infty}^\infty f(x) \mathrm{d}x& = 1
  \end{align}$$
</ul>
</div>

========================================================

## Expected value of a continuous random variable


<ul>
  <li>Now recall how we defined the expected value for a discrete random variable.</li>
  <li>We will suppose that in the first place, the possible-to-measure values are given by $\{x_i\}$; then
  $$\mu = \sum_{i}x_i P(X = x_i) = \sum_{i} x_if(x_i)$$
  </li>
  <li>If we follow the analogy above, and recall our definition of the <b>probability density function</b>, the next definition is also a straight-forward extension.</li>
  <li style="list-style-type:none;"><blockquote>For a continuous random variable $X$, the <b>expected value</b> $\mu$ is given as
  $$\mu = \mathbb{E}\left[X\right]= \int_{-\infty}^{\infty} x f(x)\mathrm{d}x$$
  </blockquote></li>
  <li>It also follows directly that if we have a continuous random variable $X$ and a function $h(X)$, we can write:</li>
  <li style="list-style-type:none;"><blockquote>For a continuous random variable $X$, the <b>expected value of $h(X)$</b> is given as
  $$\mathbb{E}\left[ h(X)\right] = \int_{-\infty}^{\infty} h(x) f(x)\mathrm{d}x$$
  </blockquote></li>
  <li>In the special case that $h(X) = aX + b$ for any constants $a$ and $b$, 
  $$\mathbb{E}[h(X)] = a\mathbb{E}(X) + b.$$</li> 
  <li>This can be shown from the <b>linearity properties</b> of integrals.</li>
</ul>


========================================================

### Example of the expected value of a continuous random variable

* Consider the copper wire example with density function

  $$\begin{align}
  f(x) = \begin{cases}
  5.0 & x\in[4.9, 5.1]\\
  0.0 & \text{else}
  \end{cases}
  \end{align}$$
  
* If we wish to take the expected value of the current, we would find,

  $$\begin{align}
  \mu &= \int_{4.9}^{5.1} x f(x) \mathrm{d}x  \\
      &= \int{4.9}^{5.1} 5.0x\mathrm{d}x \\
      &= \frac{5.0}{2.0}x^2 \vert_{4.9}^{5.1} \\
      &= 2.5(5.1)^2 - 2.5(4.9)^2 =5.0 
  \end{align}$$

* Recall, we described the mean before as the <b>center of mass</b>;

  * this interpretation holds for the expected value as described above where for the uniform density between $4.9$ and $5.1$ the center of mass is the midpoint at $5.0$.
  
  

========================================================

## Standard deviation and variance of a continuous random variable

<ul>
  <li>We can also follow the analogy with how we defined the standard deviation and variance for a discrete random variable.</li>
  <li>We will suppose that in the first place, the possible-to-measure values are given by $\{x_i\}$; then
  $$\begin{align}
  \sigma^2 &= \sum_{i}P(X = x_i) \left(x_i - \mu\right)^2 = \sum_{i} f(x_i) \left( x_i - \mu\right)^2\\
  \sigma   &= \sqrt{\sum_{i}P(X = x_i) \left(x_i - \mu\right)^2 }= \sqrt{\sum_{i} f(x_i) \left( x_i - \mu\right)^2}
  \end{align}$$
  </li>
  <li>If we follow the analogy above, and recall our definition of the <b>probability density function</b>, the next definition is also a straight-forward extension.</li>
  <li style="list-style-type:none;"><blockquote>For a continuous random variable $X$, the <b>variance</b> $\sigma^2$ and <b>standard deviation</b> are given as
  $$\begin{align}
  \sigma^2 &=  \int_{-\infty}^{\infty}  f(x)\left(x - \mu\right)^2\mathrm{d}x \\
  \sigma &=  \sqrt{\int_{-\infty}^{\infty}  f(x)\left(x - \mu\right)^2\mathrm{d}x }
  \end{align}$$
  </blockquote></li>

========================================================

## Example of the standard deviation and variance of a continuous random variable

* We consider once again the copper wire with the electrical current, with density equal to
  
  $$\begin{align}
  f(x) = \begin{cases}
  5.0 & x\in[4.9, 5.1]\\
  0.0 & \text{else}
  \end{cases}
  \end{align}$$
  
* We can compute the variance thus as

  $$\begin{align}
  \sigma^2 &= \int_{4.9}^{5.1}f(x)\left(x - \mu\right)^2\mathrm{d}x \\
           &= \int_{4.9}^{5.1}5.0 \left(x - 5.0\right)^2\mathrm{d}x
  \end{align}$$
  
* If we make a substitution as $u = x- 5.0$ then $\mathrm{d}u = \mathrm{d}x$ such that,

  $$\begin{align}
  \sigma^2 &= \int_{-0.1}^{0.1}5.0 u^2 \mathrm{d}u\\
           &= \frac{5.0}{3.0}u^3\vert_{-0.1}^{0.1} = \frac{10.0}{3.0}\times 0.1^3 \approx 0.0033
  \end{align}$$
  
* The standard deviation is thus given as $\sqrt{\frac{10.0}{30.0}\times 0.1^3}\approx 0.0577$.

========================================================

## Review of main concepts

* For a continuous random variable, the concepts from discrete random variables have direct analogs.

* We have the following correspondences

<table class="fragment">
<tr>
  <th> <strong>Discrete </strong></th> <th> <strong>Continuous</strong> </th>
</tr>
<tr>
  <td> Probability mass function $f(x)$ </td> <td> Probability density function $f(x)$</td>
</tr>
<tr>
  <td> $P(X=x_\alpha) = f(x_\alpha)$ </td> <td>$P(a \leq X \leq b) = \int_{a}^b f(x)\mathrm{d}x$</td> 
</tr>
<tr>
  <td>Cumulative distribution function $F(x)=P(X\leq x)$ </td> <td>Cumulative distribution function $F(X)=P(X\leq x)$</td>
</tr>
<tr>
  <td> $F(x) = \sum_{x_\alpha \in \mathbf{R}} f(x_\alpha)$ </td><td> $F(x) = \int_{x\in \mathbb{R}}f(x)\mathrm{d}x$</td>
</tr>
<tr>
  <td> $\mu = \sum_{x_\alpha \in \mathbf{R}} xf(x_\alpha)$ </td><td> $\mu = \int_{x\in \mathbb{R}} x f(x)\mathrm{d}x$</td>
</tr>
<tr>
  <td> $\sigma^2 = \sum_{x_\alpha \in \mathbf{R}}(x - \mu)^2 f(x_\alpha)$ </td><td> $\sigma^2 = \int_{x\in \mathbb{R}}( x -\mu)^2 f(x)\mathrm{d}x$</td>
</tr>
</table>

* Due to the diffrence between discrete measurements and continuous measurements (where we can arbitrarily sub-divide units) <strong>the probability of measuring a single value of a continuous random variable always has probability zero</strong>.

* Particularly, with continuous random variables, we always define probabilities over ranges of values, assuming some kind of truncation approximation.

* Otherwise the ideas are extremely similar by replacing sums with integrals (or Riemann sums).

* We will look at two very common continuous probability distributions next time.
  