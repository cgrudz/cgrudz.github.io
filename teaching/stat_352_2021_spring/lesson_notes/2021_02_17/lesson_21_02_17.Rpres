<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>
<!-- foo 
Highlighting
bold
strong
orange <b style="color:#d95f02"> text </b>
green  <b style="color:#1b9e77"> text </b>
purple <b style="color:#d24693"> text </b>
red    <b style="color:#FF0000"> text </b>
blue   <b style="color:#0000FF"> text </b>
-->

Probability Distributions, Mass Functions and Cumulative Distribution Functions
========================================================
date: 02/17/2021
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:

  * Cumulative Distribution Functions
  * foo
  * foo
  * foo


<!-- 3.2 -->

========================================================
##  Random Variables

<div style="float:left; width:13%"; class="fragment">
<img src="example_coin_flips.png" style="width:100%"  alt="Probability distribution for two coin flips with x number of heads.">
<p style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 6th edition
</p>
</div>

<div style="float:left; width:87%">
<ul>
  <li>Let us recall the idea of a <b>random variable</b>.</li>
  <li>Prototypically, we can consider the coin flipping example from the motivation:</li>
  <ul>
    <li>$X$ is the number heads in two coin flips.</li>
  </ul>
  <li>Every time we repeat two coin flips <strong>$X$ can take a different value</strong> due to many possible factors:</li>
  <ul>
    <li>how much force we apply in the flip;</li> 
    <li>air pressure;</li>
    <li>wind speed;</li>
    <li>etc...</li>
  </ul>
 <li>The result is so sensitive to these factors that are beyond our ability to control, <strong>we consider the result to be by chance</strong>.</li>
  <li><b>Before</b> we flip the coin twice, the <strong>value of $X$ has yet-to-be determined</strong>.</li>
  <li><b>After</b> we flip the coin twice, the <strong>value of $X$ is fixed</strong> and possibly known.</li>
  <li>Formally we will define:</li>
  <li style="list-style-type:none"><blockquote><b>Random Variable</b></br>
A random variable is a function that assigns a real number to each outcome in the sample space of a random experiment.</blockquote></li>
  <li style="list-style-type:none"><blockquote><b>Notation</b></br>
  A random variable is denoted by an uppercase letter such as $X$. After an experiment is conducted, the measured value of the random variable is denoted by a lowercase letter such as $x$</blockquote></li>
</ul>
</div>

========================================================

### Random variables continued

<div style="float:left; width:60%;text-align:center;">
<img src="sample_space.png" style="width:100%" alt="Random variables are the numerical measure of the outcome of a random process.">
<p style="text-align:center">
Courtesy of Ania Panorska  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a>
</div>  
<div style="float:left; width:40%">
<ul> 
  <li>Suppose we are considering our <b>sample space</b> $\mathbf{S}$ of all possible outcomes of a random process.</li>
  <li>Then for any particular outcome of the process,</li>
  <ul>
    <li>e.g., for the coin flips one outcome is $\{H,H\}$,</li>
  </ul>
  <li>mathematically the random variable $X$ takes the outcome to the numerical value $x=2$ in the range $\mathbf{R}$.</li> 
</ul>
</div>  
<div style="float:left; width:100%">
<ul>
  <li><b>Note:</b> <strong> $X$ must always take a numerical value</strong>.</li>
  <li>Because a <b>random variable</b> takes a <strong>numerical value</strong> (not categorical), we must consider the units that $X$ takes:</li>
  <ul>
    <li style="list-style-type:none"><blockquote><b>Discrete random variable</b> is a random variable with a finite (or <strong>countably</strong> infinite) range.</blockquote></li>
    <ul>
      <li>In particular, the unit of $X$ cannot be arbitrarily sub-divided.</li>
    </ul>
</ul> 
</div>


========================================================

### Random variables continued

<div style="float:left; width:60%;text-align:center;">
<img src="sample_space.png" style="width:100%" alt="Random variables are the numerical measure of the outcome of a random process.">
<p style="text-align:center">
Courtesy of Ania Panorska  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a>
</div>  
<div style="float:left; width:100%">
<ul>
  <ul>
    <li style="list-style-type:none"><blockquote><b>Continuous random variable</b> is a random variable with an interval (either finite or infinite) of real numbers for its range.</blockquote></li>
    <ul>
      <li>The units of $X$ <strong>can be arbitrarily sub-divided</strong> and $X$ can take any value in the sub-divided units.</li>
      <li>Necessarily, $X$ can take infinitely many values when it is continuous.</li>
      <ul>
        <li>A good example to think of is if $X$ is the daily high temperature in Reno in degrees Celsius.</li>
        <li>If we had a sufficiently accurate thermometer, we could <b>measure $X$ to an arbitrary decimal place</b> and it would make sense.</li>
      </ul>
    </ul>
  </ul>
</ul>
</div>

<!-- 3.1 -->

========================================================

## Probability distributions

<div style="float:left; width:100%">
<ul>
  <li>Given a random variable, our method for analyzing its behavior is typically through a probability "distribution".</li>
  <li style="list-style-type:none"><blockquote>The <b>probability distribution</b> of a random variable $X$ is a description of the <strong>probabilities associated with the possible values of $X$</strong>.</blockquote></li>
  <ul>
    <li>A probability distribution can thus be considered a <b>complete description</b> of the random variable.</li>
    <ul>
      <li>For <b>any possible value</b> that $x$ might attain given any possible outcome, we know with <strong>what probability this will occur</strong>.</li>   
    </ul>
    <li>It is often expressed in the format of a table, formula, or graph.</li>
  </ul>
</ul>
</div>



========================================================

### Probability Mass Function

<div style="float:left; width:100%">
<ul>
  <li>For a discrete random variable $X$, its distribution can be described by a
function that specifies the probability at each of the possible discrete values for $X$.</li>
  <li style="list-style-type:none"><blockquote><b> Probability Mass Function</b></br>For a discrete random variable $X$ with possible values $x_1, x_2,\dots, x_n$, a probability mass function is a function such that
  <ol>
    <li>$f(x_i)=P(X=x_i)$</li>
    <li>$f(x_i)\geq 0$</li>
    <li>$\sum_{i=1}^n f(x_i)=1$</li>
  </ol></blockquote></li>
</ul>
</div>

<div style="float:left; width:35%"; class="fragment">
<img src="example_digital_channel.png" style="width:100%"  alt="Probability distribution for bits in error.">
<p style="text-align:center">
Courtesy of Montgomery & Runger, <em>Applied Statistics and Probability for Engineers</em>, 7th edition
</p>
</div>

<div style="float:left; width:65%">
<ul>
  <li>On the left, we see that the <b>probability mass function</b> <strong>is the graph of each possible realization of $X$</strong>.</li>
  <li>Particularly, we see
  $$\begin{align}
  f(x) = 
  \begin{cases}
    P(X=0)=0.6561 & \text{when }x=0\\
    P(X=1)=0.2916 & \text{when }x=1\\
    P(X=2)=0.0486 & \text{when }x=2\\
    P(X=3)=0.0036 & \text{when }x=3\\
    P(X=4)=0.0001 & \text{when }x=4
  \end{cases}
  \end{align}$$</li>
  <li>The <b style="color:#d95f02">input</b> of the <b>probability mass function</b> is a <b style="color:#d95f02">possible outcome for the random variable</b>, and the <b style="color:#1b9e77">output is its associated probability</b>.</li>
</ul>
</div>

========================================================

### Probability Mass function -- example

<div style="float:left; width:100%">
<ul>
  <li><strong>EXAMPLE</strong>: Let the random variable $X$ denote the <b>number of semiconductor wafers</b> that need to be analyzed in order to detect a large particle of contamination.</li>
  <li>This is to say, we will examine wafers randomly and stop when we find one which has the contamination.</li>
  <li>Assume that the <strong>probability that a wafer contains a large particle is 0.01</strong> and that the selection of <b>wafers is independent</b>. </li>
  <li>Let's determine the probability distribution of $X$.</li>
  <ul>
    <li>Let <b style="color:#d95f02">$p$ denote a wafer in which a large particle is present</b>, and let <b style="color:#1b9e77">$a$ denote a wafer in which it is absent</b>.</li> 
    <li>The sample space of the experiment is <b>infinite</b>, and it can be <strong>represented as all possible sequences that start with a string of $a$’s and end with $p$</strong>.
  </li>
    <li>That is $$s = \{p, ap, aap, aaap, aaaap, aaaaap, \text{and so forth}\}$$</li>
    <li>Consider a few special cases. We have $$P(X = 1) = P(p) = 0.01$$</li> 
    <li>Also, using the independence assumption
        $$P(X = 2) = P(ap) = 0.99(0.01) = 0.0099$$</li>
    <li>Similarly $$\hspace{5cm}P(X = 3) = P(aap) = 0.99(0.99)(0.01) = 0.99^2(0.01)$$</li>
    <li>A general formula is 
     $$\begin{align}
     P(X = x) &= P(\underbrace{aa\dots a}_{(x-1)a's}p)\\
     P(X = x) &= 0.99^{x-1}(0.01), \text{for } x= 1,2,3,\dots
     \end{align}$$</li>
   </ul>
</ul>
</div>

========================================================

### Probability distribution -- example continued

<div style="float:left; width:100%">
<ul>  
  <ul>
    <li>Our formula for the <b>mass function</b> is $f(x_i)=P(X=x_i)=0.99^{x_i-1}(0.01)$</li>
    <li>Recall, this gives precisely the probability of finding a large particle on the $x_i$-th draw -- for all values the probability lies between $0$ and $1$.</li>
    <li>Lets check the sum of probabilities on our sample space; </li>
    <ul>
      <li>the sample space is infinite because theoretically, we could wait until an <strong>arbitrarily large, but finite, number $n$</strong> before we find a large particle.</li>
    </ul>
    <li>For simplicity in notation let's denote $x_i=i$ for $i\geq 1$; </li>
    <ul>
      <li>then the sum of $f(x_i)$ over all possible $x_i$ is given by
      $$\sum_{i=1}^\infty f(x_i)=\lim_{n \rightarrow \infty} \sum_{i=1}^n  0.99^{i-1}(0.01)$$
      as we have simply <strong>replaced $x_i$ with $i$ in the above formula</strong>, and used the <b>definition of an infinite sum</b>.
      </li>
      <li>Set $r=0.99$ -- then we recognize the sum on the right-hand-side as a geometric series (<b style="color:#d95f02">Calc 2</b>)
    $$\begin{align}
    \sum_{i=1}^\infty f(x_i)&= (0.01) \lim_{n\rightarrow \infty} \sum_{i=1}^n r^{i-1} 
    \end{align}$$</li>
  </ul>
</ul>
</div>


========================================================

### Probability distribution -- example continued

<div style="float:left; width:100%">
<ul>  
  <li>Let's recall, a geometric series is usually stated in the form where it is indexed from zero as
    $$\sum_{i=0}^n ar^i = a\left(\frac{1-r^{n+1}}{1-r}\right)$$
     whenever $n$ is finite.</li>
  <li>If we take the limit of the right-hand-size as $n\rightarrow \infty$, it is easy to see that for $\vert r\vert <1$
  $$a\left(\frac{\lim_{n\rightarrow \infty} 1 - r^{n+1}}{1-r}\right) = a\left(\frac{1}{1-r}\right)$$</li>
  <li>If we return to our geometric series, notice that 
  $$\begin{align}
  \sum_{i=1}^\infty f(x_i)&= (0.01) \lim_{n\rightarrow \infty} \sum_{i=1}^n r^{i-1} \\
  &= \lim_{n\rightarrow \infty}  (0.01)\sum_{i=0}^{n-1} r^{i}
  \end{align}$$
  because we can always shift the minus 1 into the summation index.</li>
  <li>Finally, if $n\rightarrow \infty$ then $n-1$ also goes to infinity, so this gives a standard geometric series with a finite limit.</li>
  <li><b>Q:</b> what is the limit of the geometric series above?</li>
  <ul>
    <li>The sum converges as $n\rightarrow\infty$ 
    $$\begin{align}
    \lim_{n\rightarrow \infty}\sum_{i=1}^n f(x_i)&= (0.01) \frac{1}{1-r} = (0.01)\frac{1}{1 - 0.99} =1 \end{align}$$</li>
  </ul>
</ul>
</div>

========================================================

### Probability distribution -- example continued

<ul>
<li>On the last slide, we saw that sum converges as $n\rightarrow\infty$ 
    $$\begin{align}
    \lim_{n\rightarrow \infty}\sum_{i=1}^n f(x_i)&= (0.01) \frac{1}{1-r} = (0.01)\frac{1}{1 - 0.99} =1 \end{align}$$</li>
    <li><em>Practical Interpretation</em>: The random experiment here has an <b style="color:#d95f02">unbounded number of outcomes</b>, but it can still be conveniently modeled with a <b style="color:#1b9e77">discrete random variable with a countably infinite range</b>.</li>
    <li>Take the union of all events that make up the sample space, $s = \{p, ap, aap, aaap, aaaap, aaaaap, \text{and so forth}\}$, and let's suppose we denote them as disjoint, exhaustive events 
    $$\begin{align}
    A_1 &= p\\
    A_2 &= ap\\
    A_3 &= aap\\
    \vdots
    \end{align}$$</li>
    <li>We know that $S = \cup_{i=1}^\infty A_i$ and all events are disjoint, so that    
    $$P(s) = \sum_{A_i \in s} P(A_i) =  \sum_{i=1}^\infty f(x_i)= (0.01) \sum_{x_i=1}^\infty r^{x_i-1} = 1$$</li>
    <li>The probability mass function gives another convenient way of expressing the above property, using <strong>partition of the sample space across the events that give distinct values of $X$</strong>.</li>
</ul>

========================================================

## Cumulative Distribution Functions

* An alternate method for describing a random variable’s probability distribution is with <b>cumulative probabilities</b> such as $P(X \leq x)$.

  * In some ways this may seem more complicated, but this is a mathematically more "natural" construction.

* Like a <b style="color:#d95f02">probability mass function</b>, a <b style="color:#1b9e77">cumulative distribution function</b> provides probabilities, but always <b style="color:#1b9e77">over some range of outcomes</b>.

* Consider the digital channel example from earlier: 

* <strong>EXAMPLE</strong>: There is a chance that a bit transmitted through a digital transmission channel is received in error. 

  * Let $X$ equal the <b>number of bits in error in the next four bits</b> transmitted. The possible values for $X$ are $\{0, 1, 2, 3, 4\}$ with  probabilities 
$$\begin{align}
P(X=0)=0.6561&\;\;P(X=1)=0.2916\\
P(X=2)=0.0486&\;\;P(X=3)=0.0036\\
P(X=4)=0.0001 & 
\end{align}$$
* <b>Question:</b> How can we compute the probability that <b>three or fewer</b> bits are in error?
  * The event that $\{X \leq 3\}$ is the <b style="color:#d95f02"> union </b> of the events
$\{X = 0\}$, $\{X = 1\}$, $\{X = 2\}$, and $\{X = 3\}$.
  * However, all of those events are <b style="color:#d95f02"> mutually exclusive </b>

========================================================

### Cumulative Distribution Functions -- continued

* In the last slide we saw that
 
 $$\begin{align}
 P(X=0)=0.6561&\;\;P(X=1)=0.2916\\
 P(X=2)=0.0486&\;\;P(X=3)=0.0036\\
 P(X=4)=0.0001 & 
 \end{align}$$
  
* Therefore, 

  $$\begin{align}
  P(X\leq 3) &= P(X=0 \text{ or } X=1\text{ or }X=2\text{ or }X=3)\\
             &= P(X=0)+P(X=1)+P(X=2)+P(X=3)
    \end{align}$$
  by computing the probability over the union of disjoint events.
  
* Notice, the different outcomes for $X$ correspond to collections of events in the sample space as,

  $$\begin{align}
  (X=0) \equiv\{(T,T,T,T)\} & & (X=1) \equiv \{(F,T,T,T), (T,F,T,T), (T,T,F,T), (T,T,T,F)\} , \cdots\\
  \end{align}$$
  but where these <b>collections of events are disjoint</b>.
  
  * Therefore, we can conveniently calculate 
  
  $$P(X\leq 3) = 0.6561+0.2916+0.0486+0.0036 = 0.9999$$

========================================================

### Cumulative Distribution Functions -- continued

<div style="float:left; width:100%">
<ul>
  <li>In general, for any discrete random variable with possible values $x_1, x_2, \dots,$ the events $\{X=x_1\}, \{X=x_2\},\dots$ are <b style="color:#d95f02"> mutually exclusive</b>.
  <li style="list-style-type:none"><blockquote><b>Cumulative Distribution Function</b></br>The <strong>cumulative distribution function</strong> of a discrete random variable $X$, denoted as $F(x)$, is
  $$F(x) = P(X \leq x) = \sum_{x_i \leq x} f(x_i)$$</blockquote></li>
  <li style="list-style-type:none"><blockquote><b>Properties of a Cumulative Distribution Function</b>
  <ol>
    <li>$F(x)=P(X\leq x)=\sum_{x_i\leq x}f(x_i)$</li>
    <li>$0\leq F(x) \leq 1$</li>
    <li>If $x\leq y$, then $F(x)\leq F(y)$</li>
  </ol><blockquote>
  </li>
  <li>Properties 1 and 2 of a cumulative distribution function follow from the definition.</li>
  <li>Property 3 follows from the fact that if $x \leq y$, the event that $\{X \leq x\}$ is contained in the event $\{X \leq y\}$</li>
  <li>Specifically,
  $$\{X \leq y\} = \{X \leq x\} \cup \{x &lt; X \leq y\}$$
</ul>
</div>

========================================================

### Cumulative Distribution Functions -- example

* Suppose a random variable $X$ can assume only integer values,

  * notice then that the cumulative distribution function is <b>still defined at non-integer values</b>.

* Consider again the digital channel example

* If now we want to find $P(X\leq 1.5)$

  * $$\begin{align}F(1.5)& = P(X \leq 1.5) = P(X = 0) + P(X = 1) \\
          & = 0.6561 + 0.2916 \\
          & = 0.9477\end{align}$$
          
* Therefore $F(x)=0.9477$ for all $1\leq x < 2$

* Moreover, we can then partition the cumulative distribution $F$ on intervals:
  
  $$\begin{align}
  F(x) = 
  \begin{cases}
    0 & x &lt; 0 \\
    0.6561 & 0 \leq x &lt; 1\\
    0.9477 & 1 \leq x &lt; 2\\
    0.9963 & 2 \leq x &lt; 3\\
    0.9999 & 3 \leq x &lt; 4\\
    1 & 4\leq x
  \end{cases}
  \end{align}$$

    
* This way we ca see that $F(x)$ is <em>piecewise constant</em> between values $x_1, x_2,\dots$ 

========================================================

### Cumulative Distribution Functions -- continued

<div style="float:left; width:100%">
<ul>
  <li> Furthermore, $P(X = x_i )$ can be determined from the jump at the value $x_i$. </li>
  <li>More specifically,
  $$P(X=x_i)=F(x_i)-\lim_{x \uparrow x_i} F(x)$$</li>
  <li>This expression calculates the difference between $F(x_i )$ and the limit of $F(x)$ as $x$ increases to $x_i$.</li>
  <li><strong>EXAMPLE</strong>: We will determine the probability mass function of $X$ from the following cumulative distribution function:
  $$F(x)=\left\{\begin{array}{@{}r} 
    0   & x < -2\\
    0.2 & -2 \leq x < 0\\
    0.7 &  0 \leq x < 2\\
    1 & 2\leq x
    \end{array}\right.$$</li>
</ul>
</div>

<div style="float:left; width:40%"; class="fragment">
<img src="example_cummulative.png" style="width:100%"  alt="Cumulative distribution function for Example">
<p style="text-align:center">
Courtesy of Montgomery & Runger, <em>Applied Statistics and Probability for Engineers</em>, 7th edition
</p>
</div>

<div style="float:left; width:60%">
<ul>
  <ul>
    <li>From the plot, the only points that receive nonzero probability are $−2, 0$, and $2$.</li>
    <li>The probability mass function at each point is the jump in the cumulative distribution function at the point.</li>
    <li>Therefore
    $$\begin{array}{rll}
    f(-2)&=0.2-0&=0.2\\
     f(0)&=0.7-0.2&=0.5\\
     f(2)&=1.0-0.7&=0.3
    \end{array}$$</li>
  </ul>
</ul>
</div>



========================================================

## A review of the main ideas

* We have now constructed the fundamental tools in probability for random experiments.

* The axioms of probability like the the <b>multiplication rule</b>, the <b>addition rule</b>, <b>total probability rule</b>, etc... are useful for constructing <strong.complex, theoretical probability calculations</strong>.

* However, in real examples, we usually make  <b style="color:#1b9e77">numeric measurements $x$</b> of a <b style="color:#d95f02"> random variable $X$</b>.

* The collection of  <b style="color:#1b9e77">all possible outcomes $\{x_i\}$</b> for the <b style="color:#d95f02">random variable $X$</b> <strong>partitions the sample space into disjoint events</strong>.

* Therefore, we can conveniently <strong>calculate probabilities of different measurable outcomes</strong> of the experiment through a <b>probability mass function</b> or a <b>cumulative distribution function</b>.

* For a given possible outcome $x_i$, we define the probability mass function by
  
  $$f(x_i) = P(X=x_i)$$
  
  which may be associated with a collection of different measureable events in the sample space.
  
* Similarly, for any possible value $x$, we define the cumulative distribution function as the <strong>probability of a range of values</strong>
  
  $$F(x) = P(X\leq x)$$
  
* Both of these provide different representations of the <b>probability distribution</b>, i.e., the <strong>complete collection of possible outcomes of $X$ and their associated probabilities</strong>.