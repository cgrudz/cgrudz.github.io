<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>
<!-- foo 
Highlighting
bold
strong
orange <b style="color:#d95f02"> text </b>
green  <b style="color:#1b9e77"> text </b>
purple <b style="color:#d24693"> text </b>
red    <b style="color:#FF0000"> text </b>
blue   <b style="color:#0000FF"> text </b>
-->

Parameters of probability distributions and the binomial distribution
========================================================
date: 03/03/2021
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

<blockquote>
FAIR USE ACT DISCLAIMER:</br>
This site is for educational purposes only.  This website may contain copyrighted material, the use of which has not been specifically authorized by the copyright holders. The material is made available on this website as a way to advance teaching, and copyright-protected materials are used to the extent necessary to make this class function in a distance learning environment.  The Fair Use Copyright Disclaimer is under section 107 of the Copyright Act of 1976, allowance is made for “fair use” for purposes such as criticism, comment, news reporting, teaching, scholarship, education and research.
</blockquote>


========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:

  * Review of mean, standard deviation and variance of probability distribution
  * Binomial distribution
  * Parameters of the binomial distribution

<!-- 3.2 -->

========================================================

## Motivation

* Our goal in this course is to use <b style="color:#d95f02">statistics</b> from a <b style="color:#d95f02">small, representative sample</b> to say something <b style="color:#1b9e77">general</b> about the <b style="color:#1b9e77">larger, unobservable population or phenomena</b>.</li>
  
* Recall, the measures of the <b style="color:#1b9e77">population</b> are what we referred to as <b style="color:#1b9e77">parameters</b>.

* <b style="color:#1b9e77">Parameters</b> are generally <strong>unknown and unknowable</strong>.</li>
   
   * If we have a <b style="color:#d95f02">representative sample</b> we can compute the <b style="color:#d95f02">sample mean</b>.
   
   * The <b style="color:#d95f02">sample  mean</b> will almost surely <b>not equal</b> <b style="color:#1b9e77">population mean</b>, due to the natural variation <b style="color:#d95f02">(sampling error)</b> that occurs in <b style="color:#d95f02">any given sample</b>.

* <strong>Random variables</strong> and <strong>probability distributions</strong> give us the <b>model</b> for estimating <b style="color:#1b9e77">population parameters</b>.

* Generally, we will have to be satisfied with <b>estimates of the parameters</b> that are uncertain, but also <strong>include measures of "how uncertain"</strong>.

========================================================

## Characteristics of data


<div style="float:left; width:40%">
<img src="Standard_deviation_diagram.png" style="width:100%"  alt="Diagram of the percent of outcomes contained within each standard deviation of the mean
for a standard normal distribution.">
<p style="text-align:center">
Courtesy of M. W. Toews <a href="https://creativecommons.org/licenses/by/2.5" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Standard_deviation_diagram.svg"> Wikimedia Commons</a>. 
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>In statistics, we try to characterize data and populations by a number of the features that they exhibits.</li>
  <li>For a single variable, the most common measures are:</li>
  <ol>
    <li> <b>Center:</b> A representative value that indicates where the <strong>middle</strong> of the data set
is located.</li>
    <li> <b>Spread:</b> A measure of the amount that the data values <strong>vary around the center</strong>.</li>
  </ol>
</ul>
</div>
<div style="float:left;width:100%">
<ul>  
  <li>We saw last time how the <b>mean</b> and <b>standard deviation</b> are related quantities describing these features of <b style="color:#d95f02">sample data</b> or a <b style="color:#1b9e77">population</b>.</li>
  <li>The above figure represents the theoretic description of a <b style="color:#1b9e77">normal population</b>.</li>
  <li>In particular:</li>
  <ul>
    <li>$\approx 68\%$ of the population lies within one standard deviation of the mean, $[\mu - \sigma, \mu+\sigma]$;</li>
    <li>$\approx 95\%$ of the population lies within two standard deviations of the mean, $[\mu - 2 \sigma , \mu + 2 \sigma]$</li>
    <li>$\approx 99.7%$ of the population lies within three standard deviations of the mean, $[\mu - 3 \sigma, \mu + 3 \sigma]$.
  </ul>
  <li>This is known as the empirical rule, which holds for all <b style="color:#1b9e77">normal populations</b>.</li>
  <li><b style="color:#d95f02">Sample data</b> will tend to follow this, but not exactly, if the measurements come from a <b style="color:#1b9e77">normal population.</b></li>
</ul>
</div>

========================================================

## Chebyshev's Theorem

<ul>
  <li> A very similar rule is known as Chebyshev's theorem:
  <blockquote>
  The proportion (or fraction) of any set of data lying within $K$ standard deviations of
  of the mean is always <b>at least</b> $1-\frac{1}{K^2}$ where $K>1$.
  </blockquote>
  <li><b>Q:</b> suppose $K=2$, what does this statement tell us?</li>
  <ul>
    <li>For $K=2$, we say <b>at least</b>
    $$1 - \frac{1}{2^2} = 1 - \frac{1}{4} = \frac{3}{4}$$
    of data lies within $K=2$ standard deviations of the mean.</li>
    <li>Note, <strong>this holds for any distribution</strong> whereas <b>the empirical rule only holds for normal data</b>.</li>
    <li>If we know the population is in fact normal, then $95\% > 75\% =1 - \frac{1}{2^2}$ of the population lies within $K=2$ standard deviations of the mean.</li>
  </ul>
  <li>We note thus there are <b>two major differences between Chebyshev's theorem and the empirical rule</b>:</li>
  <ol>
    <li>The empirical rule only holds for normal data, while Chebyshev's theorem holds for any type of data.</li>
    <li>However, Chebyshev's theorem is only a <b>lower bound</b> on <strong>at least how much data lies within standard deviations</strong> and is a much weaker statement on how much.</li>  
  </ol>
  <li>In either case, these  <b style="color:#1b9e77">parameters</b> tell us a lot about the center and spread of data, with some qualifications.</li>
  <li>We will now recall how we compute the mean, variance and standard deviation from a <b style="color:#1b9e77">probability distribution</b>.</li>
</ul>

========================================================

## The mean of probability distributions


<ul>
  <li>Let $X$ be a random variable that can attain the possible-to-observe values $\{x_\alpha \in\mathbf{R}\}$.</li>
  <li>We say the the probability density function $f(x_\alpha) = P(X=x_\alpha)$.</li>
  <li>The <b style="color:#1b9e77">mean of the probability distribution</b>
    $$\mu = \sum_{x_\alpha \in \mathbf{R}} x_\alpha P(X=x_\alpha) = \sum_{x_\alpha \in \mathbf{R}} x_\alpha f(x_\alpha)$$
    is computed like <strong>the formula for the mean of a frequency distribution</strong>.</li>
    <li>However, because of the difference in the interpretation, the mean of a probability distribution has a special name:</li>
  <ul>
    <li>For a random variable $x$ with probability distribution defined by the pairs of values $\{x_\alpha\}$ and $P(X=x_\alpha)$, the <b style="color:#1b9e77">expected value of $x$</b> is defined,
    $$\mu = \mathbb{E}\left[X\right] = \sum_{x_\alpha \in \mathbf{R}} x_\alpha P(X=x_\alpha).$$</li>
    <li>We call the <b style="color:#1b9e77">mean of the probability distribution the expected value</b>, because it can be thought of as the <strong>theoretical mean if we repeated an experiment infinitely many times or sampled the entire population</strong>;</li>
    <li>we would <b>expect this value on average</b>, <strong>relative to infinitely many experiments</strong>.</li>  
  </ul>
</ul> 


========================================================

## Standard deviation of a sample


<ul>
  <li>Suppose we are considering finite population of size $N$.</li>
  <li>Usually, we will not have access to the entire <b style="color:#1b9e77">population</b> $x_1, \cdots, x_N$.</li>
  <li>Instead, we will only have some <b style="color:#d95f02">smaller subset of values in a sample</b> $x_1, \cdots, x_n$ for $n< N$.</li>
  <li>Our <b style="color:#d95f02">sample standard deviation</b> can be computed as
  $$s = \sqrt{\frac{\sum_{i=1}^n\left(x_i - \overline{x}\right)^2}{n-1}}$$</li>
  <li>or
  $$s = \sqrt{\frac{n \left(\sum_{i=1}^n x_i^2 \right) - \left(\sum_{i=1}^n x_i\right)^2}{n\left(n-1\right)}}.$$</li>
  <li>However, this differs from computing the <b style="color:#1b9e77">population standard deviation</b> as
  $$\sigma = \sqrt{\frac{\sum_{i=1}^N \left(x_i - \mu\right)^2}{N}}.$$
  </li>
  <li>One key difference to remember is that for the <b style="color:#d95f02">sample standard deviation</b>, we have a very different denominator than with the <b style="color:#1b9e77">population standard deviation</b>.</li>
</ul>


========================================================

## Variance


* The word <b>variance</b> also has a specific meaning in statistics and is another tool for describing the variation / dispersion / spread of the data.

* Suppose that the data has a <b style="color:#1b9e77">population standard deviation of $\sigma$</b> and a <b style="color:#d95f02">sample standard deviation of $s$</b>.

* Then, the data has a <b style="color:#1b9e77">population variance of $\sigma^2$</b>.

* Likewise, the data has a  <b style="color:#d95f02">sample variance of $s^2$</b>.

* Therefore, for either a <b style="color:#1b9e77">population parameter</b> or a <b style="color:#d95f02">sample statistic</b>, the <strong>variance is the square of the standard deviation</strong>. 

  * Because of this, the variance has units which are the <strong>square of the original units</strong>.
  
* For example, measuring the heights of students in inches, the <strong>standard deviation is in the units inches</strong>.

  * However, the <strong>variance is in the unit $\text{inches}^2$</strong>.
  

========================================================

## The standard deviation and variance of probability distributions

<ul>
  <li>Let's recall the formula now for the <b style="color:#1b9e77">standard deviation of a population</b> with members $\{x_i\}_{i=1}^N$ 
  $$\sigma = \sqrt{\frac{\sum_{i=1}^N \left(x_i - \mu\right)^2}{N}},$$
  where we take a denominator of $N$ for the population instead of $N-1$ as in samples.</li>
  <li>Let's suppose that the <b style="color:#1b9e77">population members $x_i$</b> <strong>equal values $x_\alpha$ in the range $\mathbf{R}$ with frequencies $n_\alpha$</strong>.</li>
  <li>If we re-write the above formula in terms of $x_\alpha$ and $n_\alpha$ we can say,
  $$\begin{align}
  \sigma = \sqrt{\frac{\sum_{x_\alpha\in \mathbf{R}} n_\alpha\left(x_\alpha - \mu\right)^2}{N}} 
  = \sqrt{\sum_{x_\alpha \in \mathbf{R}} \frac{n_\alpha}{N} \left(x_\alpha - \mu\right)^2}
  = \sqrt{\sum_{x_\alpha\in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2 }.
  \end{align}$$</li>
  <li>We will denote,
  $$\sigma = \sqrt{\sum_{x_\alpha\in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2 }$$
  the <b style="color:#1b9e77">standard deviation of the probability distribution</b> associated to the random variable $X$.</li>  
  <li>That is to say that the <b style="color:#1b9e77">population standard deviation $\sigma$</b> is <b>exactly the standard deviation of the probability distribution</b>.</li> 
  <li>For infinite populations and ranges, we can use the same argument (with calculus) to show this holds in general.</li>   
</ul>

========================================================

### The standard deviation and variance of probability distributions continued

<ul>
  <li>Using the derivation from the last slide,
  $$\sigma = \sqrt{\sum_{x_\alpha\in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2 },$$</li>
  <li>we can show directly that the <b style="color:#1b9e77">variance of a probability distribution</b> is given as,
  $$\sigma^2 = \sum_{x_\alpha \in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2 .$$</li>
  <li>We can also derive the alternative forms for the <b style="color:#1b9e77">population standard deviation and variance</b> in terms of the <b style="color:#1b9e77">probability distribution</b> as
  $$\begin{align}
  \sigma &= \sqrt{\sum_{x_\alpha \in \mathbf{R}}  x_\alpha^2 P(X=x_\alpha)   -  \mu^2 }, \\ \\
  \sigma^2 &= \sum_{x_\alpha\in\mathbf{R}}  x_\alpha^2 P(X=x_\alpha) -  \mu^2  . 
  \end{align}$$</li>
  <li>This again just amounts to some algebraic manipulation and these forms are <strong>totally equivalent to the other forms</strong>.</li>
</ul>

========================================================

### Example of the variance and standard deviation

<div style="float:right; width:35%;text-align:center;">
<table class="fragment">
<tr>
  <th> <strong>Outcome </strong></th> <th> <strong>Observed value for $X=x$</strong> </th> <th><strong>Probability</strong></th></th>
</tr>
<tr>
<td> $\{H,H\}$ </td> <td>$x=2$</td> <td> $f(x)=\frac{1}{4}$</td></tr>
<tr><td>$\{H,T\}, \{T,H\}$</td> <td>$x=1$</td> <td>$f(x)=\frac{2}{4}$</td></tr>
<tr><td>$\{T,T\}$</td> <td>$x=0$</td> <td>$f(x)=\frac{1}{4}$</td></tr>
</table>
</div>
<div style="float:left; width:65%">
<ul>
<li>Let's recall the probability distribution for the two coin flipping experiment.</li>
<li>We showed already that $\mu = \mathbb{E}\left[X\right] =1$.</li>
<li> Thus we say that the  <b style="color:#1b9e77">expected value</b> of two coin flips is to observe <b>one heads</b>.</li>
<li>Recall that our variance is computed as
$$\sigma^2 = \sum_{x_\alpha \in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2$$</li>
</div>
<div style="float:left; width:100%">
<ul>
  <li>Using the above, we can show that
  $$\begin{align}
  \sigma^2 &= \sum_{x_\alpha \in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2\\
  &= \frac{1}{4}\left(2 - 1 \right)^2 + \frac{2}{4}\left(1 - 1\right)^2 + \frac{1}{4}\left(0 - 1\right)^2 \\
  &= \frac{1}{4} + \frac{1}{4} = \frac{1}{2}
  \end{align}$$</li>
  <li>Therefore, the standard deviation has to be given by
  $$\begin{align}
  \sigma = \sqrt{\frac{1}{2}}
  \end{align}$$
</ul>
</div>

========================================================

## Binomial distribution

<ul>
  <li>A coin flipping experiment is actually a simple example of a broad category of experiments.</li>
  <ul>
    <li>For example, we can consider an experiment in which we describe two possible outcomes:</li>
    <ol>
      <li><b>(success / $S$ / $1$ / $H$)</b>; or</li>
      <li><b>(failure / $F$ / $0$ / $T$)</b>.</li>
    </ol>
    <li>We can <strong>encode the outcomes</strong> any way we like;</li>
    <ul>
      <li>it is <strong>common to encode the outcomes as $S$ or $F$</strong>, where the choice of "success" is arbitrary.</li>  
    </ul>
      <li>More generally than coin flipping, we might <strong>consider the case where the probabilities</strong>,
      $$\begin{align}
      P(\text{success}) \neq P(\text{failure})
      \end{align}$$</li>
      <li>Recall, if the experiment has only <strong>two possible outcomes</strong>, if $A=$"success"   then $\overline{A}=$"failure".</li>
      <li>Therefore,
      $$\begin{align}
      P(\text{success}) + P(\text{failure}) = 1.
      \end{align}$$
      </li>
      <li>Suppose we run the experiment a total of $n$ trials.</li>
      <ul>
        <li>Because there are only <strong>two possible outcomes for each trial</strong>, and a <strong>finite number of trials</strong>, we can <b>create a list of all possible outcomes for $n$ trials</b>.</li>
        <ul>
          <li>For example, if there are <b>$x_\alpha$ total successes</b>, there must be exactly  <strong>$n - x_\alpha$ failures in $n$ trials</strong>.</li>
        </ul>
        <li>More importantly, we can also <strong>make a list of all possible ways</strong> we can have <b>$x_\alpha$ successes</b> and <b>$n-x_\alpha$ failures</b>.</li>
        <li>Let <b>$X$ be the random variable</b> equal to the <strong>number of successful trials</strong> --  we can therefore calculate the probability,
        $$P(X = x_\alpha);$$
        however, the <b>classical model for probability</b> (equal probability of all outcomes) <strong> will no longer apply</strong>.</li>  
      </ul> 
  </ul>
</ul>


========================================================

### Binomial distribution continued

<ul>
  <li>Recall our <b>random variable $X$</b> equal to the <strong>number of successful trials</strong> in $n$ total trials.</li>
  <ul>
    <li>Unlike with coin flipping, we suppose that <strong>it is possible for</strong>
    $$\begin{align}
      P(\text{success}) \neq P(\text{failure})
      \end{align}$$</li>
  </ul>
  <li>However, there are finite trials, finite possible outcomes and, for <strong>each possible number of successes $x_\alpha$</strong>, there are a <b>finite number of ways $X=x_\alpha$</b>.</li>
  <ul>
    <li>Provided <strong>all trials are independent</strong> (like coin flipping) and the <strong>probability of success is constant</strong>, we can still make a counting argument using</li>
    <ol>
      <li>The rule of complementary probability
      $$\begin{align}
      P(\text{success}) + P(\text{failure}) = 1;
      \end{align}$$</li>
      <li>independence;</li>
      <li>the list of all possible ways we can make $x_\alpha$ successes;</li>
      <li>the list of all possible ways we can make $n-x_\alpha$ failures; and</li>
      <li>a total of $n$ trials exactly;</li>
    </ol>
    <li>to compute the probability exactly for each $x_\alpha$ where $x_\alpha$ ranges from $0, 1, \cdots, n$.</li>
    <li>The list of all possible number of successes $x_\alpha = 0, 1, \cdots, n$ and the associated probabilities $P(X= x_\alpha)$ for $x_\alpha = 0, 1, \cdots, n$ is called the <b>binomial distribution</b>.</li>
  </ul>
  <li>The argument itself is somewhat long, but it really only uses tools we already know.</li>
  <ul>
    <li>Therefore, if you can understand the principles of the points 1 - 5 above, we don't need to belabor the details.</li> 
  </ul>
</ul>


========================================================

### Binomial distribution continued

<ul>
  <li>Formally, we will now describe the <b>binomial distribution</b>.</li>
  <ul>
    <li>Suppose we run an experiment with <b>two possible outcomes</b> <strong>$S=$"success"</strong> and <strong>$F=$"failure"</strong>, where
    $$\begin{align}
    P(S) = p  && P(F) = 1 - P(S) = q.
    \end{align}$$</li>
    <li>Suppose we <b>run exactly $n$ total trials</b> of the above experiment and suppose that:</li>
    <ol>
      <li><strong>each trial is independent</strong>; and
      <li><strong>$P(S)=p$ for every trial</strong>.</li>
    </ol>
    <li>Let <b>$X$ be the random variable</b> equal to the <strong>total number of successful trials</strong>.</li>  
    <li>Let <b>$x_\alpha$</b> be one of the <strong>possible number of successful trials</strong> in the range $0, 1 ,\cdots , n$.</li>
    <ul>
      <li>Then the probability of <strong>exactly $x_\alpha$ successful trials (the event $X= x_\alpha$)</strong> is given by
      $$\begin{align}
      P(X=x_\alpha) =  \frac{n!}{\left( n  - x_\alpha\right)! x_\alpha !} p^{x_\alpha} q^{(n - x_\alpha)},
      \end{align}$$</li>
      <li>where:</li>
     <ol>
       <li>The total number of ways that we can have exactly $x_\alpha$ successes in $n$ trials is given by
        $$ \frac{n!}{\left( n  - x_\alpha\right)! x_\alpha !}. $$
        </li>
        <li>The probability of $x_\alpha$ independent successes (or $n-x_\alpha$ independent failures) is $p^{x_\alpha}$ $\big($ or $q^{(n-x_\alpha)}\big)$ respectively.
    </ul> 
  </ul>   
</ul> 


========================================================

## Binomial distribution example 

<ul>
  <li>Recall our notation:
  <ol>
    <li><b>$n$</b> - <strong>the number of trials</strong>;</li>
    <li><b>$X$</b> - <strong>the random variable</strong>;</li>
    <li><b>$x_\alpha$</b> - <strong>a specific number of successes that $X$ could possibly attain</strong>;</li>
    <li><b>$P(S)= p$</b> - <strong>the probability of an independent trial's success</strong>;</li>
    <li><b>$P(F)=q$</b> - <strong>the probability of an independent trial's failure</strong>.</li>
  </ol>
  <li>Suppose that when an <strong>adult is randomly selected with replacement</strong>, there is a <b>$0.85$ probability</b> that this <strong>person knows what Twitter is</strong> (based on results from a Pew Research
Center survey from several years ago...).</li>
  <li> Suppose that we want to find the probability that exactly three of
five random adults know what Twitter is.</li>
  <li><b>Q:</b> can you identify what $n$, $X$, $x_\alpha$, $p$ and $q$ are in the above word problem?</li>
  <ul>
    <li>Here we consider the <b>random selection to be a "trial"</b> so that the number of trials is $n=5$</li>
    <li>If we consider a <b>"successful" trial</b> to be <strong>"select an adult who knows what Twitter is"</strong>, then <b>$X$</b> is <strong>"number of adults who know what Twitter is out of five"</strong>.</li>
    <li><b>$x_\alpha$</b> is the <strong>specific number of successful trials</strong> we are interested in, i.e., <b>$x_\alpha = 3$</b>.</li>
    <li><b>$p$ is the probability of an independent trial's successs</b>, i.e, $p=0.85$</li>
    <li><b>$q$ is the probability of an independent trial's failure</b>, i.e., $q=1-p = 0.15$.</li>
  </ul>
</ul>

========================================================

### Binomial distribution example continued

<ul>
  <li>Let's recall our values from the last slide,
  <ul>
   <li>Here we consider the <b>random selection to be a "trial"</b> so that the number of trials is $n=5$</li>
    <li>If we consider a <b>"successful" trial</b> to be <strong>"select an adult who knows what Twitter is"</strong>, then <b>$X$</b> is <strong>"number of adults who know what Twitter is out of five"</strong>.</li>
    <li><b>$x_\alpha$</b> is the <strong>specific number of successful trials</strong> we are interested in, i.e., <b>$x_\alpha = 3$</b>.</li>
    <li><b>$p$ is the probability of an independent trial's success</b>, i.e, $p=0.85$</li>
    <li><b>$q$ is the probability of an independent trial's failure</b>, i.e., $q=1-p = 0.15$.</li>
  </ul>
  <li>Suppose we wanted to compute the probability of one particular outcome,</li>
  <ul>
    <li>say, $S_i =$"the $i$-th particpant knows what Twitter is" and $F_i=$"the $i$-th participant does not know what twitter is", where
  $$A = S_1 \text{ and } S_2 \text{ and } S_3 \text{ and } F_4 \text{ and } F_5.$$</li>
    <li>We can use <b>independence</b> and the <b>multiplication rule</b> to show
    $$\begin{align}
    P(A) &= P(S_1)\times P(S_2)\times P(S_3)\times P(F_4)\times P(F_5) \\
    &= 0.85 \times 0.85 \times 0.85 \times 0.15 \times 0.15 \\
    &= 0.85^3 \times 0.15^2.
    \end{align}$$
  </ul>
  <li>This shows how we get one part of the binomial distribution formula.</li>
  <ul>
    <li>However, there are many combinations of $S_i$ and $F_i$ that arise in $X=3$.</li>
  </ul>
  <li>Using a counting argument, we can show that the total number of ways $X=3$ is
  $$\begin{align}
  \frac{n!}{(n- x_\alpha)! x_\alpha!} = \frac{5!}{(5 - 3)! 3!} = \frac{5!}{(2)!3!} = 10
  \end{align}$$
</ul>

========================================================

### Binomial distribution example continued

<ul>
  <li>Let's recall our values from the last slide,
  <ul>
   <li>Here we consider the <b>random selection to be a "trial"</b> so that the number of trials is $n=5$</li>
    <li>If we consider a <b>"successful" trial</b> to be <strong>"select an adult who knows what Twitter is"</strong>, then <b>$X$</b> is <strong>"number of adults who know what Twitter is out of five"</strong>.</li>
    <li><b>$x_\alpha$</b> is the <strong>specific number of successful trials</strong> we are interested in, i.e., <b>$x_\alpha = 3$</b>.</li>
    <li><b>$p$ is the probability of an independent trial's successs</b>, i.e, $p=0.85$</li>
    <li><b>$q$ is the probability of an independent trial's failure</b>, i.e., $q=1-p = 0.15$.</li>
    <li>The <b>total number of ways $X=3$</b> is
  $$\begin{align}
  \frac{n!}{(n- x_\alpha)! x_\alpha!} = \frac{5!}{(5 - 3)! 3!} = \frac{5!}{(2)!3!} = 10
  \end{align}$$</li>
  </ul>
  <li>The binomial distribution formula can then be read as,</li>
  <ul>
    <li>The probability of finding exactly $3$ out of $5$ independently, randomly selected adults who know what Twitter is, is equal to
    $$\begin{align}
    \frac{n!}{(n- x_\alpha)! x_\alpha!} p^{x_\alpha} q^{n- x_\alpha} = 10 \times 0.85^3 \times 0.15^2 \approx 0.138,
    \end{align}$$</li>
  <li>or in plain English,
  <blockquote>
  the probability of three independent successful trials, times the probability of two independent failure trials, times all possible ways we can have exactly $3$ successful trials out of five.
  </blockquote></li>
  <li>Again, the counting argument follows directly from the material we studied before the midterm.</li>
</ul> 

========================================================

### Binomial distribution example continued

* Let's now take a graphical look at the last problem in R.

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(c(0:5), dbinom(c(0:5), size=5, prob=0.85),  ylab="Probability", xlab="Number of Successses", cex=3, cex.lab=3, cex.axis=2.5, main="Probability mass function", cex.main=3)
```

* In the above, the `dbinom` is the function for the probability mass function for the binomial distribution.

* We are setting a range of values to plot `c(0:5)` and the size of the trial `size=5` and the probability of succcess `prob=0.85`.

========================================================

### Binomial distribution example continued

<ul>
  <li>We should remark the following on the last calculation.</li>
  <ul>
    <li>Technically, we could only make use of the binomial distribution because we <b>sampled with replacement</b> to enforce <strong>independent trials</strong>.</li>
    <li>If sampled our population <b>without replacement</b>, we know</li>
    <ol>
      <li>that <strong>the trials are dependent</strong>; and</li>
      <li>that the <strong>probability of success changes at each trial</strong>.</li> 
    </ol>
    <li>These conditions make it so the <strong>binomial distribution does not apply</strong> to the random variable $x$ <b>when we do not replace samples</b>.</li>
    <li>However it is common to <b>approximate sampling without replacement as independent</b> when the <strong><b style="color:#d95f02">sample size</b> is less than $5\%$ of the <b style="color:#1b9e77">population</b></strong>.</li>
    <li>In practice for polls of, e.g., all US adults, this approximation will often be used.</li>
  </ul>
</ul>

========================================================

## Parameters of the binomial distribution

<div style="float:left; width:65%;text-align:center;">
<img src="sample_space.png" style="width:100%" alt="Random variables are the numerical measure of the outcome of a random process.">
<p style="text-align:center">
Courtesy of Ania Panorska  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a>
</div>  
<div style="float:left; width:35%">
<ul>
  <li>We saw earlier the following definitions for the mean and the standard deviation of a probability distribution:
  <ul>
  <li>Suppose we have a random variable $X$ which assigns a numerical value to each outcome in the sample space $\mathbf{S}$.</li>
  <li>Suppose all values that $X$ can attain are given by a collection $\{x_\alpha\}$ in the range $\mathbf{R}$ of $X$.</li>
</ul>
</div>
<div style="float:left;width:100%">
<ul>
  <li>Then the <b style="color:#1b9e77">mean (or expected value) of the probability distribution</b> is given,
    $$\mu = \sum_{x_\alpha \in \mathbf{R}} x_\alpha P(X=x_\alpha)$$ </li>
  <li>The <b style="color:#1b9e77">standard deviation of the probability distribution</b> is given
    $$\sigma = \sqrt{\sum_{x_\alpha\in \mathbf{R}} P(X=x_\alpha) \left(x_\alpha - \mu\right)^2 }$$</li>
    <li>These formulas hold for all probability distributions (with a slight modification when the variable is continuous by using calculus).</li>
  </ul>
</ul>

========================================================

### Parameters of the binomial distribution continued

<div style="float:left; width:45%;text-align:center;">
<img src="binomial_distribution.png" style="width:100%" alt="Random variables are the numerical measure of the outcome of a random process.">

Public domain via  
        <a href="https://commons.wikimedia.org/wiki/File:Binomial_distribution_pmf.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:55%">
<ul>
  <li>The binomial distribution has a very nice structure so that the <b style="color:#1b9e77">parameters</b> have a nice form.</li>
  <li>For the <b>binomial distribution</b> the <b style="color:#1b9e77">mean</b> is given as,
  $$\mu = n \times p .$$</li>
    <li>For the <b>binomial distribution</b> the <b style="color:#1b9e77">variance</b> is given as,
  $$\sigma^2 = n \times p \times q.$$</li>
      <li>For the <b>binomial distribution</b> the <b style="color:#1b9e77">standard deviation</b> is given as,
  $$\sigma = \sqrt{ n \times p \times q}.$$</li> 
  <li><b>Q:</b> what is $\mu$ and $\sigma$ for the binomial distribution for $20$ trials and probability of success $p=0.5$?</li>
  <ul>
    <li>Notice that these are given as,
    $$\begin{align} \mu = 20 \times 0.5 = 10 & & \sigma = \sqrt{20 \times 0.5 \times 0.5} = \sqrt{ 5}\end{align}$$</li>
  </ul>
</ul> 
</div>
<div style="width:100%; float:left">
<ul>
  <li><b>Q:</b> what is $\mu$ and $\sigma$ for the binomial distribution for $40$ trials and probability of success $p=0.5$?</li>
  <ul>
    <li>Notice that these are given as,
    $$\begin{align} \mu = 40 \times 0.5 = 20 & & \sigma = \sqrt{40 \times 0.5 \times 0.5} = \sqrt{ 10}\end{align}$$</li>
  </ul>
    <li><b>Q:</b> what is $\mu$ and $\sigma$ for the binomial distribution for $20$ trials and probability of success $p=0.7$?</li>
  <ul>
    <li>Notice that these are given as,
    $$\begin{align} \mu = 20 \times 0.7 = 14 & & \sigma = \sqrt{20 \times 0.7 \times 0.3} = \sqrt{ 4.2}\end{align}$$</li>
  </ul>
</ul>
</div>

========================================================

## Review of the binomial distribution

<ul>
  <li>The binomial distribution is a key distribution that gives us a way to model a wide range of experiments probabilistically.</li>
  <li>This applies when we run an experiment with <b>two possible outcomes</b> <strong>$S=$"success"</strong> and <strong>$F=$"failure"</strong>, where
    $$\begin{align}
    P(S) = p  && P(F) = 1 - P(S) = q.
    \end{align}$$</li>
    <li>When we <b>run exactly $n$ total trials</b> of the above experiment, assuming that:</li>
    <ol>
      <li><strong>each trial is independent</strong>; and
      <li><strong>$P(S)=p$ for every trial</strong>.</li>
    </ol>
  <li>We can model the probability of a particular number of successes $x_\alpha$ like a (possibly) non-fair coin flipping experiment.</li>
  <li>We model the probability of exactly $x_\alpha$ successful trials as 
  $$\begin{align}
  P(X=x_\alpha) =  \underbrace{\frac{n!}{\left( n  - x_\alpha\right)! x_\alpha !}}_{(1) } \times \underbrace{ p^{x_\alpha}}_{(2)} \times  \underbrace{q^{(n - x_\alpha)}}_{(3)}
  \end{align}$$  
  where:
  </li> 
  <ol type="A">
    <li>Total number of ways to find exactly <b>$x_\alpha$ successful trials</b> out of <strong>$n$ total trials</strong>;</li>
    <li><b>Probability</b> of <strong>$x_\alpha$ independent succesful trials</strong>;</li>
    <li><b>Probability</b> of <strong>$n-x_\alpha$ independent failure trials</strong>;</li>
  </ol> 
  <li>The special structure of this distribution also allows us to compute the mean and standard deviation directly as
  $$\begin{align} \mu = n \times p  & & \sigma = \sqrt{n \times p\times q} \end{align}$$
</ul>

