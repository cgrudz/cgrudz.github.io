<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

Fundamentals of probability part III
========================================================
date: 03/03/2020
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style="text-align:left"> Instructions:</h2>
<p style='text-align:left'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>


========================================================


<h2>Outline</h2>

* The following topics will be covered in this lecture:
  * Review of addition rule
  * Review of conditional probability
  * Review of the multiplication rule
  * Review of independence
  * Further notes on independence
  * Further uses of conditional probability
  * Bayes' Law



========================================================

## Compound events

<ul>
   <li>We will often be concerned not with one event $A$ but some combination of some event $A$ and some event $B$.</li>
   <li> <b>Compound event</b> -- formally we define a compound event as any event combining two or more simple events.</li>
   <li> There are <strong>two key operations joining events</strong></li>
   <ol>
      <li><b>"OR"</b> -- in mathematics we refer to "or" as a <strong>non-exclusive "or"</strong>.</li>
      <ul>
         <li>The meaning of this for "$A$ or $B$" is -- event $A$ occurs, event $B$ occurs, <strong>or both</strong> events $A$ and $B$ occur.</li>
         <li>We will <strong>not consider the exclusive "or"</strong>, i.e. <b>either</b> event $A$ occurs, or event $B$ occurs, <strong>but not both</strong>.
      </ul>
      <li><b>"AND"</b> -- in mathematics we refer to "and" in an <strong>exclusive sense</strong>.</li>
      <ul>
         <li>The meaning of this for "$A$ and $B$" is -- <strong>both</strong> event $A$ <b>and</b> event $B$ occurs.</li> 
      </ul>
   </ol>
   <li>The operations "and" and "or" join events together in a way that we can compute the probability of the joint events.</li>
   </ul>
</ul>


========================================================

## Addition rule

<div style="float:left; width:40%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>Suppose we want to <strong>compute the probability of two events $A$ and $B$ joined</strong> by the compound operation <b>"or"</b>.</li>
  <li>We read the statement,
  $$P(A \text{ or } B)$$ 
  as he probability of event:</li>
  <ul>
    <li>$A$ occuring,</li>
    <li>event $B$ occuring, or</li>
    <li>both $A$ and $B$ ocurring.</li>
  </ul>
  <li>Intuitively, we can express the probability in terms of all the ways $A$ can occur and all the ways $B$ can occur, if we don't double count.</li>
  <li>Let <b style="color:red">all the ways that $A$ can occur</b> be represented by the <b style="color:red">red circle</b> to the left.</li>
  <li>Let <b>all the ways that $B$ can occur</b> be represented by the <b>dashed circle</b> to the left.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
<li>If there is any overlap between events <b style="color:red">$A$</b> and <b>$B$</b> so that they  can occur simultaneously, $P(A) +P(B)$ counts the cases where <b style="color:red">$A$</b> and <b>$B$</b> both occur twice.</li>
<li>Therefore, the <b>addition rule</b> for compound events is given as,
  $$P(A\text{ or }B) = P(A) + P(B)  - P(A\text{ and }B)$$ </li> 
</ul>
</div>

========================================================

## Complementary events

<div style="float:left; width:40%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>A special case of the addition rule comes up when the events <b style="color:red">$A$</b> and <b>$B$</b> are <strong>disjoint or mutually exclusive</strong>.</li>
  <ul>
    <li>When <b style="color:red">$A$</b> and <b>$B$</b> are <strong>disjoint</strong>, this means that there is no overlap between these events and they will <b>never occur simultaneously</b>.</li>
    <li>In this case $P(A \text{ and } B) = 0$, so the addition rule becomes,
    $$\begin{align}
    P(A \text{ or } B) = P(A) + P(B) + P(A \text{ and } B) &= P(A) + P(B) 
    \end{align}$$
    </li> 
  </ul>
  <li>Recall the complement of $A$, denoted $\overline{A}$ is the event where $A$ does not occur.</li>
  <ul>
    <li>By definition, $A$ and $\overline{A}$ are disjoint because $A$ will not both occur and not occur simultaneously.</li>
  </ul>
  <li>However, complementary events make up all possible outcomes -- $A$ will either occur or not occur, so that we are certain about the outcome of $P(A \text{ or } \overline{A})$.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <ul>
    <li>That is, we know by definition that
    $$P(A \text{ or } \overline{A}) = 1$$</li>
  </ul>
  <li>Using the above fact, along with the disjointness of $A$ and $\overline{A}$ with the addition rule,
  $$ 1= P\left(A\text{ or }\overline{A}\right) = P(A) + P\left(\overline{A}\right).$$
  for any event $A$ and its complement $\overline{A}$.</li> 
</ul>
</div>


========================================================

## Multiplication rule

<div style="float:left; width:40%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>There are several ways to consider the <b>multiplication rule for probability</b> -- the "physics" way to consider this, due to Kolmogorov, is as follows:</li>
  <ul>
    <li>Suppose that there are two related events $A$ and $B$ where <strong>knowledge of one occuring would change how likely we see the other to occur</strong>.</li>
    <ul>
      <li>For example, we can say $A=$"it snows in the Sierra" and $B=$"it rains in my garden".</li>
      <li>The day before, I don't know if either will occur.</li>
      <li>However, if I knew that $A$ occured, this would change how likely it would seem that $B$ occurs;</li>
      <li> $B$ is not guaranteed when $A$ occurs, but the probability of $B$ occuring would be higher in the presence of $A$.</li>
    </ul>
    <li>Supose that $A$ occurs hypothetically, then our <b>sample space</b> of possible events now <strong>only includes events where $A$ also occurs</strong>.</li>
    <li>I.e., we would need to restrict our consideration of $B$ relative to the case that $A$ occurs.</li>
  </ul>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>We define the <b>probability of $B$ conditional on $A$</b>,<br><br>
  $$P(B\vert A),$$ <br>
  as the <strong>probability of $B$ in the case that $A$ occurs</strong>.</li>
</ul>
</div>


========================================================

### Multiplication rule continued


<div style="float:left; width:40%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>We now consider the <b>probability of $B$ conditional on $A$</b>,<br><br>
  $$P(B\vert A),$$ <br>
  as the <strong>probability of $B$ in the case that $A$ occurs</strong>.</li>
  <ul>
    <li>For example, we can say $A=$"it snows in the Sierra" and $B=$"it rains in my garden".</li>
  </ul>
  <li>Assuming $A$ occurs, we will consider all ways for  <strong>both $A$</strong> <b>and</b> <strong>$B$ to occur</strong>.</li>
  <ul>
      <li>The <strong>sample space for $B \vert A$ has been restricted to the cases where $A$ occurs</strong>, so we compute the probability <b>relative to all the ways $A$ occurs</b>.</li>
  </ul>
  <li>Therefore the probability of $P(B\vert A)$ can be read,
  $$\frac{\text{All the ways }A\text{ and }B\text{ can occur}}{\text{All the ways }A\text{ can occur}}$$</li>
    <li>Mathematically we write this as,
  $$P(B\vert A) = \frac{P(A\text{ and } B)}{P(A)}.$$</li> 
</ul>
</div>
<div style="float:left;width:100%">
<ul>
  <li>For example, in plain English we can say
  <blockquote>
  The probability that it rains in my garden, given that it snows in the Sierra, is equal to the <b>probability of both occuring relative to the probability of snow in the Sierra</b>.
  </blockquote></li>
</ul>
</div>

========================================================

### Multiplication rule continued


<div style="float:left; width:40%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>We have now defined the conditional probability of $B$ given $A$ as,
  $$P(B\vert A)=\frac{P(A\text{ and }B)}{P(A)}$$
  in the Kolmogorov way. 
  </li>
  <li>We should make some notes about this:</li>
  <ul>
    <li>The above statment only makes sense when <b>$P(A)\neq 0$</b>, because <strong>we can never divide by zero</strong>.</li>
    <ul>
      <li>"Physically" we can interpret the meaning with $P(B\vert A)$ read as
      <blockquote>
      The probability that $B$ occurs given that $A$ occurs.
      </blockquote></li>
      <li>The above <strong>should not be defined when $A$ is impossible</strong> -- the phrase "given that $A$ occurs" makes no sense.</li>
    </ul> 
    <li>Using the definition of conditional probability, we get the <b>multiplication rule</b>.</li>
  </ul>
</ul>
</div>
<div style="float:left;width:100%">
<ul>
  <ul>
    <li>The <b>multiplication rule</b> for probability tells us that,
    $$P(A \text{ and } B) = P(B\vert A) \times P(A)$$
    </li>
    <li>We will <strong>use the above formula quite generally</strong>, but we note:</li>
    <ul>
      <li>$P(B\vert A)$ is <b>not defined</b> when $P(A)=0$</li>
      <li>However, <b>$P(A\text{ and }B) = 0$ when $P(A)=0$</b> because this is the probability that <strong>$B$ and the impossible event $A$ both occur</strong>.</li>
    </ul>
  </ul>
</ul>
</div>

========================================================

## Independence

<ul>
  <li>Closely related notions to the conditional probability are <b>independence</b> and <b>dependence</b> of events.</li>
  <ul>
    <li> <b>Dependence</b> -- two events are said to be <b>dependent</b> if the outcome of one event <strong>directly affects the probability of the other</strong>.</li>
    <ul>
      <li>In the earlier example, $A=$"snow in the Sierra" and $B=$"rain in my garden" are dependent events, because <strong>one occuring would affect the chance the other occured</strong>.</li>
      <li>However, <b>dependence</b> between events $A$ and $B$ <strong>does not mean that $A$ causes $B$ or vice versa</strong>.</li>
      <li>Rain in my garden does not cause snow in the Sierra, but the probability of snow in the Sierra is larger if there is rain in my garden.</li>
    </ul>
    <li><b>Independence</b> -- two events are said to be <b>independent</b> if the outcome of either event has <strong>no impact on the probability of the other</strong>.</li>
    <ul>
      <li>When we think of events being <b>independent</b> we should think of events that are not related to each other</li>
      <li>For example, if our process is "what happens today", $A=$"snow in the Sierra" and $B=$"coin flip heads" are <b>independent</b>, because <strong>neither outcome affects the other</strong>.</li>
    </ul>
  </ul>
  <li>Mathematically, we can see the meaning of independence clearly by stating, <b>$A$ and $B$ are independent</b> by definition <strong>if and only if both of the following hold</strong>,
  $$\begin{matrix}
  P(A\vert B) = P(A) & \text{and}  & P(B\vert A) = P(B). 
  \end{matrix}$$</li>
  <li>In plain English, the above says
  <blockquote>The probability of event $A$ does not change in the presence of $B$ and vice versa.</blockquote></li>
  <li>Particularly, the outcome of $A$ or $B$ does not affect the other.</li> 
</ul> 

========================================================

## Redundancy and the multiplication rule

<ul>
  <li>Machine systems in engineering are often designed with multiple, redundant safety features.</li>
  <li>Particularly, if there are <strong>multiple, independent</strong> saftey checks, we can reduce the probability of a catastrophic failure substantially.</li>
  <li>For example, the Airbus 310 twin-engine airliner has <strong>three independent hydraulic systems</strong>  so that if one fails, another system can step in and maintain flight control.</li>
  <li>For sake of example, we will assume that the <stong>probability of a randomly selected hydraulic system failing is $0.002$</strong>.</li>
  <li><b>Discuss with a neighbor:</b> if the airplane had only one hydraulic system, what would be the proability that an airplane would be able to maintain control for the flight?</li>
  <ul>
    <li>Let event $A=$"hydraulic system fails" so that $\overline{A}=$"airplane maintains control".</li>
    <li>We can then state,
    $$P(\overline{A}) = 1 - P(A) = 1 - 0.002 = 0.998.$$</li>
  </ul>
  <li><b>Discuss with a neighbor:</b> what is the probability that an airplane would be able to <strong>maintain control with the three independent hydraulic systems</strong>?
  <ul>
    <li>Let us denote $A_1=$"hydraulic system $1$ fails", $A_2=$"hydraulic system $2$ fails" and $A_3=$"hydraulic system $3$ fails".</li>
    <li>The event where all hydraulic systems fail is given by,
    $$\left(A_1\text{ and } A_2\text{ and } A_3\right)$$  
    so that the airplane is able to maintain control in the complement of the above event:
    $$\overline{\left(A_1\text{ and } A_2\text{ and } A_3\right)}.$$
  </ul>
</ul>


========================================================

### Redundancy and the multiplication rule continued

<ul>
  <li>We recall, the probability that a randomly selected hydraulic system fails is $0.002$ and the three systems are <b>independent</b>.</li>
  <li>Therefore, we can use the multiplication rule as,
  $$\begin{align}
  P\left(A_1\text{ and } A_2\text{ and } A_3\right) &= P(A_1\text{ and } A_2\vert A_3) \times P(A_3)\\
  &=P(A_1\text{ and } A_2) \times P(A_3) \\
  &= P(A_1 \vert A_2) \times P(A_2) \times P(A_3) \\
  & = P(A_1)\times P(A_2) \times P(A_3)
  \end{align}$$
  because <strong>each of the events are inependent</strong>.</li>
  <li>Finally, we can write,
  $$P\left(\overline{\left(A_1\text{ and } A_2\text{ and } A_3\right)}\right) = 1 - P(A_1)\times P(A_2) \times P(A_3)= 1-0.002^3=0.999999998$$ </li>
  <li>This shows how including <strong>multiple independent systems</strong> greatly improves the probability of success.</li>
</ul>

========================================================

## Another way of writing independence

<ul>
  <li>What we saw in the last slide,
  $$P(A_1 \text{ and } A_2 \text{ and } A_3) = P(A_1) \times P(A_2) \times P(A_3)$$
  actually <strong>holds generally</strong> for <b>independent events</b>.</li>
  <li>Let's suppose that $A$ and $B$ are <b>independent events</b> such that
  $$\begin{align}
  P(A\vert B) = P(A) && P(B\vert A) = P(B).
  \end{align}$$</li>
  <li>Consider the multiplication rule for the two <b>independent events</b> $A$ and $B$,
  $$\begin{align}
  P( A \text { and } B) &= P(A \vert B) \times P(B) \\
  &=P(A) \times P(B),
  \end{align}$$
  using the independence assumption.</li>
  <li>In fact, we can show that this <strong>holds for any number of independent events</strong>, re-using the argument above.</li>
  <li>Let $A_1$, $A_2$, $A_3,$ $\cdots$ $A_n$ be any <strong>arbitrary list</strong> of <b>mutually independent events</b>.</li>  
  <li>Then using the argument above $n$ times, we can show that,
  $$P(A_1 \text{ and } \cdots \text{ and } A_n) = P(A_1) \times \cdots \times P(A_n).$$</li>
  <li>Although the intuition of the statement for independence
   $$\begin{align}
  P(A\vert B) = P(A) && P(B\vert A) = P(B)
  \end{align}$$
  is easier to interpret,</li>
  <li>in practice, we will usually describe independence as 
  $$P(A_1 \text{ and } \cdots \text{ and } A_n) = P(A_1) \times \cdots \times P(A_n).$$</li>
  <li>These two notions are in fact equivalent by the argument above.</li>
</ul>


========================================================

## The probability of "at least one"


<div style="float:left; width:40%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>It is a common problem that we want to find the probability of <b>at least one</b> event occuring.</li>
  <li>For example, let's suppose that a car engine has a known defect rate in the production process of $\frac{5}{1000}$.</li>
  <li>The probability of any two randomly selected engines being defective will be considered independent;</li>
  <li>suppose we want to know the probability that a shipment of $50$ cars has at least one defective unit that will need a return shipment.</li>
  <li>Analyzing the problem directly can be difficult unless we consider <b>complementary events</b>.</li>
  <li>Let's suppose that $A=$"at least one engine is defective".</li>
  <li>Notice that using <b>complements</b>
  $$\overline{``\text{at lease one engine is defective"}} = \text{all engines work}$$ </li>
  <li>The probability that all engines work can instead be computed directly.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li><b>Discuss with a neighbor:</b> let $A_i=$"car $i$ has a working engine".  Using conjunctions what is the event where all engines work?  If we want to find the probability that at least one engine fails, how can we use complements to find this in terms of the $A_i$?</li>
  <ul>
    <li>Notice, 
    $$(A_1 \text{ and } \cdots \text{ and } A_{50}) = \text{ all cars have working engines}$$
  </ul>
  <li>Therefore we want to find the probability of $\overline{(A_1 \text{ and } \cdots \text{ and } A_{50})}$.</li> 
</ul>
</div>

========================================================

## The probability of "at least one" continued


<div style="float:left; width:40%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>We know that the probability of engine $i$ working is
  $$P(A_i) = \frac{995}{1000},$$
  because it is the complement of engine $i$ failing (with probability $\frac{5}{1000}$).</li>
  </li>
  <li>We also know that all $A_i$ are independent from each other.
  <ul>
    <li>Therefore,
    $$\begin{align}
    P(A_1 \text{ and } \cdots \text{ and } A_{50}) &= P(A_1)\times \cdots \times P(A_n)\\
    &= \left(\frac{995}{1000}\right)^{50}  
    \end{align}$$
  </ul>
  <li><b>Discuss with a neighbor:</b> if we want to find 
  $$P\left(\overline{(A_1 \text{ and } \cdots \text{ and } A_{50})}\right),$$
  </li>
  <li>how can we use the <strong>rule of complements</strong>,
  $$P(B) + P(\overline{B}) = 1$$
  to find the above probability?</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>Let $B=(A_1 \text{ and } \cdots \text{ and } A_{50})$, then we have,
  $$P(\text{at least one engine fails})= 1 - \left(\frac{995}{1000}\right)^{50} \approx 0.222.$$
</ul>
</div>

========================================================

## Review of key concepts

<ul>
  <li>The most important concepts covered here are:</li>
  <ul>
    <li>how to join events $A$ and $B$ with our two operations,</li>
    <ol>
      <li><b>$A$  "or" $B$</b> -- the case that $A$, $B$ or both $A$ and $B$ occur; and </li>
      <li><b>$A$  "and" $B$</b> -- the case that both $A$ and $B$ occur;</li>
    </ol>
    <li>how to take complements of events, and how the probabilities are related, e.g.,
    $$P(A) + P\left(\overline{A}\right) = 1;$$</li>
    <li>how to use the two probability rules,
    <ol>
      <li><b>Addition rule</b> -- for the event $A$ <b>or</b> $B$,
      $$P(A\text{ or }B)= P(A) + P(B) - P(A \text{ and } B);$$</li>
      <li><b>Multiplication rule</b> -- for the event $A$ <b>and</b> $B$,
      $$P(A \text{ and } B) = P(B\vert A) \times P(A);$$</li>
    </ol>
    <li>the notion of <b>independence</b> between events $A$ and $B$,
    $$\begin{align}
    P(A\vert B) = P(A) & & P(B\vert A) = P(B).
    \end{align}$$
    </li>
    <li>and the product rule for mutually independent events:
    <ul>
    <li>let $A_1$, $A_2$, $A_3,$ $\cdots$ $A_n$ be any <strong>arbitrary list</strong> of <b>mutually independent events</b>, then
  $$P(A_1 \text{ and } \cdots \text{ and } A_n) = P(A_1) \times \cdots \times P(A_n).$$</li>
    </ul>
  </ul>
</ul>


========================================================

## Example of conditional probability

<div style="float:left; width:55%;text-align:center;">
<img src="drug_test.png" style="width:100%" alt="Pre-employment drug screening.">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 5th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
  <li>Let's recall the multiplication rule,
  $$P(A \text{ and } B) = P(B \vert A) \times P(A).$$</li>
  <li>Note that we can always use this formula in an alternative form whenever $P(A)\neq 0$,
  $$P(B\vert A ) =\frac{ P(A \text{ and } B) }{P(A)}.$$</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li><b>Discuss with a neighbor:</b> if $1$ of the $1000$ test subjects is randomly selected, let the events be $A=$"the participant uses drugs" and $B=$"the participant has a positive test result".</li>
  <li> How can we use the above probability rules to find the probability that a random subject had a positive test result, given that the subject actually uses drugs?</li>
  <ul>
    <li>Notice that the above statement is just $P(B\vert A)$, so that we can compute this value by
    $$P(B\vert A ) =\frac{ P(A \text{ and } B) }{P(A)}.$$</li>
  </ul>
  <li><b>Discuss with a neighbor:</b> what is the value of $P(B \vert A)$?</li> 
  <ul>
    <li>Notice that
    $$\begin{align}
    P(A) &= \frac{44\text{ true positives} + 6 \text{ false negatives}}{1000} = \frac{50}{1000},\\
    P(A \text{ and } B) & = \frac{44 \text{ true positives} }{1000} = \frac{44}{1000}
    \end{align}$$ </li>
  </ul>
</ul>
</div>

========================================================

### Example of conditional probability continued


<div style="float:left  ; width:45%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:55%">
<ul>
  <li>Using the values of 
  $$\begin{align}
  P(A)= \frac{50}{1000} & &P(A \text{ and } B) = \frac{44}{1000},
  \end{align}$$
  </li>
  <li>and using the definition of conditional probability 
  $$P(B\vert A ) =\frac{ P(A \text{ and } B) }{P(A)},$$</li>
  <li>we can then show that,
  $$\begin{align}
  &P(\text{Subject has positive test result }given\text{ the subject uses drugs}) \\
  =& \frac{\frac{44}{1000}}{\frac{50}{1000}} = \frac{44}{50} = .88
  \end{align}$$</li>
  <ul>
    <li>Notice how in the above we cancel the denominators of $1000$; this corresponds "physically" once again to restricting to the sample space where $A$ occurs,</li>
    <li>i.e., we restrict to the <b style="color:red">red circle</b> from the venn diagram.</li>  
  </ul>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>We then compute the probability of $A \text{ and }B$ occuring relative to the probability of $A$,</li>
  <ul>
    <li>i.e., we compute the <b>probability of the intersection</b> relative to the probability of the <b style="color:red">red circle</b>.</li> 
  </ul>
  <li>This cancels the denominators correspoding to the full sample space, $\frac{1}{1000}$, in the conditional probability above.</li>
</ul>
</div>

========================================================

### Example of conditional probability continued


<div style="float:left; width:55%;text-align:center;">
<img src="drug_test.png" style="width:100%" alt="Pre-employment drug screening.">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 5th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
  <li><b>Discuss with a neighbor:</b> let the events be given again as $A=$"the participant uses drugs" and $B=$"the participant has a positive test result".</li>
  <li>If $1$ of the $1000$ test subjects is randomly selected, can we write
  $$P(B\vert A ) = P(A\vert B)?$$
  Why or why not?</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <ul>
    <li>Using the definition of,
    $$P(A \vert B) = \frac{P(A \text{ and } B)}{P(B)}$$
    we can see that
    $$\begin{align}
    &P(A \vert B) \\  
    = &P(\text{participant uses drugs } given\text{ participant has a positive test result})\\
    =&\frac{\frac{44}{1000}}{\frac{133}{1000}} = \frac{44}{133} \approx .33
    \end{align}$$</li>
    <li>However, on the other hand $P(B\vert A) = .88$
  </ul>
</ul>
</div>


========================================================

### Example of conditional probability continued


<div style="float:left; width:40%">
<img src="venn_diagramm.png" style="width:100%"  alt="Venn diagram of events $A$ and $B$ with nontrivial intersection.">
<p style="text-align:center">
Courtesy of Bin im Garten  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC</a> via  
        <a href="https://commons.wikimedia.org/wiki/File:Menge_Venn-Diagramm_001.svg"> Wikimedia Commons</a>
</p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>In fact, $P(A\vert B) \neq P(B\vert A)$ and they are quite different.</li>  
  <li>This actually says that
    <ul>
      <li>there is <strong>lower probability</strong> that the <b>participant uses drugs</b> given that their <b>test result is positive</b>, <strong>than</strong></li> 
      <li>the probability that the <b>test result is positive</b>, given that the <b>participant uses drugs</b>.</li> 
  </ul>
  <li>This can be understood in terms of the denominators,
  $$\begin{align}
  P(A\vert B) = \frac{P(A \text{ and } B)}{P(B)} & & P(B\vert A) = \frac{P( A \text{ and } B)}{P(A)}  
  \end{align}$$</li>
  <li>Both of the above statements measure $P(A \text{ and } B)$, but <strong>relative to different spaces</strong>.</li>
  <li>For $P(A\vert B)$, we restrict to the <b>black circle</b> and <strong>measure the intersection</strong>.</li>
    <ul>
    <li>I.e., we measure the intersection <b>relative to the number of positive test results</b>.</li>
  </ul>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>On the other hand for $P(B \vert A)$ we restrict to the <b style="color:red">red circle</b> and <strong>measure the intersection</strong>.</li>
  <ul>
    <li>I.e., we measure the intersection <b style="color:red">relative to the number of participants using drugs</b>.</li>  
  </ul>
  <li>In fact, there are far fewer participants using drugs than the number of positive test results,</li>
  <ul>
    <li>therefore, the denominator for $P(A\vert B)$ is larger, and 
    $$P(A\vert B) < P(B\vert A).$$</li> 
  </ul>
</ul>
</div>

========================================================

## Bayes' theorem

<ul>
  <li>Let us suppose that $A$ and $B$ are events for which $P(A)\neq 0$ and $P(B)\neq 0$.</li>
  <li>Consider the statement of the multiplication rule,
  $$P(A \text{ and } B) = P(A\vert B) P(B); $$
  </li>
  <li>yet it is also true that,
  $$P(B\text{ and } A) = P(B \vert A) P(A); $$</li>
  <li>and $P( A \text{ and } B) = P(B \text{ and } A)$ by definition.</li>  
  <li>Putting these statments together, we obtain,
  $$\begin{align}
  &P(A\vert B) P(B) = P(B \vert A ) P(A)\\
  \Leftrightarrow & P(A \vert B) = \frac{P(B\vert A) P(A)}{ P(B)}  
  \end{align}$$</li>
  <li>The statement that 
  $$ P(A \vert B) = \frac{P(B\vert A) P(A)}{ P(B)} $$
  is known as Bayes' theorem.</li>
  <li>This is nothing more than re-writing the multiplication rule as discussed above, but the result is extremely powerful.</li>
  <li>Bayes' theorem wasn't widely used in statistics for hundreds of years, until advances in digital computers.</li>
  <li>When digital computers became available, many tools became available using Bayes' theorem as the basis.</li>
</ul>

========================================================

## Bayes' theorem continued

<ul>
  <li>Often, Bayes 
  $$ P(A \vert B) = \frac{P(B\vert A) P(A)}{ P(B)} $$
  is used as a way to <b>update the probability of $A$</b> when you have <strong>new information $B$</strong>.</li>
  <ul>
    <li>For example, let the events $A=$"it snows in the Sierra" and $B=$"it rains in my garden".</li>
    <li>I might think there is a $P(A)$ <b>prior probability</b> for snow, without knowing any other information.</li>
    <li>$P(A\vert B)$ is the <b>posterior probability</b> of snow in the Sierra given rain in my garden.</li>
    <li>If I found out later in the day that there was rain in my garden, I could <strong>update $P(A)$ to $P(A\vert B)$</strong> by multiplying
    $$P(A\vert B) = P(A) \times \left(\frac{P(B\vert A)}{P(B)}\right)$$
    directly.</li>
    <li>Although this is a simplistic example, this logic is the basis of many weather prediction techniques.</li>  
  </ul>
  
========================================================

### Bayes' theorem example

<div style="float:left; width:55%;text-align:center;">
<img src="cancer_2.png" style="width:100%" alt="Cancer results .">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 6th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
  <li>Another classic example of using Bayes is with medical diagnoses.</li>
  <li>It is often the case that the implications of a positive test result are misunderstood.</li>
  <li>For example, let's suppose that $1\%$ of the population has some kind of cancer.</li>
  <li>Let's suppose that there is a test for cancer with a:</li>
  <ol>
    <li><b>false positive</b> rate of $10\%$; and a</li>
    <li><b>true positive</b> rate of $80\%$.</li>
  </ol>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
 <li>Before we compute Bayes' formula, we will use the above information to deduce the values for the above table.</li>
 <li><b>Discuss with a neighbor:</b> if we want to pretend that the numbers above hold exactly for some sample of $1000$ individuals, how many total individuals would have cancer.</li>
 <ul>
  <li>This would be $.01\times 1000 = 10$.</li>
  <li>This is precisely the sum of the True Positive and False Negatives in the first row.</li>
 </ul>
</ul>
</div>

========================================================

### Bayes' theorem example

<div style="float:left; width:55%;text-align:center;">
<img src="cancer_3.png" style="width:100%" alt="Cancer results .">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 6th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
 <li>Recall that $1\%$ of the population has some kind of cancer and that there is a test for cancer with a:</li>
  <ol>
    <li><b>false positive</b> rate of $10\%$; and a</li>
    <li><b>true positive</b> rate of $80\%$.</li>
  </ol>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li><b>Discuss with a neighbor:</b> if we want to pretend that the numbers above hold exactly for some sample of $1000$ individuals, how many total individuals would not have cancer.</li>
   <ul>
    <li>This would be $( 1 - .01)\times 1000 = 990$.</li>
    <li>This is precisely the sum of the False Positives and True Negatives in the second row.</li>
   </ul>
</ul>
</div>


========================================================

### Bayes' theorem example

<div style="float:left; width:55%;text-align:center;">
<img src="cancer_4.png" style="width:100%" alt="Cancer results .">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 6th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
 <li>Recall that $1\%$ of the population has some kind of cancer and that there is a test for cancer with a:</li>
  <ol>
    <li><b>false positive</b> rate of $10\%$; and a</li>
    <li><b>true positive</b> rate of $80\%$.</li>
  </ol>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li><b>Discuss with a neighbor:</b> if we want to pretend that the numbers above hold exactly for some sample of $1000$ individuals, how many total individuals would be false positives.</li>
   <ul>
    <li>This would be $990 \times 0.1 = 99$.</li>
   </ul>
</ul>
</div>

========================================================

### Bayes' theorem example

<div style="float:left; width:55%;text-align:center;">
<img src="cancer_5.png" style="width:100%" alt="Cancer results .">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 6th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
 <li>Recall that $1\%$ of the population has some kind of cancer and that there is a test for cancer with a:</li>
  <ol>
    <li><b>false positive</b> rate of $10\%$; and a</li>
    <li><b>true positive</b> rate of $80\%$.</li>
  </ol>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li><b>Discuss with a neighbor:</b> if we want to pretend that the numbers above hold exactly for some sample of $1000$ individuals, how many total individuals would be true negatives.</li>
   <ul>
    <li>This would be $990 - 99 = 891$.</li>
   </ul>
</ul>
</div>

========================================================

### Bayes' theorem example

<div style="float:left; width:55%;text-align:center;">
<img src="cancer_6.png" style="width:100%" alt="Cancer results .">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 6th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
 <li>Recall that $1\%$ of the population has some kind of cancer and that there is a test for cancer with a:</li>
  <ol>
    <li><b>false positive</b> rate of $10\%$; and a</li>
    <li><b>true positive</b> rate of $80\%$.</li>
  </ol>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li><b>Discuss with a neighbor:</b> if we want to pretend that the numbers above hold exactly for some sample of $1000$ individuals, how many total individuals would be true positives.</li>
   <ul>
    <li>This would be $10 \times 0.8 = 8$.</li>
   </ul>
</ul>
</div>

========================================================

### Bayes' theorem example

<div style="float:left; width:55%;text-align:center;">
<img src="cancer_7.png" style="width:100%" alt="Cancer results .">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 6th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
 <li>Recall that $1\%$ of the population has some kind of cancer and that there is a test for cancer with a:</li>
  <ol>
    <li><b>false positive</b> rate of $10\%$; and a</li>
    <li><b>true positive</b> rate of $80\%$.</li>
  </ol>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li><b>Discuss with a neighbor:</b> if we want to pretend that the numbers above hold exactly for some sample of $1000$ individuals, how many total individuals would be false negatives.</li>
   <ul>
    <li>This would be $10 - 8 = 2$.</li>
   </ul>
</ul>
</div>

========================================================

### Bayes' theorem example

<div style="float:left; width:55%;text-align:center;">
<img src="cancer_1.png" style="width:100%" alt="Cancer results .">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 6th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
 <li>Recall that $1\%$ of the population has some kind of cancer and that there is a test for cancer with a:</li>
  <ol>
    <li><b>false positive</b> rate of $10\%$; and a</li>
    <li><b>true positive</b> rate of $80\%$.</li>
  </ol>
  <li>We will now use the above information to compute Bayes' formula for the update of the prior to the posterior probability.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>Let $A$ be the event that a randomly selected individual has cancer.</li>
  <li>Let $B$ be the event that a randomly selected individual gets a positive test result.</li>
  <li><b>Discuss with a neighbor:</b> what is the prior probability that a randomly selected individual has cancer?</li>
   <ul>
    <li>This is precisely $P(A)=1\%$</li>
   </ul>
   <li><b>Discuss with a neighbor:</b> what is $P(B \vert A)$?</li>
   <ul>
    <li>This is precisely the rate of true positives $80\%$.</li>  
   </ul>
   <li><b>Discuss with a neighbor:</b> what is the posterior probability of a randomly selected individual having cancer knowing that they have a positive test result $P(A \vert B)$?</li>
   <ul>
    <li>This is given by
    $$P(A\vert B) = \frac{P(B\vert A) P(A)}{P(B)} = \frac{.01 \times .08 }{\frac{107}{1000}}\approx 7.48\%$$
    </li>
   </ul>
</ul>
</div>  


========================================================

### Bayes' theorem example

<div style="float:left; width:55%;text-align:center;">
<img src="cancer_1.png" style="width:100%" alt="Cancer results .">
<p  style="text-align:center">
Courtesy of Mario Triola, <em>Essentials of Statistics</em>, 6th edition
</p>
</div>
<div style="float:left; width:45%">
<ul>
  <li>Bayes' theorem thus tells us an interesting fact about the cancer diagnosis.</li>
  <li>Because the probability of a true positive depends on both:</li>
  <ol>
    <li>the base-line rate of cancer in the population (<b>the prior</b>); and</li>
    <li>the <b>likelihood</b> of a true positive versus a false positive;</li>
  </ol>
  <li>the probability of having cancer given a positive test result is quite low.</li>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
  <li>Bayes' theorem tells us that our prior probability of having cancer (with all factors held equal) is $1\%$.</li>
  <li>Even if we obtain a positive test result, because so few individuals have cancer to begin with, the probability of having cancer conditional on the test result rises to only about $7.48\%$.</li>
  <li>This is one thing that is commonly misunderstood about medical diagnoses, but which Bayes clarifies.</li>
  <li>Particularly, the <strong>probability of a true positive</strong> depends on both the <b>prior probability</b> and the <b>likelihood</b>.</li>
</ul>
</div>