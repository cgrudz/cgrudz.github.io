========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080

========================================================

<h1> Review -- linear models and hypothesis testing</h1>

* Recall from last time:

  * Suppose we have a large model $\boldsymbol{\Omega}$, which abstractly refers to the set of all linear models possible by choices of $p$ parameters in $\beta$, and their respective uncertainties, over certain variables $x_1, x_2, \cdots, x_{p-1}$ (with an intercept term).

  * Let $q < p$, and suppose that abstractly $\boldsymbol{\omega}$ represents a "smaller model", as found by a strictly smaller set of explanatory variables, $x_1, x_2, \cdots, x_{q-1}$ (with an intercept term).

  * We will say that we favor the model $\boldsymbol{\omega}$ unless $\boldsymbol{\Omega}$ provides appreciably better results.
 
   * I.e., if $RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}$ is small, then we favor $\boldsymbol{\omega}$. 

========================================================

*  Concretely, we compute the diffence in the residual sum of squares scaled by the differences of degrees of freedom, i.e.,
  * the difference of degrees of freedom between the small and the large model $(p-q)$; and
  * the difference of the degrees of freedom between the large model and the number of observations $(n-p)$.
 
* The variable $F$ defined as follows
  
   $$\begin{align}
   F &\triangleq \frac{ \left( RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}\right)/    (p-q)}{RSS_\boldsymbol{\Omega}/(n-p)} 
 \end{align}$$
is an $F$ statistic, with $F$ distribution under the null hypothesis.
 
* This is to say,

 * "If the null hypothesis holds, then $F \sim F_{(p-q),(n-p)}$"

* We will thus study how surprising this value is or not, based on the assumption that $F$ is drawn from the $F_{(p-q),(n-p)}$.


========================================================

 <div style="float:left; width:50%">
<img style="width:100%", src="F-distribution_pdf.png"/>
Courtesy of IkamusumeFan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)]
</div>

<div style="float:right; width:50%">

<ul>
<li> The hypothesis testing procedure thus follows the following idea:</li>

<ul>
 <li> Let us say (for sake of example) we want to choose a model with $\alpha =5\%$ significance. </li>
 
 <li> We will look at the appropriate $F$ distribution and find the value of 
 $$ F^\alpha_{ (p-q), (n-p) }$$ 
 
 <li> such that the probability of 
 $$F > F^\alpha_{(p-q),(n-p)}$$ </li>
 
 
 <li><b> given </b> 
 $$F \sim F_{(p-q),(n-p)}$$ </li>
 
 <li>is less than or equal to
 $$\alpha = 5 \%.$$ </li>

</ul>

</ul>

</div>
 
 
========================================================

<h2> Examples </h2>

* Last time we considered the null hypothesis that there is no structure whatsover between the number of species in the Galapagos and the explanatory variables in the gala data set. 

* That is to say the number of species $\mathbf{Y}$ can be represented by random variation around the mean

$$\begin{align}
  \mathbf{Y} = \overline{\mathbf{Y}} + \epsilon
  \end{align}$$
  
* We have learned to test an F statistic for the difference of two groups of models, where one is given by a subspace of another: 
  * $\omega$ consists of models over variables $x_1, \cdots, x_{q-1}$ and corresponds to $q$ parameters (including the intercept);
  * $\Omega$ consists of models over variables $x_1, \cdots , x_{p-1}$ and corresponds to $p$ parameters (including the itercept); 
  * such that $q \text{<} p$.

* Concretely, the null hypothesis must be $H_0 : \boldsymbol{\beta}_i = \boldsymbol{0}$ for each $i=1,\cdots, p-1$.

* We computed the F statistic,
   $$\begin{align}
   F &\triangleq \frac{ \left( TSS - RSS_\boldsymbol{\Omega}\right)/    (p-1)}{RSS_\boldsymbol{\Omega}/(n-p)} 
 \end{align}$$
 and found that 
 $$\begin{align}1 - P(x \leq F)  \approx 10^{-7} 
 \end{align}$$
 
 
========================================================

* This process was summarized in the analysis of variance (ANOVA) table.


```r
library('faraway')
lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent,
gala)
nullmod <- lm(Species ~ 1, gala)
anova(nullmod, lmod)
```

```
Analysis of Variance Table

Model 1: Species ~ 1
Model 2: Species ~ Area + Elevation + Nearest + Scruz + Adjacent
  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
1     29 381081                                  
2     24  89231  5    291850 15.699 6.838e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

* We reject the null hypothesis and conclude that <b>at least one explantory variable</b> has predictive power.


========================================================

<h2> Another example</h2>

* Suppose we wish to test the significance of a <b>single explanatory variable</b>.

* <b> Q:</b> If we wish to evaluate the variable $x_i$, what is the null hypothesis in this case?

* <b> A:</b> $H_0 : \beta_i = 0$, but we must also be careful specify relative to which other variables the null and alternative hypotheses are defined.
 
* Concretely suppose we wish to model the species in the Galapagos, where we test "area" for significance.



========================================================

* We define the model $lmods$ without area as an explanatory variable for the null hypothesis.  Then we compute the ANOVA table with the bigger model.


```r
lmods <- lm(Species ~ Elevation + Nearest + Scruz + Adjacent, gala)
anova(lmods, lmod)
```

```
Analysis of Variance Table

Model 1: Species ~ Elevation + Nearest + Scruz + Adjacent
Model 2: Species ~ Area + Elevation + Nearest + Scruz + Adjacent
  Res.Df   RSS Df Sum of Sq      F Pr(>F)
1     25 93469                           
2     24 89231  1    4237.7 1.1398 0.2963
```

* The result of the $F$ test is to say,
  
  "With probability 29.63%, we will find a value drawn from the F distribution with this value or greater"
  
* Here we <b>fail to reject the null hypothesis</b> because it is reasonable that the difference between the large model and the small model could be due to random variation.
 * <b>Note:</b> This is not the same thing as "we accept the null hypothesis".  There may be some statistical relationship, but we haven't detected one that wouldn't be surpising if it was just noise.
 
========================================================

* This can also be found with respect to the student t test.

* Recall, if we want to estimate the standard error of a given parameter $\hat{\beta}_i$, this is given
$$\begin{align}
se(\beta_{i-1}) \triangleq \hat{\sigma}\sqrt{(\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}_{ii}}
\end{align}$$
 where $\hat{\sigma}^2 = \frac{RSS}{n-p}$
 
* Then the value,
$$\begin{align}
t_i = \frac{\hat{\boldsymbol{\beta}}_i}{se(\hat{\boldsymbol{\beta}}_i)}
\end{align}$$
 has t-distribution in $(n-p)$ degrees of freedom under the null hypothesis.
  * That is, we will determine the probability of obtaining a random value $t$ where $\vert t \vert > \vert t_i \vert$ with respect to the t-distribution to determine significance. 

========================================================

* We can now interpret the entire summary table, except for adjusted $R^2$ and multiple $R^2$:

```r
summary(lmod)
```

```

Call:
lm(formula = Species ~ Area + Elevation + Nearest + Scruz + Adjacent, 
    data = gala)

Residuals:
     Min       1Q   Median       3Q      Max 
-111.679  -34.898   -7.862   33.460  182.584 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.068221  19.154198   0.369 0.715351    
Area        -0.023938   0.022422  -1.068 0.296318    
Elevation    0.319465   0.053663   5.953 3.82e-06 ***
Nearest      0.009144   1.054136   0.009 0.993151    
Scruz       -0.240524   0.215402  -1.117 0.275208    
Adjacent    -0.074805   0.017700  -4.226 0.000297 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 60.98 on 24 degrees of freedom
Multiple R-squared:  0.7658,	Adjusted R-squared:  0.7171 
F-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07
```

========================================================

* Suppose we test $H_0:\hat{\boldsymbol{\beta}}_{area}=0$ but with respect to another model:


```r
 summary(lm(Species ~ Area, gala))
```

```

Call:
lm(formula = Species ~ Area, data = gala)

Residuals:
    Min      1Q  Median      3Q     Max 
-99.495 -53.431 -29.045   3.423 306.137 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 63.78286   17.52442   3.640 0.001094 ** 
Area         0.08196    0.01971   4.158 0.000275 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 91.73 on 28 degrees of freedom
Multiple R-squared:  0.3817,	Adjusted R-squared:  0.3596 
F-statistic: 17.29 on 1 and 28 DF,  p-value: 0.0002748
```

========================================================

<h2> Testing a pair of explanatory variables</h2>

* Suppose we wish to determine if the two variables $area$ and $adjacent$ have an effect on the response relative to the model with all other variables.

* Particularly, we obtain the null hypothesis $H_0 : \boldsymbol{\beta}_{area}= \boldsymbol{\beta}_{adjacent}=0$.


```r
lmods <- lm(Species ~ Elevation + Nearest + Scruz, gala)
anova(lmods, lmod)
```

```
Analysis of Variance Table

Model 1: Species ~ Elevation + Nearest + Scruz
Model 2: Species ~ Area + Elevation + Nearest + Scruz + Adjacent
  Res.Df    RSS Df Sum of Sq      F  Pr(>F)   
1     26 158292                               
2     24  89231  2     69060 9.2874 0.00103 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

* The probability of drawing such an F value is around 1/1000, so we reject the null hypothesis.


========================================================

* Note, rejecting the null hypothesis in the last example, testing a pair of variables, cannot be inferred from their respective t-tests. 


```r
sumary(lmod)
```

```
             Estimate Std. Error t value  Pr(>|t|)
(Intercept)  7.068221  19.154198  0.3690 0.7153508
Area        -0.023938   0.022422 -1.0676 0.2963180
Elevation    0.319465   0.053663  5.9532 3.823e-06
Nearest      0.009144   1.054136  0.0087 0.9931506
Scruz       -0.240524   0.215402 -1.1166 0.2752082
Adjacent    -0.074805   0.017700 -4.2262 0.0002971

n = 30, p = 6, Residual SE = 60.97519, R-Squared = 0.77
```

* There is no simple way of combining the above information to test a pair of explanatory variables simultaneously.

* Testing a pair of variables needs to be performed with the F test.
