========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080

========================================================

<h1> Orthogonality </h1>

* We have discussed largely about the case where there is some dependence (i.e., correlation) among explanatory variables.
  
  * For super-saturated models (with $p> n$), there must be linear dependence among explanatory variables and there is no way to recover "best" values for $\beta$ as there are infinitely many solutions.
  
  * For cases of linear dependence between variables (with $p< n$), there is redundancy between these variables that adds no useful information.
  
  * Linear dependence, or <em>almost-dependence</em> (i.e., highly correlated variables), when $p < n$ leads to singularities -- particularly, there may be issues with trying to solve by least squares or the uncertainty of predictions will be extremely large.

* <b>Q:</b> Qualitatively, what occurs when the explanatory variables are totally indpedendent?

* <b>Q:</b> How does this aid our analysis?

* This will boil down to the idea of orthogonality, loosely speaking, when the space spanned by each variable is perpendicular to each other.

========================================================

<h2>What is orthogonality?</h2>

* Orthogonality can be loosely read as "perpendicular"

* Recall an equivalent description of the vector inner product,

$$\begin{align}
\mathbf{a} \cdot \mathbf{b}  & = \parallel \mathbf{a} \parallel \parallel \mathbf{b} \parallel cos(\theta) \\
&= \text{"length of } \mathbf{a}\text{"} \times \text{"length of } \mathbf{b}\text{"} \times cos( \text{"the angle between"})
\end{align}$$

* If there are 90 degrees between the two vectors $\mathbf{a}$ and $\mathbf{b}$, then  $\mathbf{a} \cdot \mathbf{b} = 0$.


========================================================

<h2>Orthogonal decomposition </h2>

* This idea extends to matrices, when the columns of two matrices are orthogonal.

* In particular, if $\mathbf{A}$ is an orthogonal matrix then $\mathbf{A}^\mathrm{T}\mathbf{A} = \mathbf{I}$.
 * The transpose product here represents the dot product of each column of $\mathbf{A}$ with each other column.
 * Each column is orthogonal to each of the others, so off diagonal entries are zero.
 * We call the <em>matrix</em> $\mathbf{A}$ orthogonal when each of the columns are also normalized to have length one, thus giving the ones on the diagonal.
 
======================================================== 

* Suppose we can decompose the explanatory variables into two groups $\mathbf{X}_1$ and $\mathbf{X}_2$ which are orthogonal to each other, <emph>but perhaps not of norm one</emph>.

 $$\begin{align}
\mathbf{X} &\triangleq 
\begin{pmatrix}
\mathbf{X}_1 \vert \mathbf{X}_2
\end{pmatrix}
\end{align}$$

* Notice that (regardless of orthogonality) we have the equality:

$$\begin{align}
\mathbf{X}\beta &= 
\begin{pmatrix}
\mathbf{X}_1 \vert \mathbf{X}_2
\end{pmatrix}
\begin{pmatrix}
\beta_1  \\ \beta_2
\end{pmatrix} \\
&= \mathbf{X}_1 \beta_1 + \mathbf{X}_2 \beta_2
\end{align}$$

 so that we split the explanatory variables and parameters into two groups.

======================================================== 


* <b>Exercise:</b> Assume that $\mathbf{X} \triangleq \begin{pmatrix}\mathbf{X}_1 \vert \mathbf{X}_2 \end{pmatrix}$ 
and $\mathbf{X}_1$ and $\mathbf{X}_2$ are orthogonal to each other.  What does $\mathbf{X}^\mathrm{T} \mathbf{X}$ equal block-wise?

* <b>Answer:</b>

 $$\begin{align}
 \mathbf{X}^\mathrm{T} \mathbf{X} &=
 \begin{pmatrix}
 \mathbf{X}^\mathrm{T}_1 \mathbf{X}_1 & \mathbf{0} \\
 \mathbf{0} & \mathbf{X}^\mathrm{T}_2 \mathbf{X}_2
 \end{pmatrix}
 \end{align}$$
 
* <b>Exercise:</b> In this case, what does the orthogonal projection operator equal block-wise?

* <b>Answer:</b>

 $$\begin{align}
  \mathbf{H} &\triangleq \mathbf{X}\left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T} \\
 &=\begin{pmatrix}
 \mathbf{X}_1\left(\mathbf{X}^\mathrm{T}_1 \mathbf{X}_1\right)^{-1}\mathbf{X}_1^\mathrm{T} & \mathbf{0} \\
 \mathbf{0} &  \mathbf{X}_2\left(\mathbf{X}^\mathrm{T}_2 \mathbf{X}_2\right)^{-1}\mathbf{X}_2^\mathrm{T}
 \end{pmatrix}
 \end{align}$$


======================================================== 

* In the previous exercises, we see that the prediction decomposes entirely along the two sub-sets of variables.

* Likewise, we will find that $\hat{\boldsymbol{\beta}}$ decomposes into two sets of parameters $\hat{\boldsymbol{\beta}}_1$ and $\hat{\boldsymbol{\beta}}_2$.

* <b> Q: </b> How does this relate to correlation/ covariance?

* Recall the anomalies around the mean of a random variable $x_1$ is given  
 $$\begin{align}
 \mathbf{a}_1 \triangleq  
 \begin{pmatrix}
 x_{1,1} - \overline{x}_1 \\
 \vdots \\
 x_{n,1} - \overline{x}_1
 \end{pmatrix}
 \end{align}
 $$
 
* With the above definition, the covariance of the two random variables $x_1$ and $x_2$ is equal to $\frac{1}{n-1}\mathbf{a}_1^\mathrm{T} \mathbf{a}_2$

* We say that if the two variables $x_1$ and $x_2$ have zero covariance, this means that the anomalies are orthogonal.

* A correlation matrix is just the covariance matrix normalized to "size=1" on the diagonal.  

* The covariance is the numerator, so equivalently, correlation zero means the anomalies are orthogonal.

* Orthogonality usually only exists in the data by good experimental design.

* Otherwise, the "important combinations" of the data can be made orthogonal by PCA/ SVD.

======================================================== 


* <b>Example:</b> an experiment was made to determine the effects of column temperature, gas/liquid ratio and packing height in reducing the unpleasant odor of a chemical product that was sold for household use.


```r
data(odor, package="faraway")
odor
```

```
   odor temp gas pack
1    66   -1  -1    0
2    39    1  -1    0
3    43   -1   1    0
4    49    1   1    0
5    58   -1   0   -1
6    17    1   0   -1
7    -5   -1   0    1
8   -40    1   0    1
9    65    0  -1   -1
10    7    0   1   -1
11   43    0  -1    1
12  -22    0   1    1
13  -31    0   0    0
14  -35    0   0    0
15  -26    0   0    0
```

* <b>Note: </b> Temperature has been rescaled by the transformation
 $$
\text{temp} = \frac{\text{Farenheit} - 80}{40}
 $$

======================================================== 

* We will compute the covariance of the explanatory variables, temp, gas and pack:

```r
cov(odor[,-1])
```

```
          temp       gas      pack
temp 0.5714286 0.0000000 0.0000000
gas  0.0000000 0.5714286 0.0000000
pack 0.0000000 0.0000000 0.5714286
```

* With the zero values in the non-diagonal elements, we know that the anomalies are orthgonal.


======================================================== 

* We will fit a linear model for the response variable odor with the explanatory variables tem, gas and pack.  <b>NOTE:</b> cor=T in the argument corresponds to printing the correlation of the explanatory variables.

```r
lmod <- lm(odor ~ temp + gas + pack, odor)
summary(lmod,cor=T)
```

```

Call:
lm(formula = odor ~ temp + gas + pack, data = odor)

Residuals:
    Min      1Q  Median      3Q     Max 
-50.200 -17.138   1.175  20.300  62.925 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   15.200      9.298   1.635    0.130
temp         -12.125     12.732  -0.952    0.361
gas          -17.000     12.732  -1.335    0.209
pack         -21.375     12.732  -1.679    0.121

Residual standard error: 36.01 on 11 degrees of freedom
Multiple R-squared:  0.3337,	Adjusted R-squared:  0.1519 
F-statistic: 1.836 on 3 and 11 DF,  p-value: 0.1989

Correlation of Coefficients:
     (Intercept) temp gas 
temp 0.00                 
gas  0.00        0.00     
pack 0.00        0.00 0.00
```


======================================================== 

* If we repeat fitting the linear model, but leave out the variable pack, we find almost the same result, except for minor changes in the uncertainty and the increased number of degrees of freedom.

```r
lmod <- lm(odor ~ temp + gas, odor)
summary(lmod,cor=T)
```

```

Call:
lm(formula = odor ~ temp + gas, data = odor)

Residuals:
   Min     1Q Median     3Q    Max 
-50.20 -36.76  10.80  26.18  62.92 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   15.200      9.978   1.523    0.154
temp         -12.125     13.663  -0.887    0.392
gas          -17.000     13.663  -1.244    0.237

Residual standard error: 38.64 on 12 degrees of freedom
Multiple R-squared:  0.1629,	Adjusted R-squared:  0.02342 
F-statistic: 1.168 on 2 and 12 DF,  p-value: 0.344

Correlation of Coefficients:
     (Intercept) temp
temp 0.00            
gas  0.00        0.00
```

======================================================== 

<h1> Uncertainty Quantification </h1>

* A quick refresher on the idea of uncertainty by example:

 * Suppose $T$ is a linear unbiased estimator for the speed of light $\theta$. 
 * For sake of example, also suppose that $T$ has standard deviation $\sigma_T$ = 100 km/sec. 
 * Recall Chebyshevâ€™s inequality, 
 
 $$\begin{align}
 P\left(\vert T - \theta \vert > k \sigma_T\right) <  \frac{1}{k^2}
 \end{align}$$
 
 * We find
 
  $$\begin{align}
 P\left(\vert T - \theta \vert < 2 \sigma_T\right) \geq  \frac{3}{4}
 \end{align}$$

 * This tells us that the probability of at least 75% that T is within 200 km/sec of the speed of light $\theta$. 
 
 * Equivalently, $\theta \in (T-200, T+200)$ with probability 75%.
 
======================================================== 
 
* Suppose the estimate $T$ gives us based on some data is $t=299852.4$

* We can say that  $\theta \in (299652.4, 300 052.4)$ with confidence 75%.
   * Note that $\theta$ is an an unkown constant -- it is either in the interval or not and there is nothing random about the above statement.
   * We can't say that the probability of $\theta \in (299652.4, 300 052.4)$ is 75%, but we used information to guarantee that our proceedure for estimation will work 75% of the time.
   
* Similarly, we will want to fit confidence intervals, and moreover to use hypothesis testing to determine the significance of our model versus the null hypothesis of random variation and no systematic structure.
 


 
