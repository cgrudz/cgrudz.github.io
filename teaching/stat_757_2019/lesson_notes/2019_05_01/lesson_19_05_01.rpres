========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080
  
========================================================

<h1> Categorical predictors </h1>

* In many circumstances we have been using categorical predictors in which there are one or more categories, e.g.,

```{r}
library(faraway)
head(teengamb, n = 3)
```

* Because the category of "M" or "F" have no mathematical meaning in the regression, this has become coded as a 1 or 0 in the model.

  * The terminology for this encoding of a category as an integer value is commonly known as a "dummy variable" where the value itself doesn't have a specific meaning, but the parameter will describe the effect.

* We have seen the effect of this this coding a giving the option of a "switch" on or off of a parameter $\beta_{sex}$, which adjusts the model for the categorical level.

  * Particularly, this gives an adjusted intercept term when it is "turned on" with a value of a 1, or the original intercept $\beta_0$ when the value of the category is 0.
 
* There are, however, several approaches we could take for a categorical variable as before with two levels...
 
========================================================


* For instance, supposing we model the response $y$ (gamble) in terms of a continuum variable $x$ (income) and a category (sex) with $d\in\{0,1\}$, we can neglect the categorical "switch" in the case we don't find it to be stastically significant, or interesting by criterion methods.  
  
  $$\begin{align}
    y = \beta_0 + \beta_1 x + \epsilon.
  \end{align}$$</li>

```{r fig.width=26, fig.height=4}
sumary(lm(gamble ~ income, data=teengamb))
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(gamble ~ income, teengamb, cex=3, cex.lab=3, cex.axis=1.5)
abline(-6.3246, 5.5205)
```

========================================================

* On the other hand, we might find that the continuum variable is not significant or interesting by criterion methods.  In this case, we can model the response purely in terms of the "switch" part of the equaiton":
 
  $$\begin{align}
  y = \beta_0 + \beta_2 d + \epsilon
  \end{align}$$
  
  so that we describe the response in terms of variation around two intercept levels.

```{r fig.width=24, fig.height=4}
sumary(lm(gamble ~ sex, data=teengamb))
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(gamble ~ sex, teengamb, cex=3, cex.lab=3, cex.axis=1.5)
points(list(x=c(0,1), y=c(29.775, 29.775-25.9092)), pch=17, cex=3)
```

========================================================

* We can also consider modeling the response with both the continuum variable and the categorical variable, where the <b>slope</b> of the response in terms of the continuum variable will be the same for each category;

  * in this case $\beta_{sex}$ describes the distance between the two regression lines (the adjusted intercept): 
  
 $$\begin{align}
  y = \beta_0 + \beta_1 x + \beta_2 d + \epsilon
  \end{align}$$
  
```{r fig.width=24, fig.height=4}
sumary(lm(gamble ~ income + sex, data=teengamb))
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(gamble ~ income, teengamb, cex=3, cex.lab=3, cex.axis=1.5)
abline(4.04083, 5.17158)
abline(4.04083-21.63429, 5.17158)
```

========================================================

* But it may also make sense to model the response with separate slopes and separate intercepts, done in R with

```{r fig.width=24, fig.height=4}
sumary(lm(gamble ~ income * sex, data=teengamb))
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(gamble ~ income, teengamb, cex=3, cex.lab=3, cex.axis=1.5)
abline(-2.65963, 6.51812)
abline(-2.65963+5.79960, 6.51812-6.34320)
```

========================================================

* This can be seen in a functional form as a model like:

  $$\begin{align}
  y = \beta_0 + \beta_1 x + \beta_2 d + \beta_{x:d} x \times d + \epsilon
  \end{align}$$
  
* Here the parameter $\beta_{x:d}$ is activated to adjust the slope when $d=1$ in the same way that the intercept is adjusted for $d=1$.

  * This parameter is commonly known as the interaction term, which can be more difficult to interpret at times; 
  
  * loosely this is the slope of the response in terms of the variable $x$ conditioned on the category.
  
  * This will also generall extend to multiple explanatory variables, where the category will shift the slope or intercept as determined optimal by the least squares solution.
  
* The interaction term can be an important element in our analysis, as it is seen in the gambling data, the relationship between income and gambling is almost flat for women.

* Performing the model fitting as above is a formal way of computing two separate models (and their differences) simultaneously when it appears that there is reason for two separate models.

* In particular, this is appropriate in the case when we don't think that the difference between the trend in the two categories should be constant.
  
========================================================

<h2> An example with heating efficiency</h2>

* A homeowner in England recorded his weekly natural gas consumption, in thousands
of cubic feet, during two winter heating seasons. 

* During the second season, additional insulation had been installed.

```{r fig.width=24, fig.height=5}
library(MASS)
library(ggplot2)
ggplot(aes(x=Temp,y=Gas),data=whiteside)+geom_point()+facet_grid(~
Insul)+geom_smooth(method="lm")+ theme(axis.text=element_text(size=20), axis.title=element_text(size=20,face="bold"), strip.text.x = element_text(size = 20, face="bold"))
```

* The horizontal axis is "probably the weekly average, daily minimum temperature" (see help file).

========================================================

* The previous plots, there is less gas used after the additional insulation is added, but the difference varies by temperature.

* Producing the multiple models with interaction, we have

```{r}
lmod <- lm(Gas ~ Temp*Insul, whiteside)
sumary(lmod)
```

* Here we see that the pre-insulation change in gas consumption is approximately .393 cubic feet per 1 degree change in temperature, whereas after the insulation it is approximately 0.278 cubic feet.

  * Producing separate slopes and intercepts does improve the overall fit of the model in this case.
  
  * Likewise, it is shown in the summary that the interaction term is significant.

* However, some addiitonal care should be taken with the intercept terms, representing the ammount of gas consumption when the temperature is zero.

  * Here there are very few observations, and therefore the uncertainty of these parameters representing the zero-temperature consumption will naturally be higher.
  

========================================================


* Generally, if the range or center of mass of the explanatory variables is far away from zero, we can rectify this issue by re-centering the variable at its mean (computing the anomalies).


* By centering at the mean, the intercept will actually describe the predicted value for the mean and the change in the explanatory variable describes the change as the variable deviates from the mean.

```{r}
mean(whiteside$Temp)
whiteside$ctemp <- whiteside$Temp - mean(whiteside$Temp)
lmodc <- lm(Gas ~ ctemp*Insul, whiteside)
sumary(lmodc)
```

* In this case, re-centering the model gives a more natural interpretation.


========================================================

* To obtain the model back in the normal coordinates, we can make a change of variables that includes this shift:

  $$\begin{align}
  &y = \beta_0 + \beta_{ctemp} \left( x- 4.875\right) + \beta_{InsulAfter} d + \beta_{ctemp:InsulAfter} x \times d\\
  \Leftrightarrow & y = 4.936788 - 0.393239 \left( x -  4.875\right) + -1.567872 d +  0.115304 x \times d \\
  \Leftrightarrow &  y = 6.853828 - 0.393239x  + -1.567872 d +  0.115304 x \times d
  \end{align}$$


========================================================

* Whereby plotting, we find:

```{r fig.width=24, fig.height=3}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0),usr=c(-2,12, 0, 8))
plot(Gas ~ Temp, whiteside[whiteside$Insul == "Before",], cex=3, cex.lab=3, cex.axis=1.5)
abline(6.853828, -0.393239)
```

```{r fig.width=24, fig.height=3}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0), usr=c(-2,12, 0, 8))
plot(Gas ~ Temp, whiteside[whiteside$Insul == "After",], cex=3, cex.lab=3, cex.axis=1.5)
abline(6.853828-1.567872, -0.393239 + 0.115304)
```




========================================================

<h2> Factors with more than two levels</h2>

* We can consider how to generalize this approach to where there are multiple levels in a categorical variable.

  * For instance, if we wanted to predict the price of a car, we might include several different color levels in a color categorical variable.
  
* If there are $f$ different levels, then we can create $f-1$ dummy variables $\{d_2,\cdots, d_f\}$ with the following meaning:

  $$\begin{align}
  d_i = 
  \begin{cases}
  0 & \text{if case is not level } i\\
  1 & \text{if cas is level } i
  \end{cases}
  \end{align}$$
  
  * When a case contains a value that isn't level $i=2,\cdots,f$, then this automatically defaults to the level $1$, described by the usual intercept, etc...
  
* We can consider multi-level factors in studying the longevity of fruitflies with respect to their sexual behaviors...

========================================================

<h2> Fruitfly example</h2>

* 125 male fruitflies are randomly divided into 5 groups of 25 each -- the response variable will be the number of days the fruitflies live.

* The explanatory variables for the response include thorax length, which is known to affect the longevity of the fly, and sexual activity.

* The groups are then studied with respect to one of several different environments:

  <ol>
    <li> one group is held without any female fruitflies;</li>
    <li> one group is held with a single sexually active female fruitfly, per day;</li>
    <li> one group is held with 8 sexually active female frutflies, per day;</li>
    <li> a control group is held with a single non-sexually active female fruitfly, per day;</li>
    <li> a final control group is held with 8 non-sexually active female fruitflies, per day.</li>
  </ol>
  
* These groups are labeled "isolated", "low", "high", "one" and "many" respectively.

========================================================

* Plotting all groups together, but distinguishing groups by symbols, we have:

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(longevity ~ thorax, fruitfly, pch=unclass(activity), cex=3, cex.lab=3, cex.axis=1.5)
legend(0.63,100,levels(fruitfly$activity),pch=1:5)
```

* However, examining the patterns within groups can be difficult with this type of plot.

========================================================

* Using ggplot again, we can use the facet wrap to plot the longevity versus the thorax length over each group individually:

```{r fig.width=24, fig.height=8}
ggplot(aes(x=thorax,y=longevity),data=fruitfly) + geom_point() +
facet_wrap( ~ activity) + theme(axis.text=element_text(size=20), axis.title=element_text(size=20,face="bold"), strip.text.x = element_text(size = 20, face="bold"))
```

* In this plot, it is more clear that the high sexual activity flies have a lower overall longevity, controlling for the thorax length.

========================================================

* If we create the model as before, allowing for different slopes and intercepts across each categorical level, we obtain the model summary as

```{r}
lmod <- lm(longevity ~ thorax*activity, fruitfly)
sumary(lmod)
```

* in this case, there is an adjustment to the thorax parameter with respect to each group outside of the default (isolation).

* The main difference for this type of multi-factor example is in terms of evaluating the significance of the interaction terms.

* In the summary table, we have four different t-test p-values to evaluate for the respective adjustments to the slope in life expectancy verus thorax size, compared to the group in isolation.

  * This will not, however, describe the overall significance of all interaction terms at once.


========================================================

* If we want to make a comparsion of the interaction between the levels and thorax all together, we can perform this with an ANOVA table:

```{r}
anova(lmod)
```

* Here, this represents a sequential evaluation of the nested models, in which
  <ol>
    <li> thorax is evaluated versus the null model;</li> 
    <li> activity and thorax is evaluated versus thorax alone; and </li> 
    <li> finally the model with interaction terms is evaluated versus the one in which all slopes are the same across groups. </li>
  </ol>
  
* In this case, the p-value for the interaction is not significant, and we seem to be able to reasonably model the response with the same slope across all groups.

========================================================

<h2> Alternative codings of categorical variables</h2>

* It is sometimes the case that a different form of coding (than 1/0) for a categorical variable can improve our analysis.

* We can imagine where we could have an issue as follows:

  * Suppose we have a matrix $\mathbf{B}$ with the $ij$-th entry representing case $i$ and factor level $j$.
  
  * We will follow the earlier convention where, if the $i$-th case is of the category level $j$, then $\mathbf{B}_{ij}=1$ and it will equal zero otherwise.
  
  * If we wanted to use $\mathbf{B}$ in the model matrix $\mathbf{X}$ as a sub-matrix to define the regression, there will be an issue with linear dependence.
  
  * Specifically, the intercept term is given by a vector of ones in $\mathbf{X}$.
  
  * On the other hand, the sum of all rows of $\mathbf{B}$ equals one, so that we can easily define a linear dependence relationship.
  
* Removing the intercept from the above will solve this for one factor, but not in general for more factors.

* Deleting one of the dummy variables to define a base-line level was the way we dealt with this in the earlier encoding.

* However, there are ways we might improve numerical stability by encoding this otherwise, or reflecting a different structure in the data.

========================================================

* If we take the above matrix $\mathbf{B}$, we can generally define how the data is coded into the model matrix with an additional <b>contrast matrix</b> $\mathbf{C}$.

* In this case, the product of $\mathbf{B}\mathbf{C}$ defines the encoding of the data in $\mathbf{X}$.

* The "treatment" coding is the method we have discussed earlier, in which each level will be compared with a base-line level.

* We can see this encoding in R using the "cont" function, with a type of coding. This will generate the contrast matrix to be right multiplied with $\mathbf{B}$.

* For a factor with 4 levels, the treatement coding contrast matrix is given as:

```{r}
contr.treatment(4)
```

* The first level is the multiplied by all zeros, which then means that this level corresponds to the intercept term (without adjustment as discussed earlier).

* Each subsequent level is given the standard "dummy" coding.  This has the interpretation of each other level being compared versus this base-line level, as discussed earlier, adjusted by the intercept.


========================================================

* "Helmert" coding is a method used for comparing each level versus all previous levels in a sequence.  

* This can be used typically when there is a natural ordering to the categories, e.g., "high", "medium", "low":

```{r}
contr.helmert(4)
```

* This can be viewed in the above where the second level is compared versus the first level, with the row of minus ones;

  * The third level is compared to the average of the first two, and so forth by the same analogy.
  
* In the same way that we understood the slope and intercept being adjusted according to the level in the earlier examples, the same logic applies here.

  * However, the type of coding changes the interpretation of the parameters and to which level the adjustment to slope and intercept is being made.

========================================================

* "Sum" coding uses the following contrast matrix:

```{r}
contr.sum(4)
```

* The sum coding can be useful to make intercomparisons where there is no natural reference level.

========================================================


<h1> Models with only categorical explanatory variables</h1>

* The "analysis of variance" that we have studied before actually comes from the situation in which we have only categorical variables as predictors.

* The name comes from the idea of partitioning the overall variance in the response into that which is due to each of the factors and the error $\epsilon$.

* While this is a classical approach, the formulation can become increasingly complex depending on the configuration of the configuration of the factors.

* In the following, we will consider a regression approach with the tools we have developed already.

* As a note on terminology, the way these variables are coded in R (factors with a number of levels) also derives from this framework.

* Additionally, for ANOVA type problems, the parameters $\beta$ are usually denoted as "effects".

* We will consider a framework where the effects are fixed values, but unkown to us;

  * if these parameters are treated as random variables, a slightly different approach is necessary.
  

========================================================

* Suppose that we have a factor (category) $\alpha$ with $i=1,\cdots, I$ different levels.

* Then suppose that there are $j=1,\cdots, J_{i}$ observations per level $i$.

* The model for the $j$-th observation of level $i$ is given a model of the form,

  $$\begin{align}
  y_{ij} = \mu + \alpha_i + \epsilon_{ij}
  \end{align}$$
  
* In the above, we have an underconstrained problem that won't have any unique solution.

  * For example, if $\mu^\ast = \mu+1$ and $\alpha_i^\ast = \alpha_i -1$, then the above equation also holds for $\mu^\ast$ and the $\alpha_i^\ast$;
  
  * Indeed, for any constant shift as above, this will hold.
  
* Some kind of constraint must be imposed to make this equation solvable uniquely...

========================================================

* Common techniques for constraining the model equation

  $$\begin{align}
  y_{ij} = \mu + \alpha_i + \epsilon_{ij}
  \end{align}$$
  
  include:
  
  <ol>
    <li> Removing the $\mu$ term and use $I$ different dummy variables to estimate $\alpha_i$ for $i=1,\cdots,I$.  Although this is feasible, it doesn't extend well to models with multiple factors. </li>
    <li> Setting $\alpha_1 = 0$, then $\mu$ corresponds to the expected mean response for the first level and $\alpha_i$ will be the difference from the base-line level as before.</li>
    <li> Setting $\sum_{i=1}^I \alpha_i = 0$, then $\mu$ represents the mean response over all levels, while each $\alpha_i$ represents the deviation of each level from the overall mean.  This corresponds to sum coding.</li>
  </ol>
  
* At the end, the choice of the constrain above defines a coding/ contrast matrix and thus the interpretation of the parameters.

  * However, no choice of coding will change the fitted values or the standard errors.
  
* Next time, we will enter into the analysis of such a model.