========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080

========================================================
<h1> Gauss Markov Theorem</h1>
* Under the conditions:
 <ol>
    <li>There is a linear relationship between the response variables and the explanatory variables
    $$\Large{\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}}$$</li>
    <li> The error is mean zero 
    $$ \Large{\mathbb{E} [ \boldsymbol{\epsilon} ] = 0 }$$</li>
    <li> The error/ variation in the data is uncorrelated and of equal variance
    $$ \Large{ cov (\boldsymbol{\epsilon} ) = \mathbb{E} [ \boldsymbol{\epsilon} \boldsymbol{\epsilon}^\mathrm{T} ] = \sigma^2 \mathbf{I}}$$</li>
    </ol>
* The least squares solution is the unique, minimum variance, linear, unbiased estimator.
* If, moreover, 
$$\Large{\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2 \mathbf{I})}$$
then the least squares solution is the maximum likelihood estimator.
* The above conditions say that least squares is "optimal" in a sense.

========================================================

<h2> How good is optimal? </h2>

* We will derive some measures of how "good" is the fit of the estimator to our data.  

* Although this may be the "best" estimator for the response variables in terms of the explanatory variables, the best may not be very good.

* In the following we define ${R^2}$ as the:
 <ul>
  <li> "coefficient of determination"; or </li>
  <li> "percentage of variance explained" </li>
</ul>

$$\begin{align} \Large{R^2\triangleq }& \Large{1 - \frac{\sum_{i=1}^n \left(\hat{\mathbf{Y}}_i - \mathbf{Y}_i\right)^2}{\sum_{i=1}^n \left(\mathbf{Y}_i - \overline{\mathbf{Y}}\right)^2 } }\\
 &\Large{=1 - \frac{ \boldsymbol{\epsilon}^\mathrm{T} \boldsymbol{\epsilon} }{\text{Sum of squares of "anomalies"} }}
\end{align}$$


========================================================

<h2> What does $R^2$ mean? </h2>

* The values for $R^2$ will lie in
$$\Large{0 \leq 1 - \frac{\sum_{i=1}^n \left(\hat{\mathbf{Y}}_i - \mathbf{Y}_i\right)^2}{\sum_{i=1}^n \left(\mathbf{Y}_i - \overline{\mathbf{Y}}\right)^2 } } \leq 1$$
where $0$ is a bad fit and $1$ is a good fit.
* Exercise: Under what condition does $R^2 = 1$?

* Answer: the numerator is zero, so the predicted value matches the observed value at each point.

* Exercise: Under what condition does $R^2 = 0$?

* Answer: The numerator equals the denominator, so the deviation of the prediction is the same as the deviation of the data from the mean.

* $R^2=0$ means that the prediction is effectively as accurate as the null hypothesis.

========================================================

<h2> A visual representation </h2>
<div style="float:left; width:40%">
<img style="width:100%", src="./estimation.png"/>
</div>

<div style="float:right; width=40%">
<h2> Graphically we can understand $R^2$</h2>
<ul>
  <li>The solid arrow represents the variance of<br> the data about the data mean.</li>
  <li> The dashed arrow represents the variance of<br> the data about the least squares prediction.</li>
  <li> $R^2$ is defined by one minus the ratio of<br> these two variances.</li>
</ul>
</div>

========================================================

<h1> Note: </h1>

* Our definition for $R^2$ is the same one used in the language R, so we emphasize this.

* However, this definition assumes that there is an intercept term for the model.

* If there is <b>no intercept term</b>, i.e., $\beta_0 = 0$, then 

$$\begin{align}
\Large{R^2} & \large{= cor^2(\hat{\mathbf{Y}}, \mathbf{Y})} \\
 & =\Large{ \frac{ \sum_{i=1}^n \left(\hat{\mathbf{Y}}_i - \overline{\mathbf{Y}}\right)^2 }{\sum_{i=1}^n \left(\mathbf{Y}_i - \overline{\mathbf{Y}}\right)^2} }
\end{align}$$

* The value of $R^2$ should be computed from this definition if the model is fitted without intercept.

========================================================
<h2> How well does $R^2$ work? </h2>

* It's really subjective...

* For physics and engineering applications, data will be produced in tightly controlled experiments.

* Measurement noise will typically be low, and there are strong correlation and causality relationships in these settings.

* In that case, we expect $R^2$ to be close to 1 to say the predicted values model the observations well.

* In the social sciences, there is much more variability, typically causal relationships are not well understood and correlations are weaker.

* In this case, we will typically expect a "good fit" to have a much lower $R^2$ score.

========================================================

<div style="float:left; width:50%">
<img style="width:100%", src="./R_2.png"/>
</div>
<div style="float:right; width:50%">
<h2> We look at several simulated examples of $R^2\approx.65$</h2>
<ul>
  <li> <b>Upper left:</b> the plot is well-behaved for R2 -- there is a clear trend with some variation.</li>
  <li> <b> Upper right: </b> the residual variation is smaller than the first plot but the variation in the observations is also smaller, so the $R^2$ score is about the same.</li>
  <li> <b> Lower left: </b> the fit looks strong except for a couple of outliers. </li>
  <li> <b> Lower right: </b> the relationship is quadratic.</li>
</ul>
</div>

========================================================

<h2> Another representation of uncertainty</h2>

* We can also consider the unbiased estimate of the residuals,
$$
\Large{\hat{\sigma} = \sqrt{\frac{RSS}{n-p}}}
$$

* The value of $\hat{\sigma}$ is in the same variables of the response, so it has a transparent (physical) interpretation.

* This requires some understanding of the response variables and what should be considered as "high deviation" in these variables.

* $R^2$ is a simple, unitless measure, but lacks the interpretation power.

========================================================

<h1> Identifiability </h1>

* Recall the normal equations:
$$\begin{align}
\Large{\mathbf{X}^\mathrm{T}\mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^\mathrm{T} \mathbf{Y}}
\end{align}$$
for $\mathbf{X} \in \mathbb{R}^n$.

* <b> Exercise:</b> When does the matrix $\mathbf{X}^\mathrm{T} \mathbf{X}$ have no inverse?

* <b> Answer:</b> This occurs when the columns of $\mathbf{X}$ are linearly dependent.  In this case there are infinitely many solutions for $\hat{\beta}$.

* This means that $\beta$ is at least in part, "unidentifiable".

========================================================

<h2> Q: What does "unidentifiability" mean in practice? </h2>

* Typically, this will occur by an oversight in the data collection or in our modeling, e.g.:

<ol>
  <li> A personâ€™s weight could be measured both in pounds and kilos and both variables are
entered into the model. One variable is just a multiple of the other.
  </li>
  <li> For each individual we record the number of years of pre-university education,
the number of years of university education, and also the total number of years
of education and put all three variables into the model. There is an exact linear
relation among the variables.
  </li>
  <li> We have more variables than cases, that is, $p > n$.  In this case, the model is sometimes called supersaturated. Such models are considered in large-scale screening experiments used in product design and manufacture and in bioinformatics where there are more genes than individuals tested, but there is no hope of uniquely estimating all the parameters in such a model. Different approaches are necessary.
</li>
</ol>

========================================================

<h2> Overfitting </h2>

* Situations where $p=n$, and $\mathbf{X}$ has rank exactly $n$, it is in fact possible to compute an inverse exactly.

* <b> Exercise: </b> The orthogonal projection operator is defined as,
$$\begin{align}
\mathbf{H} \triangleq \mathbf{X} \left( \mathbf{X}^\mathrm{T} \mathbf{X}\right)^{-1} \mathbf{X}^\mathrm{T},
\end{align}$$

* What does the orthogonal projection equal when $\mathbf{X}^{-1}$ exists?

* What does $\hat{\mathbf{Y}}$ equal when $\mathbf{X}^{-1}$ exists?

* Why is this a problem?

* Overfitted with no degrees of freedom to estimate standard error.

========================================================

<h2> A computational example -- linear dependence </h2>

* We will add a new column to the Gala data that is a linear combination of the existing columns
```{r}
library(faraway)
data(gala, package="faraway")
gala$Adiff <- gala$Area -gala$Adjacent
str(gala)
```

========================================================


* Now if we try to fit the model based on these variables

```{r}
lmod <- lm(Species ~ Area+Elevation+Nearest+Scruz+Adjacent +Adiff, gala)
sumary(lmod)
```

========================================================

* When there is actually linear dependence between the variables, it is possible to rectify this by methods of data compression, e.g. singular value decomposition, among other techniques.

* What is more problematic is when the columns are <b>very close to being dependent</b>, and it isn't clear if this is due to noise.

```{R}
set.seed(123)
Adiffe <- gala$Adiff+0.001*(runif(30)-0.5)
lmod <- lm(Species ~ Area+Elevation+Nearest+Scruz +Adjacent+Adiffe, gala)
sumary(lmod)
```

* Here it is possible to fit a model, but the standard error is extremely large.