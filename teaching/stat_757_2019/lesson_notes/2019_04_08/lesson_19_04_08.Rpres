========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080
  
========================================================

<h1>Review from last time</h1>

* When there is non-zero correlation of the error (or non-constant variance), in certain cases we can successfully reduce this back to our normal regression framework with generalized least squares.

* Particularly, let's assume that the error takes a particular (but more general) form,

  $$\begin{align}
  cov\left(\boldsymbol{\epsilon}\right) = \sigma^2 \boldsymbol{\Sigma}
  \end{align}$$
  
  for a <em>known structure matrix</em> for the error $\boldsymbol{\Sigma}$ but perhaps an <em>unkown scale coefficient</em> $\sigma^2$.  
  
* This describes a sittuation in which we have knowledge of the correlation and relative variance between the errors, but we don't know the absolute scale of the variation.

========================================================


*  We will write $\boldsymbol{\Sigma} = \mathbf{S} \mathbf{S}^\mathrm{T}$ in a Cholesky decomposition.

* Then, considering our form for the signal

  $$\begin{align}
   \mathbf{Y}& = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}&\\
  \Leftrightarrow  \mathbf{S}^{-1} \mathbf{Y}& =  \mathbf{S}^{-1}\mathbf{X} \boldsymbol{\beta} + \mathbf{S}^{-1}\boldsymbol{\epsilon}\\
  \Leftrightarrow \mathbf{Y}' &=  \mathbf{X}' \boldsymbol{\beta} + \boldsymbol{\epsilon}'
  \end{align}$$


  
* Supposing that we can compute the variables,

  $$\begin{align}
  \mathbf{X}' &\triangleq \mathbf{S}^{-1} \mathbf{X} \\
  \mathbf{Y}' &\triangleq \mathbf{S}^{-1} \mathbf{Y}
  \end{align}$$
  
  the regression of $\mathbf{Y}'$ in terms of $\mathbf{X}'$ satisfies our usual Gauss-Markov assumptions, for error $\boldsymbol{\epsilon}'$ such that $cov\left(\boldsymbol{\epsilon}\right) = \sigma^2 \mathbf{I}$.
  

* Because the assumptions of the Gauss-Markov theorem hold for the transformed variables, we can re-derive the least-squares solution minimizing the associated residual sum of squares error.

========================================================

<h2> Weighted least squares</h2>

* As a special case of generalized least squares, we can consider the case when the errors are uncorrelated, but have unequal variances.

* That is to say, $cov\left(\boldsymbol{\epsilon}\right) = \boldsymbol{\Sigma}$ where,

  $$\boldsymbol{\Sigma} = 
  \begin{pmatrix}
    1/w_1& 0 & \cdots & 0 \\    
    0 & 1/w_2 & \cdots & \vdots \\
    \vdots & \ddots & \ddots & \vdots \\
    0 & 0 & \cdots & 1/w_n
  \end{pmatrix}$$
  for possibly $w_i \neq w_j$, but $\boldsymbol{\Sigma}$ is strictly diagonal.
  
* Here, we will refer to the reciprocals of the individual variances, $w_i$, as the <b>weights</b>.

========================================================



* As a special case of generalized least squares, we can compute the transformation into the variables $\mathbf{X}', \mathbf{Y}', \boldsymbol{\epsilon}'$ as

  $$\begin{align}
  \mathbf{X}' &\triangleq \mathbf{S}^{-1} \mathbf{X} = \mathrm{diag}\left(\sqrt{w_i} \right)\mathbf{X}, \\
  \mathbf{Y}' & \triangleq \mathbf{S}^{-1}  \mathbf{Y} = \mathrm{diag}\left(\sqrt{w_i} \right) \mathbf{Y}, \\
  \boldsymbol{\epsilon}' & \triangleq \mathbf{S}^{-1} \boldsymbol{\epsilon} = \mathrm{diag}\left(\sqrt{w_i} \right)\boldsymbol{\epsilon};
  \end{align}$$
  
  * By direct matrix computation, we see that the <b>$i$-th row</b> of the matrix $\mathbf{X}$ is scaled by the inverse of the standard deviation of the $i$-th observation.

* We note, the column corresponding to the intercept term in the matrix $\mathbf{X}$ will be a vector of the form,
  
  $$\begin{align}
  \begin{pmatrix} \sqrt{w_1}, &\cdots, & \sqrt{w_n}\end{pmatrix}^\mathrm{T},
  \end{align}$$
  instead of the usual vector of ones.

========================================================


* Qualitatively, we can note that 
<ol>
  <li> the observations of high variance 
  
  $$\begin{align}
  \frac{1}{\sqrt{w_i}} \gg 1 
  \end{align}$$
  
  will recieve low weight in the regression $\sqrt{w_i} \ll 1$, while</li> 
  
  <li> the observations of low variance 
  
  $$\begin{align}
  \frac{1}{\sqrt{w_i}} \ll 1 
  \end{align}$$
  
  will recieve high weight in the regression $\sqrt{w_i} \gg 1$</li>
  
</ol>

========================================================

<h2>Testing for lack of fit</h2>

* When our assumptions (e.g., G-M) hold then the residual standard error,

  $$\begin{align}
  \hat{\sigma}^2 \triangleq \frac{\hat{\epsilon}^\mathrm{T} \hat{\epsilon}}{n-p} 
  \end{align}$$

  is an unbiased estimate of the true variance $\sigma^2$ of the error.
  
* However, we have now seen many ways how these assumptions can break down.

  * When the model doesn't have the appropriate structure or is not complex enough to fit the data, we will typically overestimate $\sigma^2$ with the emprical $\hat{\sigma}^2$;
  
  * likewise, if we over parameterize the model, then $\hat{\sigma}^2$ will typically underestimate the true variance $\sigma^2$.
  
* In an ideal sittuation, where we may actually know $\sigma^2$, comparing the two values would lead us to understanding the fit of the model by our empirical estimate versus the known value.

  * One example may be when we have only measurement errors for the observations with known variance in their measurments.

* By the same principle, we may more generally get a good estimate of $\sigma^2$ when we have multiple observations of the response for the same values of the explanatory variables.

========================================================

* We note that this requires an assumption of the <b>independence</b> of the measurements.

  * That is to say, we need to require that we have multiple independent observations corresponding to the same value of the explanatory variable.
  
* For example, we may be measuring blood presssure as the response variable and using age, height, weight, and other measurements as the explanatory varibles in a model.

* It won't suffice to re-measure the same individual to obtain an estimate of $\sigma^2$ as this will only measure the within-subjet variability.

* Rather, we would need to find multiple individuals with the same measurements for the explanatory varibles to estimate the variance $\sigma^2$ of the error around the signal.

* We call these multiple, independent observations of the response, with the identical explanatory variables <b>replicates</b>.

========================================================

* Let $\mathbf{Y}_{ij}$ be the $i$-th observation of the $j$-th group of replicates.

  * This is to say that all $\mathbf{Y}_{ij}$ correspond to the same values of $\mathbf{x}_j$ where $\mathbf{x}_j$ will be a set of measurements of the explanatory variables.
  
  
* We can then estimate $\sigma^2$ independently of the regression model as

  $$\begin{align}
  \frac{SS_{pe}}{df_{pe}} \triangleq& \frac{\sum_{j}\sum_{i}\left(\mathbf{Y}_{ij} - \overline{\mathbf{Y}}_{j}\right)^2}{\sum_j \left( \text{number of replicates of type }j - 1\right)} \\
  &=\frac{\sum_{j}\sum_{i}\left(\mathbf{Y}_{ij} - \overline{\mathbf{Y}}_{j}\right)^2}{n - \text{number of groups}}
  \end{align}$$
  
* The estimate given above is known as the <b>pure error</b>.

* We can formulate a test for lack of fit in terms of the ratio of the two estimates for variance, i.e.,
<ol>
  <li> the estimate by the pure error above;</li>
  <li> and the regression based estimate of $\hat{\sigma}^2$.</li>
  
<b>Q:</b> what test statistic can we use to test if two empirical variances match?

<b>A:</b> this can be done with the F-test.

========================================================

* A convenient way to perform the last F-test is as follows:

 1. We may fit a model in which each value for a replicate group is treated as a factor;
 
 2. by treating these as a categorical factor, our model becomes saturated as there will be one parameter in the model per replicate group;
 
 3. the fitted value for each group of replicates will be given as the mean for this group of replicates;
 
 4. while this doesn't provide a model for explaining the phenomenon, the standard error of this dummy model is equal to the pure error;
 
 5. thus, we can use the F-test in an ANOVA table to compare the regression model standard error with the pure error.

* We will demonstrate this process in an example.

========================================================

<h3> An example of testing for lack of fit</h3>

* We can perform a lack of fit test with the corrosion data in the Faraway package, studying the loss of weight in samples of copper/ nickel alloy, due to corrosion, when placed in sea water. 

```{r}
library("faraway")
head(corrosion)
```

* Each sample has varying ammounts of iron, and the loss is measured in terms of milligrams per day.

* We fit a simple regression model for the rate of loss per day based on the iron content:

```{r}
lmod <- lm(loss ~ Fe, corrosion)
sumary(lmod)
```

========================================================

* We plot the simple regression along with the data points:

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(loss ~ Fe, corrosion,xlab="Iron content",ylab="Weight loss",  cex=3, cex.lab=3, cex.axis=1.5)
abline(coef(lmod))
```

* although the $R^2$ value is strong, and the fit looks "OK" we want to test this intuition more formally.

========================================================

* Here, in the ANOVA table, the null hypothesis is that the standard error estimate and the pure error estimate are empirical variances that approximate the same $\sigma^2$.

* The alternative hypothesis is that these do not estimate the same quantity, i.e., that the regression model lacks fit.

  * In particular, this would say that the form of the model,
    
    $$\begin{align}
    \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
    \end{align}$$
    
    is not an accurate form for the model, as the pure error doesn't match the error given by this formulation. 

```{r}
lmoda <- lm(loss ~ factor(Fe), corrosion)
anova(lmod, lmoda)
```

* Due to the extremely small p-value, we reject the null at $5\%$ significance.

* In particular, the pure error is only 1.4, while the standard error is significantly larger at 3.06.

* In this case, it appears that there is still a structural issue in the model itself, which makes the $R^2$ misleading.

========================================================

* A more general question is of how close of a fit to the data is actually appropriate for the problem at hand.

* By including up to sixth degree polynomial terms in the explanatory variable, we can acheive an almost perfect fit, bu the model is completely unphysical and clearly will lack any inference power.

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
lmodp <- lm(loss ~ Fe+I(Fe^2)+I(Fe^3)+I(Fe^4)+I(Fe^5)+I(Fe^6),corrosion)
plot(loss ~ Fe, data=corrosion,ylim=c(60,130), cex=3, cex.lab=3, cex.axis=1.5)
points(corrosion$Fe,fitted(lmoda),pch=3)
grid <- seq(0,2,len=50)
lines(grid,predict(lmodp, data.frame(Fe=grid)))
```

========================================================

<div style="width:60%; float:left">
<img src="quadratic.png" style="width:100%"/>
</div>
<div style="width:40%; float:left">
<ul>
  <li> In the previous example, including higher order terms increased the $R^2$ value significantly, but became a case of obvious over-fitting. </li>
  <li> In general, we cannot fixate too much on $R^2$ for this reason, but we must be aware of the structure of the data and the model.</li>
  <li> In a different example, to the left, it is natural to use quadratic terms in the explanatory variable, rather than linear;</li>
  <li> a model that is linear in the explanatory variable, versus quadratic, will overestimate $\sigma^2$ via $\hat{\sigma}^2$, but the model that is quadratic in $x$ will be unbiased.</li>
  <li> Starting next time, we will start discussing transformations of the variables and model selection...</li>
</ul>