========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080
  
========================================================

<h1>Model Selection</h1>

* We haven't derived any formal ways yet to systematically select a model from a wide range of variables, other than t-test p-values and ANOVA.

  * The primary issue with this approach is that in doing so much multiple hypothesis testing, we we are surely going to get "false-positives" in some sense.
  
  * Specifically, we will surely overstate the signifcance of some of the variables by going through many testing proceedures.
  
  * Likewise, the relative p-values always depend on the larger model in which we start from initially, so that these values will change and may become inflated when we trim to a smaller model.
  
* Nonetheless, this is usally the first choice for model selection that we usually take, as we start empirically building a model and selecting for variables.

* We will firstly discuss how this would look if done systematically, and some of the issues.

* After this, we will develop a few more sophisticated procedures.

========================================================

<h2> Backwards elimination</h2>

* One systematic way of approaching this is to set a variable for the response and many possible variables for the predictors.

* With some pre-defined $\alpha_\mathrm{crit}$ critical value in mind for a level of signficance, we will make a model summary and use the t-test for the exclude one out hypothesis test.

* In particular, we will choose the variable for which the p-value is the largest (and larger than $\alpha_\mathrm{crit}$), and remove this variable <em>alone</em>.

* The removal of this variable will then necessarily change the significance of the other variables, so we must re-fit the model with the one variable removed and re-compute the p-values.

* Subsequently, we will iterate on the above, removing the next variable that has a p-value greater than $\alpha_\mathrm{crit}$.

* Typically in this case $\alpha_\mathrm{crit}$ might be set higher than $5\%$ in practice, to acheive some balance between the complexity reduction and the model fit.

  * We may indeed even consider setting this as high as $\alpha_\mathrm{crit} =10\%$ for creating a predictive model.

* Here, the p-value $\alpha_\mathrm{crit}$ is commonly called the p-to-remove value.

========================================================

<h2> Forward selection/ step-wise selection</h2>

* Forward selection simply reverses the last process of variable selection, where we will start with no variables in the model whatsoever.

* We will add a variable one at a time, testing the significance of this variable in a single predictor model.

  * after evaluating the p-value of each of the proposed variables, we choose the variable with the smallest p-value below $\alpha_\mathrm{crit}$ to include in the model, and <em>only this variable</em>
  
* Subsequently, we iterate on this process, including another variable to the model until there are no more that, when added, fall below the value of $\alpha_\mathrm{crit}$.

* If we decide to change our mind on a variable somewhere through either forward or backward regression, this is referred to as step-wise regression, and there are several ways to go through a forward/ backward process.

========================================================

<h2> An example of backward elimination</h2>

* We will study this in an example looking at life expectancy as regressed upon by demographic factors in various states of the USA, from the 1970 census.

* With a pre-selected value of $\alpha_\mathrm{crit}=5\%$, we will go through backward elimination:

```{r}
library("faraway")
statedata <- data.frame(state.x77,row.names=state.abb)
lmod <- lm(Life.Exp ~ ., statedata)
sumary(lmod)
```

* As a remark, in the big model we see that the murder rate is strongly significant explanatory variable on life expectancy but, unusually, income (for this model) is not.

* This may be understood in part where other variables tigthly correlated with income (illiteracy, education) might make some information redundant.

========================================================

* On the last summary, Area had the largest p-value greater than $5\%$, so we eliminate this, using the "update" function:

```{r}
lmod <- update(lmod, . ~ . - Area)
sumary(lmod)
```

========================================================


* Subsequently, illiteracy should be removed:

```{r}
lmod <- update(lmod, . ~ . - Illiteracy)
sumary(lmod)
```

========================================================

* Nex, we remove income,

```{r}
lmod <- update(lmod, . ~ . - Income)
sumary(lmod)
```


========================================================

* and finally we remove population:

```{r}
lmod <- update(lmod, . ~ . - Population)
sumary(lmod)
```

* we could possibly include population anyway (as it is really on the line), but the difference in $R^2$ is not huge, so we may prefer the smaller model.

  * Overall, we see that the reduction of the four parameters results in only a slight loss of fit to the data.

========================================================

* It is important to remember that even when we have removed a variable, this doesn't mean that it has no impact on the response -- this is always understood relative to the overall model in questions.

* Respectively, if we fit,

```{r}
sumary(lm(Life.Exp ~ Illiteracy+Murder+Frost, statedata))
```

we see that illiteracy is significant with respect to a different choice of variables.

* Particularly, we cannot reliably conclude that illiteracy is not a variable of interest, only that it is surprising that it would be more important than the other variables in the model chosen by backward selection.

========================================================

<h2> Notes on step-wise procedures</h2>

* The point of the above is to show how (what we do intuitively) can be done somewhat more systematically.

* However, this is extremely "hacky" and has fundamental issues in its approach:

  <ol>
    <li> Doing this totally "by hand", we will typically miss an "optimum" that we might find by a more rigorous method.</li>
    <li> The final p-values can't be treated very literally, due to so much multiple testing.  Particularly, we will inflate the significance as we move along through selection</li>
    <li> Variables that are dropped by this selection process can't be said not to have an important effect, and we need to be careful to specify what was actually excluded in the end.</li>
    <li> Step-wise approaches may be over-agressive in the removal of variables for prediction purposes -- even if a parameter isn't strictly significant, there is a balance to be held with predictions and our ability to make some kind of useful prediction</li>
  </ol>
  
* Genearlly, (as in the final) we should try to make a more systematic selection based upon a criterion based on the purpose of the model.

========================================================

<h2> Criterion methods</h2>

* If we re-framed this somewhat abstractly, and supposed there was a way to measure "how-much-information" is added or lost when selecting certain variables, we might be able to make a more rigorous decision.

* A widely used idea in:
  
  1. physics;
  2. statistics;
  3. computer science;
  4. information theory;
  
  is the idea of <b>entropy</b>.

========================================================

<h3> Entropy </h3>
  
* Entropy was originally developed as a concept in thermodymics, as the idea of "how many microscopic (unobservable) states are consistent with macroscopic (observable) constraints on the system?"

* In the context of of information, we can understand entropy similarly as the measure of possible unkowns (the total uncertainty) relative to what is known.

  * This can be loosely understood as the minimum unknown information to recover an unknown mincrostate given some known macrostate.
  
  * This is commonly used as a measure of the necessary information to recover a lossless, compressed signal in signal processing.
  
* In the context of regression (viewed as a kind of signal processing) we can view this likewise as a measure of how much uncertainty exists within one model versus another.

* One commonly used measure of the difference of entropy in one distribution versus another is the "Kullback-Leibler Divergence".

  * We remark, this is not to be considered a distance as it (among other issues) is <b>not symmetric</b>.
  
  * A more formal <b>distance</b> on probability distributions include Wasserstein Distances (earth-mover distances in image processing).
  
========================================================

<h3> Kullback-Leibler Divergence</h3>

*  Suppose we had some idea of what the correct form of the model for the relationship should be in terms of some function $f$.

* Let us have a family of possible models that would look like $f$ up to some choice of parameter $\theta$.

* We will measure the Kullback-Leibler divergence of $f$ and $g_\theta$ as,

  $$\begin{align}
  I(f,g) \triangleq \int f(x) \log\left(\frac{f(x)}{g(x\vert \theta)}\right) \mathrm{d}x
  \end{align}$$
  
* This measures a kind of relative entropy of $f$ with respect to $g_\theta$, and in particular, the ammount of information lost between using the "true" form of the model $f$ and our choice of model $g_\theta$ (for some choice of $\theta$).

* We are thus interested in as a general criterion of minimizing the information lost from the true model with respect to our choice of a particular approximate form.

  * The quantitity itself is positive for all choices of $g_\theta \neq f$ and zero when $g_\theta = f$.

* The main issue about the above is that we don't actually know $f$, and that this cannot be computed as it is written...

========================================================

* On the other hand, we can use a typical trick and use a maximum-likelihood estimate for $\theta$, denoted $\hat{\theta}$, to obtain a form for the equation as, 

  $$\begin{align}
  \hat{I}(f,g) = \int f(x) \log\left(f(x)\right)\mathrm{d}x - \int f(x) \log \left(g\left(x\vert \hat{\theta}\right) \right)\mathrm{d}x
  \end{align}$$
  
  with respect to the "somewhat-optimal" choice of $\hat{\theta}$.


  * Notice, while $f$ is an unkown model, we integrate out the values of $f$ over all possible values it attains, so the left term becomes a constant that we don't have to worry about in the selection of a model.
  
* Through long derivations we can show, moreover, that

  $$\begin{align}
  \mathbb{E}\left[\hat{I}\left(f,g\right)\right] \approx \log\left(L\left(\hat{\theta}\right)\right) + p + \text{ "some constant"},
  \end{align}$$
  
  where: 
  <ol>
    <li> the expectation can be understood as taking place over all possible random outcomes of the observations, etc;</li>
    <li> $p$ is the number of parameters in the model;</li>
    <li> $\log\left(L\left(\hat{\theta}\right)\right)$ is the log-likelihood of the best-choice of parameters for this model;
    <li> the constant is dependent on the unkown, true model, but doesn't affect our optimization procedure.</li>
  </ol>
  
========================================================

* Due to historical conventions, we define an AIC (Akaike Information Criterion) multiplying by 2 to get

  $$\begin{align}
  AIC =  -2\log\left(L\left(\hat{\theta}\right) \right)+ 2p
  \end{align}$$
  
* For linear regression models, the computation of $-2\log\left(L\left(\hat{\theta}\right)\right)$ boils down to a computational form of the maximium log-likelihood,

  $$\begin{align}
  -2L\left(\hat{\theta}\right) = n \log\left(\frac{RSS}{n}\right) + \text{ "some other constant"},
  \end{align}$$
  
  where:
  
  <ol>
    <li> the constants are the same for the same dataset; and</li>
    <li> the constants are the same for the same unkown, true model.</li>
  </ol>

* Therefore, when comparing models over the same dataset, we are safe to use this as a criterion for a "best" model.

  * For other types of comparision, we need to be more careful, see the book for references.

* The goal then, with this criterion, is to minimize the AIC with respect to two competing factors:
  <ol>
   <li> Improve the fit, reducing the RSS, possibly by including more explanatory variables that add new information (not heavily correlated or collinear);</li>
   <li> but not overfit, being penalized in terms of the number of parameters $p$.</li>
  </ol>

========================================================

<h3> Bayes Information Criterion</h3>

* There are many possible criterion we can consider, of similar flavor to the above, for the model selection.

* The most well-known competitor is the Bayes Information Criterion (BIC) given of the form,

  $$\begin{align}
  \mathrm{BIC} \triangleq 2n \log\left(\frac{RSS}{n}\right) + p \log(n),
  \end{align}$$
  
  where again:
  
  <ol>
    <li>we try to minimize the RSS; and</li>
    <li>try not to overparameterize with the penalty for too many parameters $p$.</li>
  </ol>
  
* The BIC is extremely similar, but it has a stronger penalty term and favors simpler models overall.

* Very loosely speaking, we might favor the AIC for predictive models (to get more accurate predictions at the cost of complexity) or the BIC for explanatory models (to get simplicity for explanation purposes at the cost of accuracy).

  * However, either can be used for either purpose.
  
========================================================

<h3> An example of the AIC</h3>

* We will use the AIC to reselect the life expectancy model by state, using the "leaps" library -- this will exhaustively search through the dataset for the best: 1 predictor model, 2 predictory model, 3 predictor model, and so forth...

```{r}
library("leaps")
sum_life <- summary(regsubsets(Life.Exp~.,data=statedata))
sum_life
```

========================================================

* With three explanatory variables, we find the model that uses the murder rate, highschool graduation and frost variables once again.

* We also see the $R^2$, adjusted $R^2_a$ or Mallow's Cp of each model by calling this as an argument of the summary of the output:

```{r}
sum_life$rsq
sum_life$adjr2
sum_life$cp
```

* As with $R^2$, the $R^2_a$ represents better fit when it is larger, whereas $C_p$ represents better fit when it is smaller.

* In each of these alternative measures, we see that the model with population included is actually favored.

* We will now discuss these two other criteria for model fit...

========================================================

<h3> Adjusted $R^2_a$</h3>

* We remember that the definition for $R^2$ with intercept is given as,

  $$\begin{align}
  1 - \frac{RSS}{TSS}
  \end{align}$$

  where the $TSS$ is the residual sum of squares for the null model, using only the mean of the data for prediction.
  
* If we try to optimize $R^2$ alone, this can lead to overparameterization -- specifically, $R^2$ will only increase with additional parameters.

* However, we can include a penalty like before for the number of parameters in the model:

  $$\begin{align}
  R^2_a &\triangleq \frac{RSS/ \left(n-p\right))}{TSS/\left(n-1\right)} \\
  &  = 1 - \frac{\hat{\sigma}^2_\text{model}}{\hat{\sigma}^2_\text{null model}}
  \end{align}$$
  
* Thus, this accounts for the the fit of the model, but penalized for overfiting and compensating only when the new parameter adds meaningful information.

========================================================

<h3> Mallows $C_p$</h3>

* As an alternative criterion, we can consider "how well does the model predict?" in the mean square sense.

* Specifically, suppose we compare the predicted value from our model $\hat{\mathbf{Y}}_i$ with the expected value of the response given the explanatory variables $\mathbb{E}\left[\mathbf{Y}_i \vert \mathbf{X}_i\right]$;

* then, we can theoreticaly compute the mean square difference of these quantities over all possible random outcomes (normalized by the variance) as,

  $$\begin{align}
  \frac{1}{\sigma^2}\sum_{i}^n \mathbb{E}\left[\hat{\mathbf{Y}} - \mathbb{E}\left[\mathbf{Y}_i \vert \mathbf{X}_i\right]\right]^2
  \end{align}$$

* While we cannot compute this in reality, this can be estimated with a sample-based statistic,

  $$\begin{align}
  C_p = \frac{RSS_p}{\hat{\sigma}^2} + 2p - n
  \end{align}$$
  
* Again, we see the competing terms between the $RSS$ and the number of parameters, favoring a compromise between the two.

========================================================

<h3> Practical computation for large models</h3>

* We note that combinatorially, if we have $q$ potential explanatory variables that we might try to choose whether or not to put into a model, there will be to choices per variable:
  <ol>
  <li>yes we include this;</li>
  <li>no we don't include this.</li>
  </ol>
* Each choice is independent from each other choice, so that the total number of combinations we can consider is $2^q$.

* Suppose there are 20 possible variables we might consider -- this corresponds to:

```{r}
as.integer(2)^20
```
 
 * That is, we will have over one million  models to compare... 
 
* Exhaustive search is strictly infeasible even for a (relatively) small number of possible explanatory variables.

  * In practice, this will depend on computational power limitations, but typically we must compromise.  
  
  * Try using exhaustive search based on your hardware, and if there is no reasonable time to convergence, quit it.

========================================================

* An alternative is to use an incremental approach, mixing the ad-hoc method we considered at the beginning, with a specific criterion for the best choice of model.

* Particularly we can compute, e.g., the AIC for all one parameter models;

* then we can compute the AIC for all two parameter models with the first chosen variable imposed on the model with two parameters.

* This may not produce an optimal model in the same sense as exhaustive search, but is a cheaper alternative that neglects the problems with many multiple hypothesis tests.

* This is done explicitly in R with the "step" function.

* Likewise, this can be done by a method of reduction starting with a large model.
  
  * by default, the step function will use this method, though by choosing parameters, it can be performed by forward, backward or both-ways selection.
  
========================================================

* The output process is too long to see completely on the slide, but the code is presented for refrence.

```{r}
lmod <- lm(Life.Exp ~ ., data=statedata)
step(lmod)
```

========================================================

<h2> A summary of model selection</h2>

* Some general (and non-exhaustive) considerations for model selection are the following:

  <ol>
    <li> All of these techniques are sensitive to outliers, and we should look for potential problems with highly influential observations.</li>
    <li> All these techniques are alos sensitive to changes of scale, and the models may need to be re-evaluated after changes of scale and/or removal of outliers of high leverage (which are themselves sensitive to changes of scale).</li>
    <li> Hypothesis testing methods, while simple to implement, greatly suffer when we perform many hypothesis tests.</li>  
    <li>It is generally preferrable to use a criterion approach (or multiple criterion approaches) as a first look and use hypothesis testing to distinguish between nested models of similar performance with ANOVA (when appropriate).</li>
    <li> Approaching the selection problem as above makes the p-values of parameters more useful for interpretation and explanation of the response with the parameters.</li>
    <li>If multiple models are being evaluated, with very similar AIC, $R^2_a$ and $C_p$, consider:
    <ol>
    <li> Do the models have similar qualitative implications for explanation and prediction?</li>
    <li> What are the relative costs and difficulty of measuring the explanatory variables?</li>
    <li> Which model has better performing diagnostic analysis?</li>
    </ol>
    </li>
    <li> If two models have very similar performance by the above, but very different qualitative implications, then it indicates that the data can't well (or definitively) explain the response.</li>
  </ol>
