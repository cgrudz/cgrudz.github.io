========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080
  
========================================================

<h1>Transformation</h1>

<h2>General considerations for the change of scale of the response</h2>

* We supposed that we could identify some change of scale for the response that would make the variation constant in each direction.

* Using some loose approximations, we identified a rationale for the following two transformations:

 * suppose that $var(Y) = var(\boldsymbol{\epsilon}) \propto \left(\mathbb{E}(Y)\right)^2$, then
   $$\begin{align}
   h(y) \approx \int \frac{dy}{y}  = log(y);
   \end{align}$$

 * suppose that $var(Y) = var(\boldsymbol{\epsilon}) \propto \mathbb{E}(Y)$, then
  $$\begin{align}
   h(y) \approx \int \frac{dy}{\sqrt{y}}  = y^{1/2}.
   \end{align}$$

* Note: neither of the above transformations make sense when $Y$ has non-positive values, due to invertibility issues.

========================================================

<h2> Box-Cox Transformation</h2>

* We may also search for a transformation by forming an appropriate "optimal" transformation problem with respect to some general form.

* The Box-Cox transformation is one particular way to perform this, which is as well only for positive data.

* We will suppose that there is some good form for the response that will solve issues around:
  1. nonconstant variances of errors; and/or
  2. non-Gaussian errors;
  
 if we can find an appropriate parameter to do so.
 
* Our hypothesis function will be of the form,

  $$\begin{align}
  h_\lambda (y)& = 
  \begin{cases}
  \Large{\frac{y^\lambda -1}{\lambda} }& \text{if }\lambda \neq 0 \\
  \log(y) & \text{if } \lambda = 0
  \end{cases}
  \end{align}$$
  
  * the above transformation allows for both the possibility of a log transform or a square-root transform, ableit one with a transformation of location and slight re-scaling.
  
  * This is designed so that the function $h_\lambda$ converges to $\log$ when $\lambda \rightarrow 0$.

========================================================

* For any fixed value of $y$ greater than zero, the function $h_\lambda$ is also continuous in the parameter value $\lambda$.

* In particular, we can try to maximize the likelihood of the known data (observations of $y$) given all choices of $\lambda$;

* The log-likelihood function is given as,

  $$\begin{align}
  L(\lambda)& = -\frac{n}{2} \log\left(\frac{RSS_\lambda}{ n} \right) + \left( \lambda -1\right) \sum_{i=1}^n \log(y_i)
  \end{align}$$
 
* Because log is a monotonic function, maximizing the likelihood function is the same as maximizing the log-likelihood function --- we typically just use the log form for computational reasons. 
  
* The above function can be understood in several different competing parts:
  <ol>
    <li> The $RSS_\lambda$ is the residual sum of squares for the model when the response is given by $h_\lambda(y)$ (the transformed variables). </li>
    <ul>
      <li> For small values of $RSS_\lambda$ normalized by $n$, the log will be negative, and therefor the term will become positive.</li>
    </ul>
    <li> The terms $(\lambda - 1)\sum_{i}^n log(y_i)$ will favor small $\lambda \leq 1$ when many of the observations are small and positive, while these will favor $\lambda\geq 1$ when many of the observations are large.</li>
  </ol>

* Using R, we can compute the maximizing value $\hat{\lambda}$ to decide on the best change of variables that is possible within this family of transformations.


========================================================

* We note however, that to interpret, we will typically reduce this to $y^{[\lambda]}$ for $\lambda\neq 0$ where $[\lambda]$ will be a value rounded to something close by and sensible to interpret.

* Particularly, in order to interpret this transformation, we should consider a confidence interval for $\lambda$.

* For an optimally chosen $\hat{\lambda}$, the $100(1-\alpha)\%$ confidence interval is given as

  $$\begin{align}
  \left\{ \lambda :L(\lambda)  > L\left(\hat{\lambda}\right) - \frac{1}{2}{\chi_1^2}^{\left(1-\alpha\right)} \right\}
  \end{align}$$
 
 where 
 
   * $L$ is the same log-likelihood function as before;
   * ${\chi_1^2}^{(1-\alpha)}$ is the Chi-squared distribution critical value corresponding to the $\alpha$ critical value.
   
* The confidence interval thus gives some sense of what values are plausible to round to in the exponent of the response variable.
   
========================================================

<h3>An example of the Box-Cox transformation</h3>

* In R, the fitting of the appropriate value of $\hat{\lambda}$ can be performed simply using the "boxcox" function from the "MASS" library.

* This is demonstrated on the savings data -- the horizontal axis will be the value of the parameter $\lambda$ while the vertical axis will be the log-likelihood:

```{r fig.width=24, fig.height=4}
library("MASS")
library("faraway")
par(cex=3, mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
boxcox(lm(sr ~ pop15+pop75+dpi+ddpi,savings), plotit=T, lambda=seq(0.5,1.5,by=0.1))
```

* In the last transformation, the confidence interval was wide, containing values both greater and less than one, indicating that we cannot reject the null hypothesis, $y \mapsto y$.

  * Particularly, these have very different interpretations for the exponent, and we should not make any power transformation in this case.
  

========================================================

*  We can try a transformation instead on the Galapagos data,

```{r fig.width=24, fig.height=8}
lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz +Adjacent,gala)
par(cex=3, mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
boxcox(lmod, lambda=seq(-0.25,0.75,by=0.05),plotit=T)
```

* In this case, we exclude the null hypothesis, and we can see that there is a reasonable choice of a cubic root transformation for the number of species.

  * A square root transformation is also just barely within our confidence interval, making this also a plausible choice.

========================================================

<h3> General considerations about Box-Cox transformations</h3>

1. Box-Cox transformations are sensitive to outliers, and high values of $\hat{\lambda}$ should be suspect.

2. If some $y_i$ <0, we can make the values positive by translating the data by a small constant --- this should be applied to all values.  However, this is bit "hacky" and reduces the interpretability.

3. If the overall spread of the data,

  $$\begin{align}
  \frac{\text{max}_i y_i}{\text{min}_i y_i}  
  \end{align}$$
  
  is not too large, transformation by powers has little effect on the overall regression.  For small enough range, polynomials can be well approximated by linear scales and therefore we don't expect much change if the relative scall is small.
  
4. It is debatable if $\lambda$ should be considered a parameter in the number of degrees of freedom (and therefore in the measure of overfitting).  In general, we should only apply a transformation of scale when it is deemed absolutely necessary.

========================================================

* A similar family of transformations can be defined by a shifted log, i.e.,

  $$\begin{align}
  g_\alpha(y) = \log\left(y + \alpha\right)
  \end{align}$$

  * This again should be used for positive data but may be used in a "hacky" way with non-positive data.
  
* We will consider using this transformation on observations of the burning time of tobacco leaves, in the dataset "leafburn"

```{r}
head(leafburn)
```

* Here the burn time is the response, while the leaves have various chemical charateristics that we want to regress on the response with.

* We will use the "logtrans" function also in the "MASS" library to perform a similar maximium likelihood optimization for the choice of $\alpha$ versus the data.

========================================================
  
```{r fig.width=24, fig.height=8}  
lmod <- lm(burntime ~ nitrogen+chlorine+potassium, leafburn)
par(cex=3, mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
logtrans(lmod, plotit=TRUE, alpha=seq(-min(leafburn$burntime) +0.001,0,by=0.01))
```

* The optimization procedure indicates that a log transformation of the response by itself isn't reasonable, as evidenced by excluding zero.

* However, it suggests that a negative shift by the parameter will make a log transformation reasonable.

* Physically, this may be interpreted as a lag-period before the burn can begin the log-scale combustion period.

========================================================

<h3> Additional transformations</h3>

* In special cases when we are looking at a response variable that is a percentage, we are already looking at a particular scale $y\in [0,1]$.

* In these cases, we may also consider using a logit transformation of the response,

  $$\begin{align}
  \log\left(\frac{y}{1 - y}\right)
  \end{align}$$
  
  * this has the effect of moving the range of the response between $-\infty$ and $\infty$, which may help the issues of non-constant variances and non-Gaussianity.
  
* Fisher's z-transformation may also be worth considering,

  $$\begin{align}
  \frac{1}{2} \log\left(\frac{1+y}{1-y}\right),
  \end{align}$$
  which has the range $[0, \infty]$.
  
* In both cases above, we must take care to interpret the response (again) by transforming it back into the normal percentage values.

========================================================

<h2> Polynomial regression</h2>

* One way to increase the flexibility of our models is to include polynomial terms of the explanatory variables in the model.

* In the case of a simple regression,

  $$\begin{align}
  y = \beta_0 + \beta_1 x + \epsilon,
  \end{align}$$
  
* We can increase the flexibility of the model by extending this relationship to

  $$\begin{align}
  y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_d x^d + \epsilon.
  \end{align}$$
  
* We note that when doing so, this is qualitatively <b>different</b> than simply writing the model as,

  $$\begin{align}
  y = \beta_0 + \beta_1 x^2 + \epsilon
  \end{align}$$
  
  which is equivalent to a simple change of scale for the explanatory variable (and thus still a fundamentally linear model).
  
* However, this is a simple and classical way to address nonlinear structure in the relationship that we cannot address otherwise.

========================================================
  
* Because the degree of the polynomial $d$ could be of arbitrary size, we should try to make the selection systematically.
  
* Firstly, we should not have a degree of polynomial even close to the number of observations --- this will generally lead to severe overfitting.
  
* Typically, we will then choose the degree by evaluating the number of parameters one at a time by their significance.

* If we start with a linear model ($d=1$) we can add higher order terms one at a time, stopping at the last $d$ for which the parameter $\beta_d$ is still significant.

* If we start with a model of degree $d\geq2$, with some known nonlinear structure in mind, we can remove higher order terms, starting with the highest, until all parameters are significant.

* We will show this process with the savings data...

========================================================

```{r}
sumary(lm(sr ~ ddpi,savings))
```

* As a simple regression, we see that the "ddpi" is significant as a linear term for the savings rate.

```{r}
sumary(lm(sr ~  ddpi+I(ddpi^2),savings))
```

* Similarly for quadratic terms....

========================================================

```{r}
summary(lm(sr ~ ddpi+I(ddpi^2)+I(ddpi^3),savings))
```

* However, the cubic term is not significant, so we only go as far as quadratic structure.

* Note that starting from quartic and moving downward through cubic would have given the same result.

========================================================

* In general, fitting this repeatedly "by hand" is difficult, and numerically unstable.

* A special class of polynomials that are "orthogonal" in a functional analysis sense can be used to perform this polynomial regression analysis at once, and in a stable way.

* Particularly, the R function "poly" will compute the orthogonal polynomials to a specified degree, and this can be used for an effcient analysis,

```{r}
lmod <- lm(sr ~ poly(ddpi,4),savings)
sumary(lmod)
```

* From the above summary, we see that the parameters are only significant to ordr $d=2$, though we note that the regression in the above is in terms of different polynomials that the $x,x^2,x^3$ etc... as before.

* The parameters for the usual form would have to be re-fit according to the method we used previously.

========================================================

* The methods of polynomial regression will extend to multinomials in different variables, using the "polym" function as follows:

```{r}
lmod <- lm(sr ~ polym(pop15,ddpi,degree=2),savings)
sumary(lmod)
```

* Here, the powers in the two variables are listed in the format $a.b$ such that the significance of the parameter for the term $x_1^a x_2^b$, denoted $\beta_{ab}$, can be evaluated.

* In this case, we can see that the higher order terms (combined degree two or greater) lack significance and that we'd be better off to simplify the model into a linear regression on the two different variables.