========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080

========================================================
<h2> From last time</h2>

* We wanted to determine if there is a linear relationship that we can model between easy-to-measure quantities, such as age, weight, height and various circumfrence measurements, and percentage body fat.  
  
  * Body fat is complicated to measure directly so, if the model is good, <b>accurate predictions</b> would be extremely useful.

```{r}
library("faraway")
lmod <- lm(brozek ~ age + weight + height + neck + chest + abdom +
hip + thigh + knee + ankle + biceps + forearm + wrist, data=fat)
head(fat)
```

========================================================

* We made a prediction given from the median of the explanatory variables 

```{r}
x <- model.matrix(lmod)
(x0 <- apply(x,2,median))
predict(lmod,new=data.frame(t(x0)))
```


* The prediction of $\approx 17.5\%$ body fat for the median measurements can be considered in these two, qualitatively different ways.

* If we are to construct $95\%$ confidence intervals, we need to decide whether we predict:
  <ol>
     <li> a future observation of the body fat of a man with the median characteristics; or </li>
     <li> or the mean body fat of all men who each have the median measurements.
  </ol>
  
========================================================

* Specifically,  
  
```{r}
med_obs <- predict(lmod,new=data.frame(t(x0)),interval="prediction")
med_mean <- predict(lmod,new=data.frame(t(x0)),interval="confidence")
med_obs
med_mean
```

* We recall, the predictions of future observations generally have a much wider range than the prediction of the mean.

  * In this case, despite the tight interval around the mean response, there is a practical difference in the lower and upper bounds of the $95\%$ prediction interval: 
  <ul>
    <li> the person with median characteristics could range from a professional athlete to someone is borderline obese.</li>
  
  <li> The mean response might be useful for average statements about public health, but to predict an individual's body fat, there is almost no use.</li>
  </ul>

========================================================


* We note the relative widths of these intervals:
```{r}
med_obs_width <- med_obs[3] - med_obs[2]
med_obs_width
med_mean_width <- med_mean[3] - med_mean[2]
med_mean_width
```

* Particulary, the width of the $95\%$ prediction interval for an observation with median characteristics is on the order of the prediction itself.

* On the other hand, the width of the $95\%$ confidence interval for the mean response is on the order of $5\%$ of the prediction.

========================================================

* The wide prediction intervals in fact occurs even though the $R^2$ is a relatively strong score:

```{r}
sumary(lmod)
```

========================================================

* Suppose we want to make a prediction given a set of observed explanatory variables that lie far from the previously observed data.

* Consider 

$$\begin{align}
var[\mathbf{X}_0^\mathrm{T} \hat{\boldsymbol{\beta}} ]
& =\mathbf{X}_0^\mathrm{T} \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1} \mathbf{X}_0\sigma^2
\end{align}$$

* <b>Q:</b> given the above calculation of the variance, do we expect the variance of the prediciton to grow or shrink as we move away from the previous observations?

* Predicting the response variables given values $\mathbf{X}_0$ for the explanatory variables in the signal <em>that lie beyond the previously observed quantities</em> is known as <b>extrapolation</b>.

* Intuitively, we should expect greater uncertainty in our predictions the less the situation looks like our previous experience.

* This is indeed reflected in the calculation of the variance of the prediction.

========================================================

* We see this concretely reflected in the $95\%$ prediction interval and confidence interval of the mean response.

* Suppose we take the $95$ percentile of the observed explanatory variables and compute these intervals.

* We will compute the increase in the width of these intervals, versus the width at the median:

```{r}
x1 <- apply(x,2,function(x) quantile(x,0.95))
nine_five_obs <- predict(lmod, new=data.frame(t(x1)), interval="prediction")
nine_five_mean <- predict(lmod, new=data.frame(t(x1)), interval="confidence")
(nine_five_obs[3] - nine_five_obs[2])/ med_obs_width
(nine_five_mean[3] - nine_five_mean[2])/ med_mean_width
```

* Here we see only a slight increase in the width of the $95\%$ prediction interval;

  * However, we see a dramatic increase in the width of the $95\%$ confidence interval for the mean, indicating that the uncertainty at these extreme values is dominated by the uncertainty in the mean response itself.

* Unfortunately, for the  above example, we have substantial uncertainty of the form that the model should take. 

* While we are able to quantify the uncertainty of the parameters, e.g., with hypothesis testing and confidence intervals, we haven't dealt with the uncertainty of the model when we are unsure if a linear model is appropriate in the first place. To be continued...

========================================================

<h2> Autoregression</h2>

* In many data sets, we will be interested in how a value changes in time, often as a funciton of the previous values for the response itself.

* Examples include:

  * stock prices, where we might try to infer whether price will go up or down based on the history of the price over several time point measurements;
  
  * the spread of disease, where we might try to infer whether the disease will be transmitted through a population at an increasing or decreasing rate, depending on the number of cases observed in time;

  * aftershocks of earthquakes, where we might try model further tremors based on the frequency or intensity of the first and subsequent tremors.
  
* The principle for using such a model is that there is strong <b>correlation in time</b> between the observations of the response.

* In the following, we will be interested in modeling the future response as a linear model in terms of the previously observed responses.

========================================================

<h2>Autoregression example: airline passengers</h2>

<div style="float:left; width:50%">
<img style="width:100%", src="passengers_plot.png"/>
</div>

<div style="float:left; width:50%">
<ul>
  <li> To the left, we see the monthly number of airline passengers in the UK between years 1949 and 1961.</li>
  <li> There apears to be an increasing trend in the data, with seasonal variation</li>
  <li> <b>Q:</b> do you think we can model the trend in the response variable as a linear model of time?</li>
  <li> <b>A:</b> this is feasible, but we should note the growth rate is nonlinear...</li>
</ul>
</div>

========================================================

<div style="float:left; width:50%">
<img style="width:100%", src="linear_passengers.png"/>
</div>

<div style="float:left; width:50%">
<ul>
  <li> If we first try to model the trend naively in the form
  $$\begin{align}
  Y_{passengers} = \beta_0 + \beta_1 X_{time}
  \end{align}$$
  we obtain only a crude fit.
  </li>
</ul>
</div>

========================================================

<div style="float:left; width:50%">
<img style="width:100%", src="exp_passengers.png"/>
</div>

<div style="float:left; width:50%">
<ul>
  <li> If we model the number of passengers in log scale as a function of time
  $$\begin{align}
  log(Y_{passengers}) = \beta_0 + \beta_1 X_{time}
  \end{align}$$
  we obtain a fit back in linear scale (same as the plot) as,
  $$\begin{align}
  Y_{passengers} = e^{\beta_0 + \beta_1 X_{time}}
  \end{align}$$
  </li>
  <li> This is pictured left.</li>
  <li> However, while this tracks the trend well, we lose all the variation around the trend.</li>
  <li> We will construct an autoregressive model in order to pick up the variations in time, as they correlate between different time points.</li>
</ul>
</div>

========================================================

* By intuition, or expert prior knowledge, we might figure that the number of passengers in a given month would strongly depend on:
 
  * the previous month's number of passengers, giving some indication of the trend in time; and
  
  * the seasonal variation, giving the local oscillation.
  
* In this case, our best model for the seasonal variation might be the seasonal variation we noticed one year prior.

* However, if we wish to obtain the variation seasonally, we need the difference between months.

* This above reasoning suggests a model might look like the following:

$$\begin{align}
y_t = \beta_0 + \beta_1 y_{t-1} + \beta_{12}y_{t-12} + \beta_{13} y_{t-13} + \epsilon_t
\end{align}$$

* Here, $y_{t}$ refers to the response (number of passengers) at time $t$.

* Therefore, $y_{t-12}$ represents the response one year in the past;

* $y_{t-13}$ represents the response one year and a month in the past;

* and $\epsilon_t$ represents the random variation at the current time.

* In general, we denote the $y_{t-i}$ for $i>0$ the <b>lagged variables</b>.

========================================================

* In R, we will use the "embed" function to create a matrix of the lagged variables.

* For an example, let the vector $x$ represent the time series from time 1 to time 10
```{r}
x = c(1:10)
print(x)
```

* We use the "dimension" parameter in the function to describe how many columns will exist in each row of the dataframe.

```{r}
embed(x, dimension=3)
```

========================================================

* In the following, we use the same to create 13 lagged variables in the past for each row;

* we rename the column names of the new dataframe correspondingly;

* and we create a linear model in terms of the lagged variables as demonstrated before:

```{r}
lagdf <- embed(log(airpass$pass),14)
colnames(lagdf) <- c("y",paste0("lag",1:13))
lagdf <- data.frame(lagdf)
armod <- lm(y ~ lag1 + lag12 + lag13, data.frame(lagdf))
sumary(armod)
```

* An explicit time variable isn't necessary here because the $y_{t-1}$ plays the role in describing the trend.

* Notice the alternating sign for the coefficients in the $y_{t-13}$ and $y_{t-12}$ lag variables, giving the response precisely as the weighted difference between these two months.

* The $R^2$ is extremely good and each variable is strongly significant... let's see the result.


========================================================

<div style="float:left; width:90%">
<img style="width:100%", src="auto_reg_air.png"/>
</div>

========================================================

* Suppose we wish to predict future values.

* The last observation in the time series is given as:

```{r}
lagdf[nrow(lagdf),]
```

where each row entry corresponds to the current (final) time step or up to 13 lagged variables.

* We can perform a $95\%$ prediction interval, thus with a new vector of data corresponding to the values above:

```{r}
predict(armod, data.frame(lag1=6.0684, lag12=6.0331, lag13=6.0039),
interval="prediction")
```

* By producing new predicted values, we can <em>recursively</em> compute future values, using predictions of new variables to give future lag variables.