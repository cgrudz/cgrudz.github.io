========================================================
author: 
date: 
autosize: true
incremental: true
width: 1920
height: 1080

========================================================

<h1> Notes on prediction and explanation</h1>

<ul>
  <li><b>Q:</b> What can go wrong with predictions?</li>
  <li><b>A:</b> Lots of things... here's a non-exhaustive list:</li>
<ol>
  <li> We create a bad model -- for instance, a linear model was not appropriate to begin with.</li>
  <li> We try to extrapolate too far <b>quantitatively</b> from our observed data --- this is a practical problem in assessing the risk from low exposure to substances which are dangerous in high quantities</li>
  <ul>
    <li>e.g., second-hand tobacco smoke, asbestos, radon, or currently as a high-profile court case with weed-killer.</li>
  </ul>
  <li> We try to extrapolate <b>qualitatively</b> on a population different from the one that was observed</li>
    <ul>
      <li>e.g., there is a long history of biased 20th century medicine studies that only ran measurements and trials on men but extrapolated results to women.
    </ul>
  </li>
  <li>Overconfidence and overfitting.  We try to model the data too closely and get unrealistically small residual standard errors.</li>
  <li> We make predictions based on limited data that doesn't know about extreme events.</li>
  <ul>
    <li>e.g., financial crises --- the Black–Scholes model is among the most commonly used in pricing derivatives in financial models; however, this is based on certain assumptions including normality of the data.  While this may be true for short time series, non-normal extreme events occur and have dramatic effects if not accounted for (like 2008).</li>
  </ul>
</ol>
</ul>

========================================================

<h2>Explanation is sometimes harder philosophically...</h2>

* Sometimes explanation means causation by physical principles,

  * E.g., if we fit a linear model for a physical application, a paramater $\beta$ might be physical constant;
  
  * in particular, it could represent the ammount of energy lost due to friction in the moving parts of machine.
  
* However, sometimes explanation is just a description of the (conditional/ stastical) relationships between the variables. 

* Causal conclusions require stronger assumptions than those used for our predictive models we have discussed.

* Sometimes, we only wish to understand correlations, i.e., variables that (co)-vary together or asymmetrically...

========================================================

* ... but sometimes we can't read much into correlation:

<div style="float:left; width:75%">
<img style="width:100%", src="correlation_spider_bee.jpeg"/>
<b>Correlation: $80.5\%$ -- Courtesy of <a href="http://tylervigen.com/spurious-correlations" target="blank">Tyler Vigen</a>, Creative Commons License</b>
</div>

========================================================

<h3> A simple meaning of explanation</h3>

* When we produce a linear model, we can consider the coefficients $\hat{\boldsymbol{\beta}}_i$ to provide an "explanation" of the effect of an explanatory variable on the response, e.g.,

```{r}
library("faraway")
lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)
sumary(lmod)
```

* in this case, if we found a new island with elevation 100m higher (and with all other variables held constant) would appear to produce $\approx 32$ addtional species on a given island.


========================================================

* However, if we try to perform the same analysis with respect to a different choice of model,

```{r}
sumary(lm(Species ~ Elevation, gala))
```

* we would have an explained response of about 20 additional species with a change of 100m of additional elevation.

* In general, as discussed above, the change in response variable in terms of an explanatory variable <b>needs to be qualified with respect to all variables in the model</b>.
  
  * "a unit increase in $\mathbf{X}_1$ with the other <b>(named)</b> predictors held constant will produce a change
of $\hat{\boldsymbol{\beta}}_1$ in the response $\mathbf{Y}$". 

* For the simple regression model with one variable explaning the response, the change in elevation is associated with a change in the other variables (not included in the model). 

* This explains why the two predictions are different.

* The regression coefficients thus give some explanation power, but it is <b>conditional</b> and lacks the notion of causality.

========================================================

<h2>Causality</h2>

* One common notion of causality is the following:
 
 * "<em>the causal effect of an action is the difference between the outcomes where the action was or was not taken</em>"
   
* In this understanding of causality we will typically consider prototypically an experiment with

  * a participant indexed by the letter $i$;
  
  * a measured quantity $\mathbf{Y}_i^T$ that may or may depend on the effect of some treatement;
  
  * $\mathbf{Y}_i^0$ will correspond to a control, where treatement <b>is not applied</b>;
  
  * $\mathbf{Y}_i^1$ will correspond to a test subject, where the treatment <b>is applied</b>; and
  
  * an effect $\delta_i \triangleq \mathbf{Y}_i^1 - \mathbf{Y}_i^0$.
  
* Usually, however, there will be no identical test subjects, and we can only observe one of $\mathbf{Y}^T_i$ for $T \in \{0, 1\}$.

* The outcome that we <em>cannot see</em> is called the <b>counterfactual</b>.

* In designed experiments, we may actually be able to control $T$, but for example with the Galapagos data, we cannot feasibly test a change in the elevation.

* Because many applications in data science and statistics will not (and often cannot) involve the best experimental design, we will focus our discussion on these cases.

========================================================

<h3> A concrete example</h3>

* In the 2008 Democratic primary election in New Hampshire, Hillary Clinton won despite the pre-election polls suggesting that Barack Obama would win.

<ul>
  <li>Two voting technologies were used in New Hampshire:</li>
  <ol>
    <li>. paper ballots counted by hand;</li>
    <li> optically scanned ballots, machine processed.</li>
  </ol>
<li>Controversially, on paper ballots Obama had more votes, whereas Clinton only won the primary in districts that used the machine counting.</li>
</ul>
```{r}
colSums(newhamp[newhamp$votesys =='D',2:3])
colSums(newhamp[newhamp$votesys =='H',2:3])
```

<li> Because the voting method shouldn't have any significant effect onthe outcome of the election, the integrity of the election was in question.</li>

========================================================

* In the following, we will try to model this to determine if the outcome would be surprising if it was due to random chance.

* <b>Note:</b>

  * Strictly speaking, this should be modelled by a <b>binomial distribution</b>; nonetheless, the normal can be a good approximation for the binomial given a large enough sample and probabilities not close to zero or one.
  
  * A binomial variance is $np(1− p)$ for probability of success $p$, and sample size $n$. 
  
  * For both the votes for Clinton and Obama, the probability and sample sizes differ such that our assumption of constant variance is violated; with generalized least squares we can take this into account, but it doesn't make an appreciable difference in this case so we save this for later.
  
* We will start by fitting  a model with only the voting system as the explanatory variable...

========================================================

```{r}
newhamp$trt <- ifelse(newhamp$votesys == 'H',1,0)
lmodu <- lm(pObama ~ trt, newhamp)
sumary(lmodu)
```

* The model takes the form:

  $$\begin{align}
  y_i = \beta_0 + \beta_1T_i + \epsilon_i
  \end{align}$$
  
* where:

  * the response variable is the percentage of votes for Obama;
  
  * the base percentage of votes is represented by $\beta_0 \approx 35\%$;

  * when the voting system is by hand counting, $T_i = 1$ and the effect of a hand counting <em>in this model</em> is to add approximately $4\%$ to the vote total.
  
* We note that with the extremely small p-value for $\hat{\boldsymbol{\beta}}_1$, the effect of hand counting is statistically significant.

* In this case, we may conclude that Obama did receive significantly more votes in wards using hand counting, but this doesn't imply that <em>the voting method itself is the explanation</em>.

========================================================

* We will suppose that this is the correct model, up to an additional variable we have left out of our analysis:

  $$\begin{align}
  y_i = \beta_0 + \beta^\ast_1T_i + \beta^\ast_2 Z_i + \epsilon_i 
  \end{align}$$
  
  * Where, as an ansatz, we take
  $$\begin{align}
  Z_i \triangleq \gamma_0 + \gamma_1 T_i + \epsilon^{\ast}_i 
  \end{align}$$
  
* We call the above $Z_i$ the <b>confounding variable</b>.

* By substition, we recover a new model

  $$\begin{align}
  y_i = \left(\beta_0 + \beta_2^\ast \gamma_0 \right) + \left(\beta^\ast_1 + \beta_2^\ast \gamma_1\right)T_i + \epsilon_i + \beta_2^\ast \epsilon^\ast_i
  \end{align}$$

* There are two scenarios where the effect of digital voting or hand counted voting agree between the original model and the model with the confounding variable:
<ol>
  <li> $\beta_2^\ast = 0$, so that $Z_i$ has no effect on the response; or </li>
  <li> $\gamma_1=0$, such that the effect of the treatement (hand or digital voting) has no effect on $Z_i$, the confounding variable.</li>
</ol>

* In any other case, our original model excluding $Z$ will be biased in its estimation of the effect of digital versus hand counting.

* In a designed experiement, $\gamma_1=0$ by randomization of the populations to which the control or the treatment is applied, but in this case we cannot assume this.

========================================================

* We wish thus to determine if there is a possible effect of a confounding variable in the model.

* In particular, we will consider the proportion of votes given to Howard Dean, the Democratic presidential candidate in 2004 as an explanatory variable on the response:

```{r}
lmodz <- lm(pObama ~ trt + Dean , newhamp)
sumary(lmodz)
```

* Things we thus note immediately are:
<ol>
  <li> the percentage of votes that went to Howard Dean in 2004 has a positive effect in this model, that dramatically counter-balances the effect of the counting method;</li>
  <li> the counting method in this model is no loner statistically significant, where the p-value is on the order of $54\%$.  That is, under the null hypothesis, leaving out the counting method as an explanatory variable, we would expect to see such a t-test value $\approx 54\%$ of the time.  The effect of the counting  method in this model (with respect to the Dean variable) is quite feasibly due to random chance;</li>
  <li> on the other hand, dropping the Dean variable out of the model seems like a bad choice; under the null hypothesis, leaving out Dean, the p-value for the t-test is on the order zero.
</ol>

========================================================

* Recall the ansatz,
 
  $$\begin{align}
  Z_i \triangleq \gamma_0 + \gamma_1 T_i + \epsilon^{\ast}_i 
  \end{align}$$

* If we try to fit such a model, we see that

```{r}
sumary(lm(Dean ~ trt, newhamp))
```
we have a nonzero $\gamma_1$ with effectively $100\%$ confidence.

* It appears that there is an active confounding variable, but we will need additional analysis to make conclusions...

========================================================

* Suppose we wish to follow the analogy of the clinical trial again, 

  * we might have individuals who differ in gender, age, health conditions, and other factors that can affect the outcome of the treatment;

  * we want to have individuals with equal (or extremely similar) traits balanced between the control and treatment populations to ensure that the traits of either group don't bias the results;.

  * Knowing that there is some effect of a confounding variable in this analysis, we should specifically try to balance the effect of the confounding variable in both the control and treatment populations
  
  * matching certain participants based on their relationship to the confouding variable, we will randomly select individuals from the matched groups as either a control or a treatment case.

* For the primary data, we will perform a similar procedure to match voting districts based on similar <em>proportions of votes going to Dean in 2004</em>, in order to evaluate the effect of the treatment, i.e., machine or hand counting of votes.

* Because this is observed data, we can't randomly assign the treament or control to these matched populations, but we will try to emulate the process.

========================================================

* Dean is a continuous variable (precentage of district that voted for Dean) so we set a threshold for matches.

* The GenMatch is a (stochastic) selection algorithm based on genetic principles --- by random initialization and selection, it goes through iterations to find a "best fit".

```{r results='hide'}
library("Matching")
set.seed(123)
mm <- GenMatch(newhamp$trt, newhamp$Dean, ties=FALSE, caliper=0.05, pop.size=1000)
```

* We print some of the matches:

```{r}
head(mm$matches[,1:2])
```

* and note the first case of the matches, where the proportion of votes is quite similar:

```{r}
newhamp[c(4,218),c('Dean','pObama','trt')]
```

========================================================

<div style="float:left; width:50%">
<img style="width:100%", src="matching_plot.png"/>
</div>

<div style="float:left; width:50%">
<ul>
  <li> In the left plot, we show the matched pairs --- hand voting wards are shown as
triangles and digital voting wards as circles. 

<li>Matched pair are linked by a line. In the right plot, we show the matched pair differences.</li>

<li>We will continue discussing the explanation of the observed difference between digital and hand counting districts, controlling for confounding variables, on Monday.</li>
</ul>
</div>

