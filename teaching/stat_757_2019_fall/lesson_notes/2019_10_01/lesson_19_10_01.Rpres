<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

Multiple regression and inference
========================================================
date: 10/01/2019
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style='color:black'>Instructions:</h2>
<p style='color:black'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:
  * Multiple regression in R
  * Identifiability
  * Orthogonality
  * A review of hypothesis testing
  
========================================================
<h2>Multiple regression in R</h2>

* We have now covered the basic theory that will structure the course.</li>

* We will now look at a concrete example of multiple regression from the Faraway package "gala" data set.

```{r}
library(faraway)
str(gala)
```

* <b>Q:</b> how many observations $n$ do we have in this data set, and what is the largest $p$ number of parameters can we put into a model?

* <b>A:</b> there are $n=30$ observations, and the largest number is $p=6 +1$ parameters (including an intercept and minus one for the response).

========================================================
<h3>Moving into multiple regression -- an example</h3>


* In this case, there are 30 islands in the Galápagos with 7 variables -- each observation corresponds to a particular island:

```{r}
row.names(gala)
```

========================================================

<h3>Multiple regression in R -- an example</h3>

* Recall, We can see some basic summary statistics with "summary"

```{r}
summary(gala)
```

========================================================

<h3>Multiple regression in R -- an example</h3>

```{r}
head(gala)
```

* The meaning of different variables can be found in the help file for the "gala" data using the command ```?gala```.

* Species -- number of species found on the island
* Endemics -- number of endemic species
* Area -- the area of the island in km^2
* Elevation -- the highest elevation of the island
* Nearest -- the distance from the nearest island
* Scruz --- the distance from Santa Cruz island
* Adjacent -- the area of the adjacent island

========================================================

<h3>Creating a linear model in R</h3>

* We will create a linear model now from scratch with the funciton ```lm()```

```{r}
lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent,data=gala)
```

* Notice the code syntax:
  - The first argument is the "Species", indicating that we have the number of species as our response variable $\mathbf{Y}$
  - The ~ here separates the response variables from the explanatory variables.
  - Finally, the keyword argument "data" indicates to the function to draw these variables all from the variable "gala"
  - This is designed to resemble our  mathematical form for the model, where we may write,
  
  $$\begin{align}
  \mathbf{Y}_{\mathrm{species}} = \beta_0 + \beta_{\mathrm{Area}}\mathbf{X}_{\mathrm{Area}} + 
  \beta_{\mathrm{Elevation}}\mathbf{X}_{\mathrm{Elevation}} + \beta_{\mathrm{Nearest}}\mathbf{X}_{\mathrm{Nearest}} + \beta_{\mathrm{Scruz}}\mathbf{X}_{\mathrm{Scruz}} + \beta_{\mathrm{Adjacent}}\mathbf{X}_{\mathrm{Adjacent}}
  \end{align}$$

* Like in the simple regression model, we can see exactly how (for all other variables held fixed) the increase in, e.g., $\mathbf{X}_\mathrm{Area}$ by 1 $\mathrm{km}^2$  corresponds to an increase in the predicted mean function by $\beta_{\mathrm{Area}}$;

  - Thus the $\beta$ values in multiple regression correspond again to the slope due to increase in a particular predictor.

========================================================

<h3>Examining a linear model in R</h3>

  
* What does our linear model look like?

```{r}
summary(lmod)
```

* The model summary contains a great deal of information, which we will become familiar with.

========================================================

<h3>Examining a linear model in R</h3>

* For instance, we can obtain quantities of interest from the "lmod"
```{r}
names(lmod)
beta_hat <- lmod$coefficients
print(beta_hat)
```

========================================================

<h3>Examining a linear model in R</h3>

* However, linear models can be far from perfect

```{r}
predicted_Y <- lmod$fitted.values
print(predicted_Y)
```

* <b>Q:</b> what appears to be at issue with our model?

* <b>A:</b> in this example, the predicted values are un-physical, including negative numbers of species.

* In practice, a great deal of the work will be identifying when the assupmtions of the Gauss-Markov theorem fail to hold;

  * when they fail to hold in practice, we will look for transformations of variables and alternative models for which the new system is better conditioned.


========================================================
  
<h2> Identifiability </h2>

<ul>
  <li> As within the context of the Gauss-Markov theorem, we will be interested in when functions are estimable or not.</li>
  <li> Consider the normal equations in matrix form,
$$\begin{align}
\mathbf{X}^\mathrm{T}\mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^\mathrm{T} \mathbf{Y}
\end{align}$$
for $\mathbf{X} \in \mathbb{R}^{n \times p}$. </li>
  <li>If $n>p$ and the columns of $\mathbf{X}$ are linearly independent, then $\left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)$ will have an inverse that is defined.</li>
  <li>Particularly, this allows us to construct the estimate of the true parameters by least squares,
  $$\hat{\boldsymbol{\beta}} = \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}\mathbf{Y}.$$</li>
</ul>  

========================================================
  
<h3> Identifiability </h3>

<ul>
  <li>In the case when $\left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)$ is not invertible, this has an important consequence for the normal equation,
  $$\begin{align}
  \mathbf{X}^\mathrm{T}\mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^\mathrm{T} \mathbf{Y}.
  \end{align}$$</li>
  <li>In this case there are infinitely many solutions for $\hat{\boldsymbol{\beta}}$, as the system of equations is under-constrained.</li>
  <li> Importantly, this means that $\hat{\boldsymbol{\beta}}$ is at least in part, "unidentifiable", as there is no unique solution.</li>
  <li> Typically, this will occur in practice by an oversight in the data collection or in our modeling, e.g.:</li>
  <ul>
    <li> A person’s weight could be measured both in pounds and kilos and both variables are
entered into the model. One variable is just a multiple of the other. </li>
    <li> For each individual in a data set we have:</li>
    <ol>
      <li> $X_1=$ the number of years of pre-university education,</li>
      <li> $X_2=$ the number of years of university education</li>
      <li> $X_3=$ the total number of years of education and put all three variables into the model. </li>
    </ol>
    <li> In this case, there is an exact linear relation among the variables $X_1 +X_2 =X_3$.</li>
     <li> We will also have a dependence relationship when have more variables than cases, that is, the model is supersaturated $p > n$.</li>  
  <li> Such models are considered in large-scale screening experiments used in product design and manufacture and in bioinformatics;</li>
  <li> for example, there are more genes than individuals tested, but there is no hope of uniquely estimating all the parameters in such a model. 
</li>
</ul>



========================================================

<h3> A computational example -- linear dependence </h3>

* We will add a new column to the Gala data that is a linear combination of the existing columns
```{r}
library(faraway)
gala$Adiff <- gala$Area -gala$Adjacent
str(gala)
```

========================================================

<h3> A computational example -- linear dependence </h3>


* Now if we try to fit the model based on these variables

```{r}
lmod <- lm(Species ~ Area+Elevation+Nearest+Scruz+Adjacent +Adiff, gala)
sumary(lmod)
```

* The default behavior in the R language is to neglect any variables in the design matrix that are clearly linearly dependent to others.

* Here, the ```Adiff``` variable, which is included last, has been neglected from the model.

========================================================

<h3> A computational example -- linear dependence </h3>

* When there is actually linear dependence between the variables, it is possible to rectify this by methods of data compression, e.g. singular value decomposition, among other techniques.

* What is more problematic is when the columns are <b>very close to being dependent</b>, and it isn't clear if this is due to noise.

```{R}
set.seed(123)
Adiffe <- gala$Adiff+0.001*(runif(30)-0.5)
lmod <- lm(Species ~ Area+Elevation+Nearest+Scruz +Adjacent+Adiffe, gala)
sumary(lmod)
```

* Here it is possible to fit a model, but the standard error is extremely large.

========================================================

<h2> Orthogonality </h2>

* We have discussed largely about the case where there is some dependence (i.e., correlation) among explanatory variables.
  
  * For super-saturated models (with $p> n$), there must be linear dependence among explanatory variables and there is no way to recover "best" values for $\beta$ as there are infinitely many solutions.
  
  * For cases of linear dependence between variables (with $p< n$), there is redundancy between these variables that adds no useful information.
  
  * Linear dependence, or <em>almost-dependence</em> (i.e., highly correlated variables), when $p < n$ leads to singularities -- particularly, there may be issues with trying to solve by least squares or the uncertainty of predictions will be extremely large.

* <b>Q:</b> Qualitatively, what occurs when the explanatory variables are totally <b>statistically indpedendent</b>?

* <b>A:</b> each variable contributes unique information into the model which cannot be inferred from the values of the other variables.

* <b>Q:</b> How does this aid our analysis?

* <b>A:</b> in one sense, we maximize the value of each estimated parameter $\beta_i$ as it corresponds to a statistically independent variable's contribution to the response.

* This is closely related to the idea of orthogonality, when the space spanned by each variable is perpendicular to each other.

========================================================

<h3>A review of orthogonality</h3>

* Orthogonality can be loosely read as "perpendicular".

* Recall an equivalent description of the vector inner product,

$$\begin{align}
\mathbf{a} \cdot \mathbf{b}  & = \parallel \mathbf{a} \parallel \parallel \mathbf{b} \parallel cos(\theta) \\
&= \text{"length of } \mathbf{a}\text{"} \times \text{"length of } \mathbf{b}\text{"} \times cos( \text{"the angle between"})
\end{align}$$

* <b>Q:</b> If there are 90 degrees between the two vectors $\mathbf{a}$ and $\mathbf{b}$, then what does the inner product $\mathbf{a} \cdot \mathbf{b}$ equal?

* <b>A:</b> $cos(90^\circ)=0$, such that the inner product must vanish -- therefore, orthogonal vectors have a zero inner product.

========================================================

<h3>Orthogonal decomposition </h3>

* This idea extends to matrices, when the columns of two matrices are orthogonal.

* In particular, if $\mathbf{A}$ is an orthogonal matrix then $\mathbf{A}^\mathrm{T}\mathbf{A} = \mathbf{I}$.
 * The transpose product here represents the dot product of each column of $\mathbf{A}$ with each other column.
 * Each column is orthogonal to each of the others, so off diagonal entries are zero.
 * We call the <em>matrix</em> $\mathbf{A}$ orthogonal when each of the columns are also normalized to have length one, thus giving the ones on the diagonal.
 
======================================================== 

<h3>Orthogonal decomposition </h3>

* Suppose we can decompose the explanatory variables into two groups $\mathbf{X}_1$ and $\mathbf{X}_2$ which are orthogonal to each other, 

  * we will not assume, however, that each column is of norm 1:

 $$\begin{align}
\mathbf{X} &\triangleq 
\begin{pmatrix}
\mathbf{X}_1 \vert \mathbf{X}_2
\end{pmatrix}
\end{align}$$

* Notice that (regardless of orthogonality) we have the equality:

$$\begin{align}
\mathbf{X}\beta &= 
\begin{pmatrix}
\mathbf{X}_1 \vert \mathbf{X}_2
\end{pmatrix}
\begin{pmatrix}
\beta_1  \\ \beta_2
\end{pmatrix} \\
&= \mathbf{X}_1 \beta_1 + \mathbf{X}_2 \beta_2
\end{align}$$

 so that we split the explanatory variables and parameters into two groups via the definition of the matrix product.

======================================================== 
<h3>Orthogonal decomposition </h3>

* <b>Exercise (5 minutes):</b> Assume that $\mathbf{X} \triangleq \begin{pmatrix}\mathbf{X}_1 \vert \mathbf{X}_2 \end{pmatrix}$ 
and $\mathbf{X}_1$ and $\mathbf{X}_2$ are orthogonal to each other.  Discuss with a partner, what does $\mathbf{X}^\mathrm{T} \mathbf{X}$ equal block-wise? Then use this fact to derive what the orthogonal projection operator 
$$\mathbf{H}= \mathbf{X}\left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}$$
equals block-wise.

* <b>Solution:</b> we find that the matrix product yields,

 $$\begin{align}
 \mathbf{X}^\mathrm{T} \mathbf{X} &=
 \begin{pmatrix}
 \mathbf{X}^\mathrm{T}_1 \mathbf{X}_1 & \mathbf{0} \\
 \mathbf{0} & \mathbf{X}^\mathrm{T}_2 \mathbf{X}_2
 \end{pmatrix}
 \end{align}$$
 due to the orthogonality of the two columns.

* From this fact, we can now write the product

 $$\begin{align}
  \mathbf{H} &\triangleq \mathbf{X}\left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T} \\
 &=\begin{pmatrix}
 \mathbf{X}_1\left(\mathbf{X}^\mathrm{T}_1 \mathbf{X}_1\right)^{-1}\mathbf{X}_1^\mathrm{T} & \mathbf{0} \\
 \mathbf{0} &  \mathbf{X}_2\left(\mathbf{X}^\mathrm{T}_2 \mathbf{X}_2\right)^{-1}\mathbf{X}_2^\mathrm{T}
 \end{pmatrix}
 \end{align}$$


======================================================== 

<h3>Orthogonal decomposition </h3>

* In the previous exercise, we see that the prediction of the fittted values decomposes entirely along the two sub-sets of variables.

* Likewise, we will find that $\hat{\boldsymbol{\beta}}$ decomposes into two sets of parameters $\hat{\boldsymbol{\beta}}_1$ and $\hat{\boldsymbol{\beta}}_2$.

* <b> Exercise (2 minutes): </b> recall that the estimated covariance of the parameter values is given as,
 $$\begin{align}
  cov\left(\hat{\boldsymbol{\beta}}\right) &=\sigma^2 \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1} .
\end{align}$$
What does orthogonality of the columns of $\mathbf{X}$ imply about the covariance of the parameters $\hat{\boldsymbol{\beta}}$?

* <b>Answer:</b> in particular, if the columns of $\mathbf{X}$ are orthogonal to each other, we find that the estimated parameters $\hat{\boldsymbol{\beta}}$ are uncorrelated.

* Qualitatively, we should understand that the value of one parameter estimate $\hat{\beta}_i$ does not inform the value of the estimate $\hat{\beta}_j$ for $i\neq j$.


======================================================== 

<h3>Orthogonality and correlation </h3>

* We want to relate the notion of orthogonality more directly to the correlation between variables in the statistical sense.

* Let $\overline{\mathbf{X}}^{(i)}$ be the mean of column $i$ of the design matrix, i.e.,
  $$\overline{\mathbf{X}}^{(i)} \triangleq \frac{1}{n} \sum_{k=1}^n X_{k,i},$$
  summing over the matrix entries $X_{k,i}$ along the rows $k=1,\cdots,n$.

* We will then define the $(k,i)$-th anomaly as
  $$
  a_{(k,i)} = X_{k,i} - \overline{\mathbf{X}}^{(i)},
  $$
  such that the matrix $\mathbf{A}$ is defined column-wise as
  $$\begin{align}
  \mathbf{A}^{(i)} \triangleq \mathbf{X}^{(i)} -\overline{\mathbf{X}}^{(i)} \mathbb{1}
  \end{align}$$
  where $\mathbb{1}$ is the vector of ones,
  $$\begin{align}
  \mathbb{1} \triangleq \begin{pmatrix} 1 & 1 & \cdots & 1 \end{pmatrix}^\mathrm{T}.
  \end{align}$$

======================================================== 

<h3>Orthogonality and correlation </h3>

* If we consider as is standard that $\mathbf{X}$ are deterministic constants, we may then discuss the sample-based correlation of the predictors.

* The sample-based correlation coefficient of the variables $X_i$ and $X_j$ can be written,
  $$\begin{align}
  cor(X_i,X_j)\triangleq \frac{\left(\mathbf{A}^{(i)}\right)^\mathrm{T} \mathbf{A}^{(j)}}{\sqrt{\left[\left(\mathbf{A}^{(i)}\right)^\mathrm{T}\mathbf{A}^{(i)}\right] \left[\left(\mathbf{A}^{(j)}\right)^\mathrm{T}\mathbf{A}^{(j)}\right]}}.
  \end{align}$$

* <b>Q:</b> if the varaibles $X_i$ and $X_j$ are uncorrelated, what does this say about their anomalies?

* <b>A:</b> they must be orthogonal.


======================================================== 

<h3>Orthogonality and correlation </h3>

* We can consider thus a change of variables for our standard model, in the case our variables are uncorrelated.

* Let us suppose the form of the model is
  
  $$\begin{align}
  Y &= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1} X_{p-1} \\
    &= \beta_0 + \sum_{i=1}^{p-1} \beta_i X_i\\
    &= \beta_0 + \sum_{i=1}^{p-1} \beta_i \left(X_i- \overline{\mathbf{X}}^{(i)} +  \overline{\mathbf{X}}^{(i)}\right)\\
    &= \left[\beta_0 + \sum_{i=1}^{p-1} \beta_i\overline{\mathbf{X}}^{(i)}\right] + \sum_{i=1}^{p-1} \beta_i A_i
  \end{align}$$
  i.e., where we re-write the model in terms of the anomalies as the predictors.
  
* Recall, by our assumptions, $\left[\beta_0 + \sum_{i=1}^{p-1} \beta_i\overline{\mathbf{X}}^{(1)}\right]$ is just a constant which can be re-named as $\tilde{\beta}_0$.

* <b>Q:</b> for the above model, are the parameter estimates for $\tilde{\beta}_0, \beta_1, \cdots \beta_{p-1}$ correlated?

* <b>A:</b> no, by the orthogonality of the anomalies.


======================================================== 

<h3>Orthogonality and correlation </h3>

* We can thus see that having uncorrelated predictors allows us to construct a model (possibly in terms of the anomalies) in which the estimated parameters are also uncorrelated.

* In this case, we can view the parameter estimates loosely as "close-to-independent".

  * However, remember that uncorrelated is only <b>close-to-independent</b> but not equivalent.

* It tells us that we cannot infer information about one parameter from the value of another;

  * therefore, this also tells us that the estimated values for a particular parameter will not change based on the other predictors included in the model.
  
* This is an extremely useful property that typically is only a product of good experimental design -- in situ data from observations often have more complicated correlation structures.

  * Particularly, the estimated value of one parameter will often depend on the other variables included in our model.

======================================================== 

<h3> An example of orthogonal predictors </h3>

* An experiment was made to determine the effects of column temperature, gas/liquid ratio and packing height in reducing the unpleasant odor of a chemical product that was sold for household use.

```{r}
odor
```

* <b>Note: </b> Temperature has been rescaled by the transformation
 $$
\text{temp} = \frac{\text{Farenheit} - 80}{40}
 $$

======================================================== 

<h3> An example of orthogonal predictors </h3>


* We will compute the covariance of the explanatory variables, temp, gas and pack:
```{r} 
cov(odor[,-1])
```

* With the zero values in the non-diagonal elements, we know that the anomalies are orthgonal.


======================================================== 

<h3> An example of orthogonal predictors </h3>


* We will fit a linear model for the response variable odor with the explanatory variables tem, gas and pack.
```{r}
lmod <- lm(odor ~ temp + gas + pack, odor)
summary(lmod,cor=T)
```


======================================================== 

<h3> An example of orthogonal predictors </h3>

* If we repeat fitting the linear model, but leave out the variable pack, we find almost the same result, except for minor changes in the uncertainty and the increased number of degrees of freedom -- this is not guaranteed for all models.
```{r}
lmod <- lm(odor ~ temp + gas, odor)
summary(lmod,cor=T)
```

======================================================== 

<h2> Uncertainty Quantification </h2>

* A quick refresher on the idea of uncertainty by example:

 * Suppose $T$ is a linear unbiased estimator for the speed of light $\theta$. 
 * Like our parameters $\beta$, we will assume that $\theta$ is a deterministic but unkown constant.
 * For sake of example, also suppose that $T$ has standard deviation $\sigma_T$ = 100 km/sec. 
 * Recall Chebyshev’s inequality, 
 
 $$\begin{align}
 P\left(\vert T - \theta \vert > k \sigma_T\right) <  \frac{1}{k^2}
 \end{align}$$
 
 * We find
 
  $$\begin{align}
 P\left(\vert T - \theta \vert < 2 \sigma_T\right) \geq  \frac{3}{4}
 \end{align}$$

 * This tells us that there is a probability of at least 75% that $T$ is within 200 km/sec of the speed of light $\theta$. 
 
 * Equivalently, $\theta \in (T-200, T+200)$ with probability 75%.
 
======================================================== 

<h3> Uncertainty Quantification </h3>
 

* Suppose the estimate $T$ gives us based on some data is $t=299852.4$

* We can say that  $\theta \in (299652.4, 300 052.4)$ with <b>confidence</b> 75%.
   * Note that $\theta$ is an an unkown constant -- it is either in the interval or not and there is nothing random about the above statement.
   * Therefore, we can't say that the probability of $\theta \in (299652.4, 300 052.4)$ is 75%, but we used information to guarantee that our proceedure for estimation will work 75% of the time.
   
* Similarly, we will want to fit confidence intervals, and moreover to use hypothesis testing to determine the significance of our model parameters versus the null hypothesis of random variation and no systematic structure.
 
* Particularly, we will be concerned with dual questions:

  1. Does our confidence interval for a parameter $\beta_i$ contain the value $0$? And
  2. how unlikely would it be for $\beta_i$ to equal zero based on our observations?

========================================================

<h2> Inference</h2>

* Recall hypothesis testing:
 * Suppose we have a set of response variables and explanatory variables.
 * We want to see if the response variables have a systematic relationship with the explantory variables, or if this can be reduced to random variation.
 * Our null hypothesis $H_0$ is that there is no systematic structure, i.e.,
 $$
 \begin{align}
 \mathbf{Y} = \overline{\mathbf{Y}} + \boldsymbol{\epsilon}
 \end{align}
 $$
 
 * Our alternative hypothesis is that there does exist some (unknown) set of values $\boldsymbol{\beta}$ for which
 
 $$
 \begin{align}
 \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
 \end{align}
 $$
 
 * Hypothesis testing is a systematic methodology to determine <b>significance</b> of events.  
 * <b>Note:</b> this does not imply causation -- rather the significance can be thought of as<br><br>
   <b>"how suprised would we be if this was just random variation?"</b>
 
* In addition to the null model of random variation, we will be concerned with whether a particular variable, or combination of variables, is significant in the presence of other variables considered.



========================================================

<h3> Inference</h3>

 
* The significance level we assign to the test is the measure of "how suprising" it would be to find out there was no structure.
 
   * For example, a significance value of $5\%$ indicates that the observed outcomes would only be attributable to random variation in about $\frac{1}{20}$ trials.
   
* When we build a model, we will typically look at a pre-set value for significance of $5\%$, but other values could be chosen.

 * If we find the p-value, the probability of our outcome, to be less than the significance level, this is an indication of a relationship for <b>further study</b>.
 
 * However, due to the <a href="https://projects.fivethirtyeight.com/p-hacking/" target="blank">saturation of p-values</a>, we should be cautious about putting much importance on the p-values.

 * We will see more robust metrics for model selection later in the course, but because of how widely used p-values are in the literature, we need to understand them.


* To use standard methods for hypothesis testing and confidence intervals, <b>we will now assume additionally Gaussianity</b>, $\boldsymbol{\epsilon} \sim N(0, \sigma^2 \mathbf{I})$.


========================================================

<h3>Occam's Razor</h3>

* In principle, we favor solutions to problems that are as simple as possible.
 * Occam's Razor is the philisophical principle that, <br><br>
 <b>"When presented with competing hypotheses to solve a problem, one should select the solution with the fewest assumptions."</b>

* This makes the problem easier to interpret, and our models transparent in their predictions.

* Suppose we have a large model $\boldsymbol{\Omega}$, which abstractly refers to the set of all linear models possible by choices of $\beta_i$, and their respective uncertainties, over certain variables $X_1, X_2, \cdots, X_{p-1}$.

* Let $q < p-1$, and suppose that abstractly $\boldsymbol{\omega}$ represents a "smaller model", as found by a strictly smaller set of explanatory variables, $X_1, X_2, \cdots, X_q$.

* We will say that we favor the model $\boldsymbol{\omega}$ unless $\boldsymbol{\Omega}$ provides appreciably better results.
 
 * For example, we may consdider, if $RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}$ is small, then we favor the former, small model. 
 
 * With an additional scaling factor, we can use this principle directly as a test statistic for the null hypothesis, i.e.,
 
 $$\begin{align}
 \frac{RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}}{RSS_\boldsymbol{\Omega}}
 \end{align}$$
 
========================================================

<h2>Likelihood Ratio test</h2>

* The quantity defined,
 
 $$\begin{align}
 \frac{RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}}{RSS_\boldsymbol{\Omega}}
 \end{align}$$

 is known as a test-statistic, which is actually defined in terms of the ratio of likelihood functions.

* Let's recall our Gaussian likelihood function

 $$\begin{align}
 \mathcal{L} (\boldsymbol{\beta}, \sigma \vert \mathbf{Y} =Y_{1,\cdots,n} )
 \end{align}$$
 representing the likelihood of the parameter vector $\beta$ and the associated uncertainties with respect to the observed outcomes of the response variable.
 
* The following,
 
 $$\begin{align}
 max_{\boldsymbol{\beta}, \sigma \in \boldsymbol{\Omega}} \mathcal{L} (\boldsymbol{\beta}, \sigma \vert\mathbf{Y} =Y_{1,\cdots,n}  )
 \end{align}$$
 
 will represent the <b> maximum likelihood attainable</b> over all choices of $\boldsymbol{\beta}$ and choices of $\sigma$ in the large model space $\Omega$.


========================================================

<h3>Likelihood Ratio test -- continued</h3>

 
* We can imagine intuitively that,

 $$\begin{align}
 \frac{ max_{\boldsymbol{\beta}, \sigma \in \boldsymbol{\Omega}} \mathcal{L} (\boldsymbol{\beta}, \sigma \vert \mathbf{Y} =Y_{1,\cdots,n}  )}{max_{\boldsymbol{\beta}, \sigma \in \boldsymbol{\omega}} \mathcal{L} (\boldsymbol{\beta}, \sigma \vert\mathbf{Y} =Y_{1,\cdots,n}  )}
 \end{align}$$
 is a reasonable measure of whether the model over the large model space (including more parameters) is more likely than the model over the smaller model space (with fewer parameters).

* If the likelihood ratio statistic is sufficiently large, we can say that:
 * "it would be very surprising that the high likelihood of the larger model versus the low likelihood of the small model is just due to random variation."  

* In the above situation, we reject the null hypothesis, i.e., we reject the small model $\boldsymbol{\omega}$.
 
