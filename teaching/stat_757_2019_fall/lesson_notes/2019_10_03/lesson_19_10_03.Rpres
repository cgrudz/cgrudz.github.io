<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

Inference continued
========================================================
date: 10/04/2019
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style='color:black'>Instructions:</h2>
<p style='color:black'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

========================================================

<h2>Outline</h2>

* The following topics will be covered in this lecture:
  * A review of hypothesis testing
  * The F-test and ANOVA
  * The t-test

========================================================

<h2> A review of hypothesis testing</h2>

* The process of hypothesis testing is always defined in terms of a null and an alternative hypothesis.

* In regression, we will denote the hypothesis that there is a systematic, statistical relationship the alternative hypothesis $H_1$.

* The hypothesis that the observed structure can be explained by random variation will be denoted the null hypothesis $H_0$.

* Hypothesis testing thus assumes that the null holds, and finds how surprising it would be to see the observed structure in the case it was just due to random variation.

* We always choose a pre-set value of significance $\alpha$ (typically $5\%$) and determine if the probability of finding a value at least as extreme as our observed results is less than $\alpha$.

  * This probability is known as the p-value.
  
* If the p-value falls below $\alpha$ we reject the null hypothesis in favor of the alternative.

  * On the other hand, if the p-value is greater, then we fail to reject the null model.
  
* Neither indicates causality or the lack thereof, but are good indications of points for further investigation.

  * Examining p-values can also provide corroborating evidence of the explanatory or predictive power of a model when used in conjunction with more robust forms of model selection.

========================================================

<h3>Occam's Razor</h3>

* In principle, we favor solutions to problems that are as simple as possible.
 * Occam's Razor is the philisophical principle that, <br><br>
 <b>"When presented with competing hypotheses to solve a problem, one should select the solution with the fewest assumptions."</b>

* This makes the problem easier to interpret, and our models transparent in their predictions.

* Suppose we have a large model $\boldsymbol{\Omega}$, which abstractly refers to the set of all linear models possible by choices of $\beta_i$, and their respective uncertainties, over certain variables $X_1, X_2, \cdots, X_{p-1}$.

* Let $q < p$, and suppose that abstractly $\boldsymbol{\omega}$ represents a "smaller model", as found by a strictly smaller set of explanatory variables, $X_1, X_2, \cdots, X_{q-1}$.

* We will say that we favor the model $\boldsymbol{\omega}$ unless $\boldsymbol{\Omega}$ provides appreciably better results.
 
 * For example, we may consdider, if $RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}$ is small, then we favor the former, small model. 
 
 * With an additional scaling factor, we can use this principle directly as a test statistic for the null hypothesis, i.e.,
 
 $$\begin{align}
 \frac{RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}}{RSS_\boldsymbol{\Omega}}
 \end{align}$$
 
========================================================

<h2>Likelihood Ratio test</h2>

* The quantity defined,
 
 $$\begin{align}
 \frac{RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}}{RSS_\boldsymbol{\Omega}}
 \end{align}$$

 is known as a test-statistic, which is actually defined in terms of the ratio of likelihood functions.

* Let's recall our Gaussian likelihood function

 $$\begin{align}
 \mathcal{L} (\boldsymbol{\beta}, \sigma \vert \mathbf{Y} =Y_{1,\cdots,n} )
 \end{align}$$
 representing the likelihood of the parameter vector $\beta$ and the associated uncertainties with respect to the observed outcomes of the response variable.
 
* The following,
 
 $$\begin{align}
 max_{\boldsymbol{\beta}, \sigma \in \boldsymbol{\Omega}} \mathcal{L} (\boldsymbol{\beta}, \sigma \vert\mathbf{Y} =Y_{1,\cdots,n}  )
 \end{align}$$
 
 will represent the <b> maximum likelihood attainable</b> over all choices of $\boldsymbol{\beta}$ and choices of $\sigma$ in the large model space $\boldsymbol{\Omega}$.


========================================================

<h3>Likelihood Ratio test -- continued</h3>

 
* We can imagine intuitively that,

 $$\begin{align}
 \frac{ max_{\boldsymbol{\beta}, \sigma \in \boldsymbol{\Omega}} \mathcal{L} (\boldsymbol{\beta}, \sigma \vert \mathbf{Y} =Y_{1,\cdots,n}  )}{max_{\boldsymbol{\beta}, \sigma \in \boldsymbol{\omega}} \mathcal{L} (\boldsymbol{\beta}, \sigma \vert\mathbf{Y} =Y_{1,\cdots,n}  )}
 \end{align}$$
 is a reasonable measure of whether the model over the large model space (including more parameters) is more likely than the model over the smaller model space (with fewer parameters).

* If the likelihood ratio statistic is sufficiently large, we can say that:
 * "it would be very surprising that the high likelihood of the larger model versus the low likelihood of the small model is just due to random variation."  

* In the above situation, we reject the null hypothesis, i.e., we reject the small model $\boldsymbol{\omega}$.
 


========================================================

<h3> F-test for model selection </h3>

* Let us recall, the above intuition was formalized somewhat in our discussion of ANOVA.

  * Particularly, we saw the $RSS$ divided by the number of degrees of freedom as a "mean-square", which has a known expected value.

  * When we explicitly utilize the degrees of freedom for each model, we get a statement we can evaluate based on the theoretical expected values.

* Recall, $\boldsymbol{\omega}$ uses $q < p$ parameters, while $\boldsymbol{\Omega}$ uses $p$.

* We find

 $$\begin{align}
 F &\triangleq \frac{ \left( RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}\right)/ (p-q)}{RSS_\boldsymbol{\Omega}/(n-p)} 
 \end{align}$$
 
 is an $F$ statistic, with $F$ distribution under the null hypothesis.
 
* This is to say, "if the null hypothesis holds (such that the smaller model is favorable), then $F \sim F_{(p-q),(n-p)}$."

* We will thus study how surprising this value is or not, based on the assumption that $F$ is drawn from the $F_{(p-q),(n-p)}$.


========================================================

<h3> F-test for model selection </h3>


 <div style="float:left; width:50%">
<img style="width:100%", src="F-distribution_pdf.png" alt="Diagram of the F distribution"/>
<p style="text-align:center">
Courtesy of IkamusumeFan <a href="https://creativecommons.org/licenses/by-sa/4.0" target="blank">CC BY-SA 4.0</a></p>
</div>

<div style="float:right; width:50%">

<ul>
<li> The hypothesis testing procedure thus follows the following idea:</li>

<ul>
 <li> Let us say (for sake of example) we want to choose a model with $\alpha =5\%$ significance. </li>
 
 <li> We will look at the appropriate $F$ distribution and find the value of 
 $$ F^\alpha_{ (p-q), (n-p) }$$ 
 
 <li> such that the probability of 
 $$F \geq F^\alpha_{(p-q),(n-p)}$$ </li>
 
 
 <li><b> given </b> 
 $$F \sim F_{(p-q),(n-p)}$$ </li>
 
 <li>is equal to
 $$\alpha = 5 \%.$$ </li>

</ul>
</ul>

</div>
<div style="width:100%; float:left">  
<ul>  
  <li> If our observed 
$$\begin{align}
 F &\triangleq \frac{ \left( RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}\right)/ (p-q)}{RSS_\boldsymbol{\Omega}/(n-p)}  \geq F_{(p-q),(n-p)}^{\alpha},
 \end{align}$$
 we find this to be an unlikely outcome under the null hypothes (due to random variation).</li>
</ul>
</div>
 
========================================================

<h2> Geometric interpretation</h2>

 <div style="float:left; width:50%">
<img style="width:100%", src="F_statistic_graph.png" alt="Geometric view of the F-statistic as a difference of subspaces"/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p>
</div>

<div style="float:right; width:50%">

<ul>
  <li> When we have <b>Gaussian errors, $\boldsymbol{\epsilon}\sim N(0, \sigma^2 \mathbf{I})$</b> ,</li>
  <ul>
    <li> if we have a "large model" and a smaller, simpler version of the model,</li>
    <li> where the smaller version of the model is defined as a sub-combination (subspace) of the larger model,</li>
    <li> we can express the difference in the models as $RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}$.</li>
    <li> When this difference is "small" relative to various considerations, we should reject the large model for simplicity.</li>
  </ul>
</ul>
</div>
<div style="float:left; width:100%">
<ul>
   <li> However, when there is an appreciable difference in the results, i.e.,</li>
    <ul>
      <li>with significance in the F-test (we would be very suprised that this result was due to random variation)</li>
    </ul>
  <li> we accept the larger model.</li>
  </ul>
</ul>

</div>

========================================================

<h2> An example</h2>

* Let's consider the null hypothesis that there is no structure whatsover between the response variables and the explanatory variables.

* That is, we suppose the relationship looks like
 $$\begin{align}
  \mathbf{Y} = \overline{\mathbf{Y}} + \boldsymbol{\epsilon}
  \end{align}$$
  
* The null hypothesis is thus, $H_0 : \boldsymbol{\beta} = \boldsymbol{0}$.

* <b>Exercise (2 minutes):</b> discuss with a partner what is $RSS_\boldsymbol{\omega}$ in this case?

* <b>Solution:</b> this is the sum of square diffences of the predicted value (always the mean) versus the observed values, i.e.,

  $$\begin{align}
  \boldsymbol{\epsilon}_\boldsymbol{\omega}^\mathrm{T}   \boldsymbol{\epsilon}_\boldsymbol{\omega} &= \left(\mathbf{Y} - \overline{\mathbf{Y}}  \right)^\mathrm{T}\left(\mathbf{Y} - \overline{\mathbf{Y}}\right) \\  
  &=TSS
  \end{align}$$


========================================================

<h3> An example -- continued</h3>

* Let us consider again the ```gala``` data...

```{r}
library('faraway')
lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent,
gala)
sumary(lmod)
```

* Here we fit the model with explanatory variables describing a response with "lmod"

========================================================

<h3> An example -- continued</h3>

* Secondly, we will fit the null model as variation around a constant value, i.e.,

```{r}
nullmod <- lm(Species ~ 1, gala)
sumary(nullmod)
mean(gala$Species)
```

* where the fixed constant is indeed the empirical mean value of the response.

========================================================

<h3> An example -- continued</h3>


* Explicitly, the F-test can be computed as follows

 * The RSS of each of the models
    ```{r}
    (rss0 <- deviance(nullmod))
    (rss <- deviance(lmod))
```

 * The degrees of freedom of each model  
    ```{r}
    (df0 <- df.residual(nullmod))
    (df <- df.residual(lmod))
```

========================================================

<h3> An example -- continued</h3>


* Then, we compute the fstatistic with the ratio of likelihoods
```{r}
(fstat <- ((rss0-rss)/(df0-df))/(rss/df))
```

* Finally, we determine the probability of this value being drawn from the F distribution in the two parameters ```df0 - df``` and ```df```.
```{r}
1-pf(fstat, df0-df, df)
```

* The function ```pf``` above evaluates the F distribution at the point ```fstat``` with respect to the degrees of freedom parameters. 

* The probability of this value  is approximately zero, on the order of $10^{-7}$.

========================================================

<h3> An example -- continued</h3>


* More compactly, this is computed in an analysis of variance (ANOVA) table

```{r}
anova(nullmod, lmod)
```

* We tentatively reject the null hypothesis and conclude that at $5\%$ significance, <b>at least one subspace of the explantory variables</b> has predictive power.

* This does not say which one, or if a combination of the explanatory variables gives the predictive power.

  * This only says, it is very unlikely that there is no relationship between the space of explanatory variables and the changes in the response.
  

========================================================

<h3> An example -- continued</h3>

* The same result can be observed in the F-statistic of the model summary,

```{r}
summary(lmod)
```


========================================================

<h2>Testing one predictor</h2>

* As a general method, we can always use the F-statistic for <b>nested models</b>.

* Specifically, whenever one model is given by a subspace of another: 
  * $\boldsymbol{\omega}$ consists of models over variables $x_1, \cdots, x_{q-1}$ and corresponds to $q$ parameters (including the intercept);
  * $\boldsymbol{\Omega}$ consists of models over variables $x_1, \cdots , x_{p-1}$ and corresponds to $p$ parameters (including the itercept); 
  * such that $q \text{<} p$.

* Concretely, the null hypothesis must be $H_0 : \boldsymbol{\beta}_i = \boldsymbol{0}$ for each $i=q,\cdots, p-1$.

* The alternative hypothesis is that the larger model holds,
  
  $$H_1: \boldsymbol{\beta} \neq 0$$

* We compute the F statistic as,
   $$\begin{align}
   F &\triangleq \frac{ \left( RSS_\boldsymbol{\omega} - RSS_\boldsymbol{\Omega}\right)/    (p-1)}{RSS_\boldsymbol{\Omega}/(n-p)} .
 \end{align}$$
 

========================================================

<h3>Testing one predictor -- continued</h3>


* Suppose there is one particular variable that we want to determine the significance of for our model.

* Specifically, suppose we have a model,

  $$\begin{align}
  \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},
  \end{align}$$
  with respect to some choice of variables $\mathbf{X}$.
  
* Our alternative hypothesis in this case is,

  $$\begin{align}
  H_1: \boldsymbol{\beta} \neq \boldsymbol{0}.
  \end{align}$$
  
* <b>Q:</b> if we want to determine if $\boldsymbol{\beta}_i$ specifically gives an appreciable difference in this model, what is our null hypothesis?

* <b>A:</b> our null hypothesis takes the form,

  $$\begin{align}
  H_0: \boldsymbol{\beta}_i = 0
  \end{align}$$
  
 

========================================================

<h3>Testing one predictor -- continued</h3>

* We will examine this on the ```gala``` data once again.

* We define the model ```lmods``` without ```area``` as an explanatory variable for the null hypothesis.  Then we compute the ANOVA table with the bigger model.

```{r}
lmods <- lm(Species ~ Elevation + Nearest + Scruz + Adjacent, gala)
anova(lmods, lmod)
```

* The result of the $F$ test is to say,
  
  "With probability 29.63%, we will find a value drawn from the F distribution with this value or greater"

* <b>Q:</b> do we reject or fail to reject the null hypothesis at $5\%$ significance here?  

* <b>A:</b> Here we <b>fail to reject the null hypothesis</b> because it is reasonable that the difference between the large model and the small model could be due to random variation.
 * <b>Note:</b> there may be some statistical relationship, but we haven't detected one that wouldn't be surpising if it was just noise.
 
========================================================

<h3>Student t-distribution</h3>

 <div style="float:left; width:40%">
<img style="width:100%" src="Student_t_pdf.png" alt="Image of student t-distributions varying with the number of degrees of freedom"/>
Courtesy of Skbkekas  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC BY-SA 3.0</a>

</div>

<div style="float:left; width:60%">

<ul>
  <li> The significance of a <b>single variable</b> can also be found with respect to the student t-test.</li>
  <li> Generally, suppose that we have $n$ independent samples of a Gaussian distribution $\left\{Y_i\right\}_{i=1}^n$, with unknown true mean $\mu_Y$ and standard deviation $\sigma$.</li>
  <li> As usual, our sample-based estimate of the mean is given by,

  $$\begin{align}
  \overline{Y} = \frac{1}{n}\sum_{i=1}^n Y_i ;
  \end{align}$$</li>
  <li> and our unbiased, sample-based estimate of the variance is given as,

  $$\begin{align}
  S^2 = \frac{1}{n-1} \sum_{i=1}^n \left(Y_i - \overline{Y}\right)^2.
  \end{align}$$</li>
</ul>
</div>
<div style="width:100%; float:left">
<ul>
  <li> It is a powerful and non-trivial result that,

  $$\begin{align}
  \frac{\overline{Y} - \mu_Y}{S/ \sqrt{n}},
  \end{align}$$
  is distributed according to a student t-distribution, in $n-1$ degrees of freedom.</li>
</ul>
</div>

========================================================

<h3>Student t-distribution</h3>

 <div style="float:left; width:40%">
<img style="width:100%" src="Student_t_pdf.png" alt="Image of student t-distributions varying with the number of degrees of freedom"/>
Courtesy of Skbkekas  <a href="https://creativecommons.org/licenses/by-sa/3.0" target="blank">CC BY-SA 3.0</a>

</div>

<div style="float:left; width:60%">
  <li> The student t-distribution, pictured left, is similar to a Gaussian but with only polynomial decay of the probability in the tails.</li>
  <li>Particularly, for one degree of freedom $\nu = 1$, the density function takes the form,
  $$\begin{align}
  p(x) = \frac{1}{\pi \left( 1 + x^2\right)}.
  \end{align}$$
  <li> In general, the student t-distribution is defined for any number of degrees of freedom $\nu$ as a parameter, where
  $$\begin{align}
  p(x) = \frac{\Gamma(\frac{\nu+1}{2})} {\sqrt{\nu\pi}\,\Gamma(\frac{\nu}{2})} \left(1+\frac{x^2}{\nu} \right)^{-\frac{\nu+1}{2}},
  \end{align}$$
  and $\Gamma$ is special function known as the Gamma function.</li>
</div>
<div style="width:100%; float:left">
<ul>
  <li><b>Exercise (2 mins):</b> notice, the t-distribution is symmetric about zero. 
  <li>Discuss with a partner, if $L>0$ and $P(X\geq L) = \alpha/2$ then what is 
  $P(-L  &lt; X &lt; L)$?
  </li>
</ul>

========================================================

<h3>Student t-distribution</h3>

* <b>Solution: </b> we can consider the two events,
  $$\begin{align}
  A: X\geq L & & B: X \leq -L.
  \end{align}$$
  
* Due to the symmetry of the student t-distribution about zero, we find,

  $$\begin{align}
  P(A)=P(B) = \frac{\alpha}{2}.
  \end{align}$$
  
* Then, notice that,

  $$\begin{align}
  P(A\cup B) &= P(A) + P(B) + P(A\cap B) \\
  &= \frac{\alpha}{2} + \frac{\alpha}{2} + 0
  \end{align}$$
  as the intersection is empty.
  
* Finally, the complement of the event $A\cup B$ is the event that $-L &lt; X &lt; L$,
  such that, $P(-L &lt; X &lt; L) = 1 - \alpha$.
  
========================================================

<h3>Testing one predictor -- continued</h3>


* Suppose again, we have $n$ independent samples of a Gaussian distribution $\left\{Y_i\right\}_{i=1}^n$, with unknown true mean $\mu_Y$ and standard deviation $\sigma$.</li>

* <b> Q:</b> if we want to test the hypothesis that $\mu_Y \neq 0$, what are the null and alternative hypotheses?

* <b>A:</b>
  
  $$\begin{align}
  H_0 &: \mu_Y = 0 \\
  H_1 &: \mu_Y \neq 0
  \end{align}$$

* To test the hypothesis, we assume the null hypothesis and refer to the quantity,
  $$\begin{align}
  t^\ast = \frac{\overline{Y} - \mu_Y}{S/ \sqrt{n}} = \frac{\overline{Y}}{S / \sqrt{n}}
  \end{align}$$
  with respect to the null.
 
* The value $t^\ast \sim t_{n-1}$ so that we can identify the value $t^\alpha$ for which 
  
  $$P(t \geq t^\alpha) = \frac{\alpha}{2}.$$

* By symmetry, we see that

  $$\begin{align}
  P( \vert t \vert &lt \vert t^\alpha\vert) = 1 - \alpha
  \end{align}$$


========================================================

<h3>Testing one predictor -- continued</h3>

* If we want thus to test the hypothesis that a single parameter $\boldsymbol{\beta}_i \neq 0$, we can follow a similar procedure under the Gaussian assumption.

* Recall, the standard error of a given parameter $\hat{\boldsymbol{\beta}}_i$ is given
$$\begin{align}
se(\hat{\boldsymbol{\beta}}_{i-1}) \triangleq \hat{\sigma}\sqrt{(\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}_{ii}}
\end{align}$$
 where $\hat{\sigma}^2 = \frac{RSS}{n-p}$.

* The value
$$\begin{align}
t_i = \frac{\hat{\boldsymbol{\beta}}_i}{se(\hat{\boldsymbol{\beta}}_i)}
\end{align}$$
 has t-distribution in $(n-p)$ degrees of freedom under the null hypothesis $H_0:\boldsymbol{\beta}_i=0$.

* Particularly, we will determine the probability of obtaining a random value $t$ where $\vert t \vert \geq \vert t_i \vert$ with respect to the t-distribution to determine significance. 

  * If the above probability falls below our pre-chosen $\alpha$, or equivalently, 
  $$\vert t_i\vert &gt; \vert t^\alpha \vert,$$ 
  we reject the null that $\beta_i = 0$ with $\alpha$ significance.
  
========================================================

<h2>Model summary</h2>

* We can now interpret the entire summary table, except for adjusted $R^2$:
```{r}
summary(lmod)
```

========================================================

<h3>Model summary -- continued </h3>

* <b>Exercise:</b> State the null and alterantive hypothesis for each t-test and F-test below.  Suppose $\alpha=5\%$.  Where do we reject and where do we fail to reject the null hypothesis?

```{r}
summary(lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala))
```


========================================================


* <b>NOTE:</b> The null hypothesis will have a different interpretation if it is stated with respect to different alternative hypotheses.  

```{r}
summary(lm(Species ~ Area, gala))
```

 * <b> Q:</b> what do you notice about the p-value for the F-statistic and the p-value for the t-statistic for ```area```?
   * <b> A:</b> These correspond to the <b>same</b> test of the null versus alternative hypothesis in this case, i.e., leaving out ```area``` is the same null hypothesis as the data is expressed only as random variation around the mean $\overline{\mathbf{Y}}$.  
   * The p-value for the t-test on ```Area``` is not the same here as it was on the last slide, because this is framed with respect to a different alternative hypothesis.
   

========================================================

<h2>Review</h2>

* We now have two methods for comparing different models: the F-test and the t-test.

* The F-test is defined to compare the likelihood of any two models, as long as one lies in a subspace of another model.

  * Particularly, this is defined as long as the small model $\boldsymbol{\omega}$ uses some combination of the same explanatory variables of the large model $\boldsymbol{\Omega}$.
  
  * This measures the ratio of the maximum likelihoods of both models, which is distributed according to an F distribution under the null hypothesis (the smaller model is accurate). 
  
* The t-test is defined for a single parameter, rather than a combination of parameters like the F-test.

  * This computes the probability of 
  $$\begin{align}
t_i = \frac{\hat{\boldsymbol{\beta}}_i}{se(\hat{\boldsymbol{\beta}}_i)},
\end{align}$$
  under the null hypothesis where $\boldsymbol{\beta}_i = 0$.

* In particular, the two tests are equivalent when we test the two models 
  
  1. $\boldsymbol{\Omega}$ defined over $X_1, \cdots, X_{p-1}$; and 
  2. $\boldsymbol{\omega}$ defined over $X_1,\cdots, X_{p-2}$

========================================================

<h2> Testing a pair of explanatory variables</h2>

* Suppose we wish to determine if the two variables ```area``` and ```adjacent``` have an effect on the response relative to the model with all other variables.

* Particularly, we obtain the null hypothesis $H_0 : \boldsymbol{\beta}_{area}= \boldsymbol{\beta}_{adjacent}=0$.

```{r}
lmods <- lm(Species ~ Elevation + Nearest + Scruz, gala)
anova(lmods, lmod)
```

* The probability of drawing such an F value is around 1/1000, so we reject the null hypothesis.

* This tells us that it is extremely unlikely that there is no combination of the variables ```area``` and ```adjacent``` which have some effect on the response, when comparing the two models.

========================================================

<h3> Testing a pair of explanatory variables</h3>


* Rejecting the null hypothesis in the last example, testing a pair of variables, cannot be inferred from their respective t-tests. 

```{r}
sumary(lmod)
```

* There is no simple way of combining the above information to test a pair of explanatory variables simultaneously.

* <b>Testing a pair of variables needs to be performed with the F-test over the two nested models</b>.

========================================================

<h2>Testing subspaces</h2>

* Suppose we think we can make a simpler model by combining two variables as some kind of linear combination, 
  * e.g., what if we want to create a new variable 
  $$\begin{align}
  \tilde{X}_i \triangleq X_i + X_j
  \end{align}$$
  corresponding to two explanatory variables $X_i,X_j$.
 
 * This still makes sense with an F-test because we are again testing a null hypothesis as a <b>nested subspace of the space of all variables</b>.
 

========================================================

<h3>Testing subspaces -- continued</h3>

* Concretely, suppose we wish to take a new variable corresponding to the sum of the area of the island itself and the area of the adjacent island.
 
* Our null hypothesis will be that $\boldsymbol{\beta}_{area} = \boldsymbol{\beta}_{adjacent}$
 

```{r}
lmod  <- lm(Species ~ Area + Adjacent + Elevation + Nearest + Scruz, gala)
lmods <- lm(Species ~ I(Area+Adjacent) + Elevation + Nearest + Scruz, gala)
anova(lmods,lmod)
```

* In the above, notice the only difference is the ```I()``` function, which enforces that the "+" expression isn't read as a formula.

* <b>Q:</b> do we reject or fail to reject the null hypothesis?

* <b>A:</b>In this case, we can reject the null hypothesis at the $\alpha = 5\%$ significance level.

========================================================

<h3>Testing subspaces -- continued</h3>

* We have learned so far how to test the significance of a given parameter $\boldsymbol{\beta}_i=0$, which would reduce the model space $\boldsymbol{\Omega}$ to the subspace $\boldsymbol{\omega}$ where $X_i$ has no effect on the response.

* We may quite similarly test the significance of a parameter having a <b>specific value (other than zero);</b> 

  * here $\boldsymbol{\omega}$ will represent the subspace of the big model space $\boldsymbol{\Omega}$ where $X_i$ has a specified effect.

* Consider the null hypothesis $H_0: \boldsymbol{\beta}_{Elevation} = 0.5$, versus the alternative hypothesis where we consider the space of models over 

```
Species ~ Area + Adjacent + Elevation + Nearest + Scruz.
```

* We use the function ```offset``` to fix the associated value $\boldsymbol{\beta}_{Elevation} = 0.5$

```{r}
lmods <- lm(Species ~ Area+ offset(0.5*Elevation) + Nearest + Scruz + Adjacent, gala)
```

========================================================

<h3>Testing subspaces -- continued</h3>

* Comparing the nested sub-model with an ANOVA table,

```{r}
anova(lmods, lmod)
```

* <b>Q:</b> do we reject or fail to rejec the null hypothesis?

* <b>A:</b> we reject the null hypothesis at $5\%$ significance.

========================================================

<h3>Testing subspaces -- continued </h3>

* <b>Note:</b> specifying a particular value for the parameter can also be seen in terms of specifying a non-zero mean for the parameter $\hat{\boldsymbol{\beta}}_i$ in a t-test.

*  Suppose we want to test the following null hypothesis, 
 $$H_0: \boldsymbol{\beta}_{Elevation} = C$$
 for some constant $C$ as discussed above.
 
* If $\hat{\boldsymbol{\beta}}_i$ is the solution by least squares, we can compute<br>
$$\begin{align}
t_i = \frac{\left(\hat{\boldsymbol{\beta}}_i - C\right)}{se(\boldsymbol{\beta})}
\end{align}$$

* By the same reasoning as before, this is t-distributed in $n-p$ degrees of freedom with the assumption under the null hypothesis that 

 $$\mathbb{E}\left[\hat{\boldsymbol{\beta}}_i\right] = C$$


========================================================



<h2> Review of main ideas for hypothesis testing</h2>

* We can use the F-test in many cases to study the significance of explanatory variables, combinations of explanatory variables and particular values for the effect on the response...
 * <b>when the null hypothesis is repsented as a "nested sub-model"</b>.
 * Examples are:
 <ol>
    <li> excluding some number of explanatory variables from the large model;</li>
    <li> combining several explanatory variables from the large model into a single explanatory variable;</li>
    <li> specifying a particular value for the effect of some explanatory variable on the response.</li>
 </ol>
 * For special cases:
  <ol>
    <li> excluding a <b>single</b> explanatory variable;</li>
    <li> specifying a particular value for the effect on the response;</li>
  </ol>
  
   * this is equivalent to a t-test with the appropriate number of degrees of freedom (n-p).
   
========================================================

<h2> Confidence intervals </h2>

* We return now to the idea of confidence intervals as an alternative (and dual) idea to hypothesis testing.

* Recall that under our Gaussian assumptions, 
 
 $$\boldsymbol{\epsilon} \sim N\left(\boldsymbol{0}, \sigma^2 \mathbf{I} \right),$$

* we have derived,

  $$\hat{\boldsymbol{\beta}} \sim N\left(\boldsymbol{\beta}, \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1} \sigma \right).$$
  
* In this context, we have an unbiased, sample based estimate for the true mean $\boldsymbol{\beta}$, with a known variance and $n-p$ degrees of freedom.


========================================================

<h3> Confidence intervals -- continued </h3>

* We recall from the t-test that the value

$$\begin{align}
t_i = \frac{\hat{\boldsymbol{\beta}}_i - \boldsymbol{\beta}_i}{se(\hat{\boldsymbol{\beta}}_i)}
\end{align}$$

is t-distributed in $n-p$ degrees of freedom.

* Suppose that $t^\alpha$ is chosen such that $P(X\geq t^\alpha) = \alpha/2$; 

 * then recall that we derived in an earlier exercise that $P(-t^\alpha  &lt; X &lt; t^\alpha)=1- \alpha$.

* Using this dual notion to the hypothesis test, we can create an interval centered at $\hat{\boldsymbol{\beta}}$ (using the t-distribution) with some measure of confidence that the true $\boldsymbol{\beta}$ lives within it.

 * Again, we do not say that the "true" $\boldsymbol{\beta}$ lives within such an interval with probability $100(1-\alpha)$, it lies there or not.  
 
 * Rather, we perform a procedure that over many sets of observations, and with respect to the precedure, we have guaranteed that the true value will lie in our interval with certain confidence $100(1-\alpha)\%$.
 
========================================================

<h3> Confidence intervals -- continued </h3>


* Let us say (similarly to hypothesis testing) we wish to guarantee $\alpha=5\%$ significance.

* Our confidence interval will take the form of

$$\begin{align}
\left( \hat{\boldsymbol{\beta}}_i - t_{n-p}^{\frac{\alpha}{2}} se(\hat{\boldsymbol{\beta}}_i),  \hat{\boldsymbol{\beta}}_i + t_{n-p}^{\frac{\alpha}{2}} se(\hat{\boldsymbol{\beta}}_i)\right)
\end{align}$$

* Where we define $\large{t_{n-p}^{\frac{\alpha}{2}} }$ to be the <b>critical value</b> for which, 
  * if $t \sim t_{n-p}$,
  
  * the probability of 
  $$\begin{align} 
  t \geq t_{n-p}^{\alpha/2}
  \end{align}$$
  is equal to $\frac{\alpha}{2}$.
  
* The symmetry of the student t-distribution guarantees that, with $100(1-\alpha)\%$ confidence, the true value 
  
$$\begin{align}
\boldsymbol{\beta}_i \in \left( \hat{\boldsymbol{\beta}}_i - t_{n-p}^{\frac{\alpha}{2}} se(\hat{\boldsymbol{\beta}}_i),  \hat{\boldsymbol{\beta}}_i + t_{n-p}^{\frac{\alpha}{2}} se(\hat{\boldsymbol{\beta}}_i)\right)
\end{align}$$

========================================================

<h3> Confidence intervals -- continued </h3>


* Suppose that we wish to compute a confidence interval on $\boldsymbol{\beta}_{Area}$, such that with $95\%$ confidence, we say the "true value" of $\boldsymbol{\beta}_{Area}$ lives within it.
  
```{r}
sumary(lmod)
```
* We note that the standard error for ```area``` is given as $\approx 0.022422$, 

* We compute the $2.5\%$ critical value for $T_{30 - 6}$ and obtain the confidence interval as,

```{r}
t_crit <- qt(0.975, 30-6)
-0.02394 + c(-1,1) * t_crit * 0.02242
```

* <b>Exercise (2 minutes):</b> does this indicate we can reject the null hypothesis $H_0: \boldsymbol{\beta}_{Area}=0$ at $5\%$ significance?

========================================================

<h3> Confidence intervals -- continued </h3>

* <b>Solution:</b> notice, the interval contains $0$ so that we cannot reject the null at $5\%$ significance.

```{r}
-0.02394 + c(-1,1) * t_crit * 0.02242
```

* This can be seen dually through the p-value of the parameter for ```area```

```{r}
sumary(lmod)
```

========================================================

<h3> Confidence intervals -- continued </h3>

* Similarly, we can compute the $95\%$ confidence interval for $\boldsymbol{\beta}_{Adjacent}$ with,

 * the $t_{30-6}^{2.5}$ critical value; and
 * the $se(\boldsymbol{\beta}_{Adjacent})$

```{r}
-0.07480 + c(-1,1) * t_crit * 0.01770
```

* <b> Q:</b> Based on the above confidence interval, can we reject the null hypothesis $H_0: \boldsymbol{\beta}_{Adjacent}=0$ with $5\%$ significance?

* <b> A:</b> Yes.  

 * However, while we say (with $95\%$ confidence) that the effect of the adjacent island area is negative on the response, but the uncertainty is large, with the width of the interval on the same order as the coefficient itself,
```{r}
(-0.038269 + 0.111331)/abs(-0.07480)
```

========================================================

<h3> Confidence intervals -- continued </h3>

* Confidence intervals, while dual to hypothesis test, provide slightly more information than the p-values.

* Particularly, we can examine a range of equally plausible values for the parameter of interest.

  * If the confidence interval is relatively tight around the value of the parameter, we can provide more evidence of the actual effect of the predictor.
  
  * However, if the range of values is large, this indicates a great deal of uncertainty of the effect of the predictor and the scale of the effect.
  
  * When the confidence interval contains zero, then it is equally plausible that the effect of changing the predictor will be positive or negative (or nothing) on the response.

========================================================

<h3> Confidence intervals -- continued </h3>

* We can find the confidence intervals of all parameters simultaneously (at the $95\%$ confidence level) as

```{r}
 confint(lmod)
```

* Setting another confidence level, e.g., $99\%$ can be done with a keyword argument,

```{r}
 confint(lmod, level = 0.99)
```

* <b>Exercise (1 minute):</b> discuss with a partner, why is the $99\%$ confidence interval wider than the $95\%$ confidence interval?

* <b>Solution:</b> to be more confident in the location of the true value, we need to widen the interval correspondingly.

========================================================

<h2> Confidence regions</h2>

* The previous example only considered the univariate confidence interval, analogous to the t-test for a single parameter, i.e.,
 * $H_0: \boldsymbol{\beta}_{p-1} = 0$
 * $H_1: \boldsymbol{\beta}_{i} \neq 0$, $i=0,\cdots, p-1$.
 
* Suppose we want (analogous to the F-test) to find a multivariate confidence <em>region</em>.  

* For $\alpha$ the significance level, we find the $100(1-\alpha)\%$ confidence region for $\hat{\boldsymbol{\beta}}$ is given by the relationship

$$\begin{align}
\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^\mathrm{T}\left(\mathbf{X}^\mathrm{T}\mathbf{X} \right) \left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right) \leq p \hat{\sigma}^2 F^{(\alpha)}_{p, n-p}
\end{align}$$

 * The above can be interpreted as a <b>weighted inner product</b> of $\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)$ with itself, i.e., a measure of distance squared, weighted by $\left(\mathbf{X}^\mathrm{T}\mathbf{X} \right)$.
 
 * We say that with $100(1-\alpha)\%$ confidence the true parameter $\boldsymbol{\beta}$ will lie within the domain of the weighted distance function, for which the output is less than 
 $$
  p \hat{\sigma}^2 F^{(\alpha)}_{p, n-p}.
  $$


========================================================

<h3> Confidence regions -- continued</h3>


 <div style="float:left; width:50%">
<img style="width:100%", src="confidence.png" alt="Confidence ellipsoid for adjacent and area variables"/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p>
</div>

 <div style="float:right; width:50%">
<ul>
<li> We can visualize such a region in 2-D, but in higher dimensions it becomes a hyper-ellipsoid.</li>

<li> In the ellipsoid plot, we see the $95\%$ confidence region for $\boldsymbol{\beta}_{area}$ and $\boldsymbol{\beta}_{Adjacent}$ simultaneously. </li>

<ul>
 <li> the vertical lines represent the <em>univariate</em> $95\%$ confidence interval for ```area```;</li>

 <li> the horizontal lines represent the <em>univariate</em> $95\%$ confidence interval for ```adjacent```;</li>
 <li> the center dot represents the estimated value.</li>
</ul>
<li> In this case, we see how the confidence region isn't simply the box of the combined intervals;
<ul>
  <li> this is similar to the fact that the individual t-tests for including or excluding a variable can't be direclty combined.</li>
    <li> Particularly, the weighted distance must always define an ellipsoid, due to the properties of eigenvalues of symmetric matrices.</li>
  </ul>
  </li>
</ul>
 <div style="float:left; width:50%">

========================================================

 <div style="float:left; width:50%">
<img style="width:100%", src="confidence.png"/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p>
</div>

 <div style="float:right; width:50%">
<ul>
  <li> We can observe several hypothesis tests with this confidence region:</li>
    <ul>
      <li> Firstly, $(0,0)$ is excluded from the ellipse. <b>Q:</b> what does this represent at $5\%$ significance?</li> 
        <ul>
          <li> <b>A:</b> We reject the hypothesis $H_0:\boldsymbol{\beta}_{area} = \boldsymbol{\beta}_{adjacent}=0$ for the alternative $H_1: \boldsymbol{\beta}_i \neq 0$ for $i=1,\cdots, 6$.</li>
        </ul>
      <li> Secondly, $0$ is excluded from the horizontal lines. <b>Q:</b> what does this represent at $5\%$ significance?</li> 
        <ul>
          <li> <b>A:</b> We reject the hypothesis $H_0:\boldsymbol{\beta}_{adjacent}=0$ for the alternative $H_1: \boldsymbol{\beta}_i \neq 0$ for $i=1,\cdots, 6$.</li>
        </ul>
      <li> Thirdly, $0$ is not excluded from the vertical lines. <b>Q:</b> what does this represent at $5\%$ significance?</li> 
        <ul>
          <li> <b>A:</b> We fail to reject the hypothesis $H_0:\boldsymbol{\beta}_{area}=0$ for the alternative $H_1: \boldsymbol{\beta}_i \neq 0$ for $i=1,\cdots, 6$.</li>
        </ul>
    </ul> 
</ul>
 <div style="float:left; width:50%">
 
========================================================

<h2> Prediction</h2>

* We have now seen how to construct a model, and assess some of its uncertainty and significance versus the null hypothesis.

* One of the main points of constructing such a model is to produce new predicted values for the response based upon new values for the explanatory data.

* Consider values of the predictors which are not observed in our data set, but fall within the scope of the model;

 * that is, we are interested in values of the predictors that are similar to those that are observed, but for which we do not have an associated observation.

* We can construct an arbitrary vector of values for the predictors, $\mathbf{X}_0$, and define the prediction based on our least-squares model as,
$$\begin{align}
\hat{\mathbf{Y}}_0 = \mathbf{X}_0^\mathrm{T} \hat{\boldsymbol{\beta}}
\end{align}$$
where $\mathbf{X}_0\in \mathbb{R}^p$.

* If $\mathbf{X}_0$ was an observed case, this is precisely the fitted value.

* Among the primary issues is quantifying the uncertainty of the prediction.

  * E.g., if we form a model for the high and low values for the height of a river in a flood plane, it is of practical importance to quantify what would be the confidence range of a prediction;
  
  * here, even if we get the predicted "high level" mark right on average, the damages that can occur only due to variance about the mean can be substantial if we don't take this into account in city planning.
  
========================================================

<h3> Prediction -- continued</h3>


* Concretely, we will differentiate between the <b>predicted mean response</b> and the <b>prediction of a future observation</b>

* Suppose we build a model that predicts the rental price of houses in a given area based on: (i) the number of bedrooms, (ii) the number of bathrooms; and (iii) distance to a major highway.

* Given some new set of values for the explanatory variables $\mathbf{X}_0$,

 * the rental value of the house with characteristics described in $\mathbf{X}_0$ is defined as
 $$\begin{align}
 \mathbf{X}_0^\mathrm{T} \boldsymbol{\beta} + \boldsymbol{\epsilon}
 \end{align}$$
 
 * Because we assume $\mathbb{E}[\boldsymbol{\epsilon}]=0$, our prediction for the price will be
  $$\begin{align}
 \mathbf{X}_0^\mathrm{T} \hat{\boldsymbol{\beta}};
 \end{align}$$
  
  * however, we should state the (un)certainty of our predction in terms of the variance of $\boldsymbol{\epsilon}$.

 * On the other hand, if we ask "what would be the mean price of a house with characteristics $\mathbf{X}_0$ be?
 
  * our prediction is again the same, but our (un)certainty is given in terms of the variance of $\hat{\boldsymbol{\beta}}$.


