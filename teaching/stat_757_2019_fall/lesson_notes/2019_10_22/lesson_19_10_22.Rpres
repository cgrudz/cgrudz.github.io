<style>
.section .reveal .state-background {
   background: #ffffff;
}
.section .reveal h1,
.section .reveal h2,
.section .reveal p {
   color: black;
   margin-top: 50px;
   text-align: center;
}
</style>

Diagnostics part 2
========================================================
date: 10/22/2019
autosize: true
incremental: true
width: 1920
height: 1080

<h2 style='color:black'>Instructions:</h2>
<p style='color:black'>Use the left and right arrow keys to navigate the presentation forward and backward respectively.  You can also use the arrows at the bottom right of the screen to navigate with a mouse.<br></p>

========================================================

### A review of diagnostics

<ul> 
  <li> We will consider 3 categories of potential issues with the model:</li>
  <ol>
    <li> Issues with the error/ variation.  Particularly, we have assumed that $\boldsymbol{\epsilon}\sim N(0, \sigma^2 \mathbf{I})$ such that the errors are normally distributed, uncorrelated and of constant variance.</li>
    <li> Issues with the systematic part of the model.  Particularly, we have assumed that there is an actual signal in the data of the form
    $$\begin{align}
    \mathbb{E}[\mathbf{Y}] = \mathbf{X} \boldsymbol{\beta},
    \end{align}$$
    which may not be valid.</li>
    <li> Issues with unusual observations.  Some of the observations may not fit the model, and they might change the choice and the fit of the model</li>
  </ol>
</ul>


========================================================

<h2> Checking error assumptions</h2>

* If we wish to check the assumptions on the error or variation in the signal $\boldsymbol{\epsilon}$, we need to consider, $\boldsymbol{\epsilon}$ itself is not observable.

* Given our assumptions for $\boldsymbol{\epsilon}$, then we can instead compare the behavior of the <b>residuals</b> for consistency.
  
* Note, even though the errors are assumed to be independent and of equal variance $\sigma^2$, the same doesn't hold for the covariance of the residuals,
  $$cov(\hat{\boldsymbol{\epsilon}})= \sigma^2\left(\mathbf{I} - \mathbf{H}\right)$$

  * In particular, the operator $\mathbf{I} - \mathbf{H}$ is not generally diagonal (such that the residuals have correlation); nor are the diagonal values equal (so that the variances don't all match).
  
* Nonetheless, we use the residuals to underrstand how the true errors are behaving, which are unobservable.

========================================================

<h2>Visual diagnostics</h2>

<div style="float:left; width:40%">
<img style="width:100%", src="no_problem.png" alt="Image of residuals versus fitted values with constant variance in the residuals over cases."/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p>
</div>

<div style="float:left; width:60%">
<ul>
  <li> To the left is a plot where:</li>
  <ol>
    <li> The horizontal (x-axis) represents the fitted value $\hat{\mathbf{Y}}$, as in some model $\hat{\mathbf{Y}}=\mathbf{X}\hat{\boldsymbol{\beta}}$.</li>
    <li> The vertical axis (y-axis) represents the corresponding residual $\hat{\boldsymbol{\epsilon}}$ value, equal to $\mathbf{Y} - \hat{\mathbf{Y}}$.
  </ol>
  <li>Suppose that the variances of the error $\boldsymbol{\epsilon}$ are <b>not fixed</b>, i.e.,</li> 
    <ul>
      <li> suppose that <b>some</b> observations have <b>more</b> variation and <b>some</b> observations have <b>less</b> variation around the signal, $\mathbf{X}\boldsymbol{\beta}$. </li>
    </ul>
  <li>In this case, there is <b>dependence</b> of the variation (or error $\boldsymbol{\epsilon}$) on the <b>observation</b>.</li>
  <li> To the left, this is actuall a <b>well behaved</b> situation.  In particular, the residuals show variation that doesn't seem to depend on the value of the fitted value/ observation.</li>
  <li> The mean of the residuals appears to be zero as well, because there isn't a clear preference for positive or negative residuals</li>
  <li> The situation to the left is denoted <b> homoscedasticity.</b></li>
  <li> We have now seen one test for constant variance using the F-test in the savings data.</li> 
</ul>
</div>

========================================================

### Visual diagnostics

<div style="float:left; width:40%">
<img style="width:100%", src="hetero.png" alt="Image of residuals versus fitted values with non-constant variance in the residuals over cases."/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p></div>

<div style="float:left; width:40%">
<ul>
  <li> The non-constant variance of $\hat{\boldsymbol{\epsilon}}$ to the left is known as <b>heteroscedasticity</b>.</li>
  <li> In this case, there is a clear dependence on the <b>variation</b> of $\hat{\boldsymbol{\epsilon}}$ on the fitted value/ the observation.</li>
  <li> Breaking the constant variance assumption, the Gauss-Markov theorem no longer applies</li>
  <li> Heteroscedasticity does not, in itself, cause ordinary least squares coefficient estimates to be biased;</li>
  <li> however, the theoretical estimates of the variance of the residuals (and therefore the standard errors) will become biased.</li>
</ul>
</div>

========================================================
### Visual diagnostics

<div style="float:left; width:40%">
<img style="width:100%", src="hetero.png" alt="Image of residuals versus fitted values with non-constant variance in the residuals over cases."/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p>
</div>

<div style="float:left; width:60%">
<ul>
  <li> The bias in the standard errors complicates our ability to accurate quantify the uncertainty, and therefore to accurately:</li>
  <ol>
    <li> make hypothesis tests on the parameters for significance;</li>
    <li> provide confidence intervals for the parameters</li>
    <li> provide accurate prediction intervals and confidence intervals for the  mean response;</li>
    <li> provide explanatory power in the relationship between the response and the explanatory variables.</li>
  </ol>
</ul>
</div>

========================================================
### Visual diagnostics

<div style="float:left; width:40%">
<img style="width:100%", src="nonlinear.png"/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p>
</div>

<div style="float:left; width:60%">
<ul>
<li> Using the same plot as before, we can likewise determine if there is actually nonlinearity in the residuals.</li>
<li> The plot at the left exhibits a nonlinear dependence of the residual on the fitted/ observed values</li>
<li> The same technique can also be used replacing the fitted values $\hat{\mathbf{Y}}$ in the horizontal axis with any other variable in the model $\mathbf{X}_i$, to determine dependence of the residual on the explanatory variables</li>
<li> Additionally, we may consider how the residual varies across variables $\mathbf{X}_i$ that we have data for, but have not included as explanatory variables on the response.</li>
<li>If there is a dependence structure for the residuals on the variable that was left out of the model, it suggests that we should consider its impact on the response and/or if it is a variable that is tightly correlated with our existing model variables.</li>
</ul>
  
========================================================

<h2> Changes of scale for variables</h2>

* Often, when non-constant variances are detected, we may consider a change of scale as a remedial measure.

* We will return to a more general formulation later, but for now, we will motivate why we often consider log or square root scale.

* If we detect non-constant variance in the residuals with respect to the fitted values, we may consider a tranformation of the response variable $Y$ by a (nonlinear) function $h()$ for which
$$
var\left( h(Y)\right) = \sigma_0^2
$$
for some value of $\sigma_0$.

* With this motivation, we suppose that such a transformation does exist and has a Taylor series defined around the mean of the response:
  
  $$\begin{align}
  h(Y) &= h\left(\mathbb{E}(Y)\right) + \left(Y- \mathbb{E}(Y)\right) h'(\mathbb{E}(Y)) + \mathcal{O}\left(\delta_Y^2\right)
\end{align}$$

* We will suppose that all terms that are of order 2 or above in the expansion of the anomalies, 
  
  $$\begin{align}
  \delta_Y & \triangleq Y - \mathbb{E}(Y)
\end{align}$$
can be neglected.

========================================================
### Change of scale for variables -- heuristic

* Supposing as on the last slide, we can neglect the higher order terms, we approximate,

  $$\begin{align}
\mathbb{E}\left[ h(Y)\right]& \approx \mathbb{E}\left[h\left(\mathbb{E}(Y)\right) + \left(Y- \mathbb{E}(Y)\right) h'(\mathbb{E}(Y)) \right]\\
& \approx h\left(\mathbb{E}(Y)\right)
\end{align}$$

as long as the deviations from the mean are not too large, and/or the function $h$ is not extremely nonlinear.

* With the above approximation, we can compute,

  $$\begin{align}
  var(h(Y)) & \approx \mathbb{E}\left[\left(\left(Y- \mathbb{E}(Y)\right) h'(\mathbb{E}(Y))\right) \left(\left(Y- \mathbb{E}(Y)\right) h'(\mathbb{E}(Y))\right) \right]\\
  & \approx \left(h'(\mathbb{E}(Y))\right)^2 var(Y)
\end{align}$$

* Thus, up to our approximations, this suggest the following relationship:

  * suppose that
    $$\begin{align}
    h'\left(\mathbb{E}(Y)\right) \propto \left(var(Y)\right)^{-1/2}  & \Leftrightarrow  & h'\left(\mathbb{E}(Y)\right) =  \left(var(Y)\right)^{-1/2} C
    \end{align}$$
for some constant $C$.
  
  * If the above relationship holds, then we find  by our approximation that
  $$\begin{align}
  var(h(Y)) & \approx C
  \end{align}$$
  
========================================================
### Change of scale for variables -- heuristic

* Let's suppose that all the above holds and $h'\left(\mathbb{E}(Y)\right) \propto \left(var(Y)\right)^{-1/2}$.

* In this case, if such a transformation did exist, this suggests a functional form such as,

  $$\begin{align}
  h(y) = \int \frac{dy}{\sqrt{var(y)}} = \int\frac{dy}{sd(y)}
  \end{align}$$

though noting that the above doesn't have a real meaning.

*  However, using the intuition derived this way, we can consider the following derived "rules-of-thumb",

 * suppose that $var(Y) = var(\boldsymbol{\epsilon}) \propto \left(\mathbb{E}(Y)\right)^2$, then
   $$\begin{align}
   h(y) \approx \int \frac{dy}{y}  \approx log(y);
   \end{align}$$

 * suppose that $var(Y) = var(\boldsymbol{\epsilon}) \propto \mathbb{E}(Y)$, then
  $$\begin{align}
   h(y) \approx \int \frac{dy}{y^\frac{1}{2}}  \approx y^{1/2}.
   \end{align}$$

* Note: neither of the above transformations make sense when $Y$ has non-positive values, due to invertibility issues.

========================================================
### Change of scale for variables -- heuristic

<ul>
  <li>As with many of these techniques, there isn't generally an exact way to derive such a transformation of scale, and our derivations ealier were very loose approximations.</li>
  <li>Generally, we must perform the residual versus fitted plots to:</li>
  <ol>
    <li>look for non-constant variance;</li>
    <li>nonlinear behaviors;</li>
    <li>possible suggestions for nonlinear changes of scale, based on the standard deviation as above.</li>
  </ol>
  <li>When trying a change of scale, we should repeat the above steps to determine if the change of scale has had a remedial effect, or if we should try to use a different change of scale again.</li>
</ul>

========================================================
### Change of scale example

* As a concrete example of the change of scale of the response, let's look at the Galapagos data once again.

```{r fig.width=24, fig.height=8}
library(faraway)
lm_gala <- lm(Species ~ Area + Elevation + Scruz + Nearest + Adjacent, gala)
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(fitted(lm_gala),residuals(lm_gala),xlab="Fitted",ylab="Residuals", cex=3, cex.lab=3, cex.axis=1.5)
abline(h=0)
```

* We see a nonlinear relationship suggested by the residual versus fitted plot.

========================================================
### Change of scale example

<div style="float:left; width:50%">
<img style="width:100%", src="poisson.png"/>
Courtesy of Skbkekas <a href="https://creativecommons.org/licenses/by/3.0" target="blank">CC BY 3.0</a> via Wikimedia Commons 
</div>

<div style="float:left; width:50%">
<ul>
  <li> Consider the particular response variable, "number of plant species". </li>
  <li> We note that the Poisson distribution is a distribution that is typically used to model queues and counting, and may be appropriate for this response.</li>
  <li> A special feature of this particular distribution is that the mean is actually equal to the variance. </li>
  <li> Given the previous derivations, this suggests that we might re-scale the response by a square root transformation.</li>
</ul>
</div>


========================================================
### Change of scale example


* We re-fit the model with the square root of the number of species as the response variable instead:
```{r}
lm_sqrt_sp <- lm(sqrt(Species) ~ Area + Elevation + Scruz + Nearest + Adjacent, gala)
```

* and we furthermore plot the residuals versus fitted values
```{r fig.width=24, fig.height=7}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
plot(fitted(lm_sqrt_sp),residuals(lm_sqrt_sp),xlab="Fitted",ylab="Residuals", cex=3, cex.lab=3, cex.axis=1.5)
abline(h=0)
```

* In this case, the nonliear structure is basically eliminated, and the variance is basically constant.

========================================================
### Change of scale example

* We verify the previous visual inspection once again with the quantitative inspection via a linear model:

```{r}
summary(lmod <- lm(lm_sqrt_sp$residuals ~ lm_sqrt_sp$fitted.values))
```

* with a p-value for the F-test at 1, we must almost surely accept the null hypothesis that there is no linear relationship between the residual and the fitted values.

========================================================
## A summary of ideas so far

<ul>
  <li>Though non-constant variance doesn't <b>bias</b> the least squares estimate of the parameters $\hat{\boldsymbol{\beta}}$, it does bias the estimates of standard errors.</li>
  <li>In particular, our confidence intervals and uncertainty quantification can become unreliable.</li>
  <li>We can't observe the true error $\boldsymbol{\epsilon}$ direcly, so we must use the residuals $\hat{\boldsymbol{\epsilon}}$ as a kind of proxy measurement.</li>
  <ul>
    <li>Even when the errors $\boldsymbol{\epsilon}$ are uncorrelated and of constant variance, the same does not hold generally for the residuals.</li>
  </ul>
  <li>Nonetheless, plotting residuals versus fitted values and versus predictors reveals structures in the errors.</li>
  <li>To ameliorate issues of non-constant variance, one common technique is to use a change of scale for the either:</li>
  <ol>
    <li>the response variable; or</li>
    <li>one or more of the predictors.</li>
  </ol>
  <li>We have some "rules-of-thumb" on how to make a change of scale, which we will develop more formally later in the course.</li>
  <li>Whenever we try a change of scale, we should repeat the diagnostics to see if this had the intended effect, or if other changes may be neccessary.</li>
</ul>

========================================================


<h2> Tests for Gaussianity</h2>


<div style="float:left; width:40%">
<img style="width:100%", src="cdf.png" alt="Cumulative probability distributions of Gaussians"/><p style="text-align:center">
Courtesy of Inductiveload via <a href="https://upload.wikimedia.org/wikipedia/commons/c/ca/Normal_Distribution_CDF.svg" target="blank">Wikimedia Commons</a> </p>
</div>
<div style="float:left; width:60%">
<ul>
  <li>The hypothesis testing and confidence interval procedures we have learned are based upon assumptions of Gaussian errors.</li>
  <li>We can assess the Gaussianity of the error with a quantile-quantile <b>(Q-Q)</b> plot.</li>
  <li>To define a Q-Q plot, we will consider the <b>theoretical</b> versus the <b>empirical</b> values for such a plot.</li>
  <li>Suppose we have two probability measures $P_1$ and $P_2$.</li>
  <li>Let each distribution have a corresponding cumulative distribution function <b>(CDF)</b> given by $C_1$ and $C_2$ respectively. </li>
  <li>Recall, the CDF of a probability measure $P$ is defined,
  $$\begin{align}
  C(x) \triangleq& P(X \leq x )\\
   \equiv& \int_{-\infty}^x f(t) dt,
  \end{align}$$
  whenever $P$ has a continuous probability density function $f(t)$
  </li>
  <li> On the left, we illustrate this with the CDF of the normal distribution with several different parameters.</li>
  
</ul>
</div>


========================================================

### Quantile-Quantile plots

* Given the two probability measures $P_1,P_2$, and their respective CDF's, we can define their theoretical Q-Q plot as the graph of the $\mathbb{R}^2$ valued function,
  
  $$\begin{align}
  G:&[0,1]  &\mapsto &\mathbb{R}^2& \\
    &p      &\mapsto &(C_1^{-1}(p), C_2^{-1}(p)) & 
  \end{align}$$

  *  Suppose $p$ is a particular quantile, e.g., $p=50\%$.  Then the map $G$ takes $p$ to the point in the plane, $(x_1, x_2)$, for which
    $$\begin{align}
    P_1(X \leq x_1) &= 50\% \\
    P_2(X \leq x_2) &= 50\%
    \end{align}$$

* <b>Q:</b> in the above, what does the point $(x_1,x_2)$ correspond to?

  * <b>A:</b> The point $(x_1,x_2)$ is just the plot of the medians of the two distributions in the plane.
  
* <b>Q:</b> What kind of shape do we expect for the plot when the two CDFs are equal? 

  * <b>A:</b> If the two CDFs are equal, $F_1 = F_2$, the Q-Q plot of $G$ is just the straight line with slope one through the origin.

========================================================

### Quantile-Quantile plots

* Suppose that $P_1,P_2$ represent the same family of probability measures, such that their CDFs differ only by location and shape, i.e.,     
   * suppose $C_1(x) = C_2\left(\frac{x - \mu}{\sigma}\right)$.  
   * Then $C_1^{-1}(p) \equiv \mu +  \sigma C_2^{-1}(p)$, and there is a linear relationship between the quantiles of the two measures.
   
* Therefore, if two distributions differ only in location and scale,
  
  * e.g., $f_1 = N(0,1)$ and $f_2= N(\mu, \sigma^2)$,
  
* the Q-Q plot is just a straight line with slope $\sigma$ and intercept $\mu$.

* For this reason, when making a Q-Q plot, the CDFs (and/or the data) are typically standardized to be mean zero and variance one.

* By doing so, when measuring two distributions in the same family (as above), the Q-Q plot will just be the central diagonal of the plane.

========================================================

### Quantile-Quantile plots

<ul>
  <li>Typically, we will use the Q-Q plot to make a visual inspection of the empirical quantiles of our data versus the theoretical quantiles of a target distribution.</li>
  <li> Let's suppose that we have samples $\left\{X_i\right\}_{i=1}^n$ of some random variable $X\sim f_2$ where $f_2$ is an unknown distribution.</li>
  <li> Suppose that we <em>believe</em> $f_2 = f_1$ for some known, theoretical distribution $f_1$.  That is,

  $$\begin{align}
  H_0: f_1 = f_2 \\
  H_1: f_1 \neq f_2
  \end{align}$$</li>
  
  <li>We will make a hypothesis test as above where, </li>
  <ol>
    <li>if the Q-Q plot of the empirical versus theoretical CDF is "almost" the central diagonal, </li>
    <li>we will <em>tentatively</em> fail to reject the null hypothesis.</li>
  </ol>
  <li>Recall, that the inverse of the CDF applied to $50\%$, $C^{-1}\left(50\%\right)$, is just the median.  </li>
  <li><b>Q:</b> Then, how do we compute the inverse of the empirical CDF based on the samples $\left\{X_i\right\}_{i=1}^n$?</li>
  <li><b>A:</b> For the samples $\left\{X_i\right\}_{i=1}^n$, the inverse of the empirical CDF is simply the re-ordered values such that $X_i \leq X_{i+1}$ for all $i=1,\cdots, n-1$.</li>
</ul>


========================================================

### Quantile-Quantile plots

* With $n$ samples, we can make $n$ plotting positions for the inverse of the empirical and theoretical CDF, i.e., we can plot the points,

  $$\begin{align}
  \left\{\left(C_1^{-1}\left(\frac{i}{n+1}\right), X_i\right) \in \mathbb{R}^2:  i = 1,\cdots,n \right\}
  \end{align}$$

  * <b>Q:</b> Notice, we do not compute $C_1^{-1}\left(1\right)$, what would be the possible issue with this plot?
  * <b>A:</b>  If, for instance, $f_1 = N(\mu, \sigma^2)$, then $C_1^{-1}(1)= \infty$.
  
  * Generally, some adjustment must be made in the above equation including, e.g., the $n+1$ as the denominator. 
  * Several different approximations can be made, but some adjustment is unavoidable.
  * For a sufficiently large number of independent, identically distributed samples $n$, the approximation won't make a very large difference, but will keep the plot finite. 

========================================================

<h2> An example of Q-Q plots</h2>

* We return to the savings data, with the savings rate defined as the response variable, while the demographic and income values as explanatory variables.

```{r}
lmod <- lm(sr ~ pop15+pop75+dpi+ddpi,savings)
```

* We want to test the residuals of this model for Gaussianity, so we will:

  1. order the values of the residuals in ascendingly such that $\hat{\boldsymbol{\epsilon}}_i \leq \hat{\boldsymbol{\epsilon}}_{i+1}$ for each $i = 1,\cdots,n$; and
  2. plot these values as the y-coordinate versus the corresponding inverse value of the CDF for the standard normal in the x-coordinates.

========================================================

### An example of Q-Q plots

* The function ```qqnorm``` performs the previous steps in a Q-Q plot with respect to a Gaussian (normal) target distribution automatically;

* the function ```qqline``` plots a line joining the first and third quartiles of the empirical distribution.

* We show this in the next slide.

========================================================

### An example of Q-Q plots


```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
qqnorm(residuals(lmod),ylab="Residuals", main="",  cex=3, cex.lab=3, cex.axis=1.5)
qqline(residuals(lmod))
```

* <b>Q:</b> can you conjecture if we would accept the residuals as following a Gaussian distribution based on the above plot?

* Here, the residuals <em>appear</em> to be sufficiently Gaussian.

========================================================

### Q-Q plots versus histograms

* In the following is a histogram of the same residuals as plotted in the Q-Q plot.

* <b>Q:</b> can we determine the Gaussianity of the data from this plot?

```{r fig.width=24, fig.height=8}
par(mai=c(1.5,1.5,.5,.5), mgp=c(3,0,0))
hist(residuals(lmod),xlab="Residuals",main="", cex=3, cex.lab=3, cex.axis=1.5)
```

========================================================

### Q-Q plots versus histograms


* A primary issue of histograms for testing for Gaussianity is that the bin-size is not unique.

* There are various selection algorithms to determine a good estimate of how many bins and of what size to aid us, but histograms can be unreliable compared to a Q-Q plot.

* In this context, as a test for Gaussianity, a histogram should only be used as exploratory analysis and shouldn't be relied upon for our conclusions.

========================================================

### Shapiro-Wilk test


* We can numerically test the Gaussianity of the residuals with the Shapiro-Wilk test, validating our Q-Q plot.  

* Given samples $\left\{X_i\right\}_{i=1}^n$, we compute a (slightly complicated) test statistic for the hypothesis test:

  $$\begin{align}
  H_0: &X_i \sim N\left(\mu_X,\sigma^2_x\right)\\
  H_1: &X_i \hspace{2mm}\text{not Gaussian distributed}
  \end{align}$$
  
* We will not belabor the construction of this test statistic, only perform the test in R:

```{r}
shapiro.test(residuals(lmod))
```

* <b>Q:</b> does this support the notion that the residuals are Gaussian?

  * <b>A:</b> we fail to reject the null, i.e., that they appear to be Gaussian.

* Note: without performing a Q-Q plot before hand, we can't diagnose what possible issues may exist by the p-value alone.

  * Additionally, the Shapiro-Wilk test can best measure extreme departures from Gaussianity;
  
  * generally, a Q-Q plot will give insight into the structure that might not otherwise be detected.

========================================================

### Examples of Q-Q plots

<div style="float:left; width:70%">
<img style="width:100%", src="qq_plots.png" alt="Image of three Q-Q plots with differing behavior"/>
<p style="text-align:center">Courtesy of: Faraway, J. Linear Models with R. 2nd Edition</p>
</div>

<div>
<ul>
  <li>We illustrate three examples of how the Q-Q plot can diagnose non-Gaussianity:</li>
  <ol>
    <li> Left, there is an example of a highly skewed data, drawn from a log-normal distribution.</li>
    <li> Middle, there is an example of heavy-tailed data, drawn from a Cauchy distribution.</li>
    <li> Right, there is an example of short-tailed data, drawn from a uniform distribution.</li>
  </ol>
<li>Note: Q-Q plots are powerful, but are sensitive to the tails of distributions. </li>
<li>It can be hard at times to tell at times if the tails that lie off the diagonal are trully from a heavy-tailed distribution, or if these are simply outliers in otherwise well-behaved data.</li>
</ul>
</div>

========================================================

## Gaussianity assumptions

* We note that Gaussianity of the error wasn't required by the Gauss-Markov theorem, rather this gauranteed that least-squares was the <emph>maximum likelihood estimator</em>.

* Additionally, our confidence intervals and hypothesis tests utilized the Gaussian assumption on the error.

* Without Gaussianity of the errors, least-squares is still the best linear unbiased estimator, but we may find that a linear model in itself is not appropriate or that a biased estimator may perform better.

* However, we can sometimes make due "OK" with slightly innacurate uncertainty quantification, if the <b>sample sizes are sufficiently large</b>...
  
* Particularly, the hypothesis testing and confidence intervals we have developed can be understood as good approximations due to the Central Limit Theorem.


========================================================

### A summary of issues with non-Gaussianity

* Generally, when facing a non-Gaussian error distribution the solution will depend on the types of issues detected.

* For a large number of observations, we can usually ignore issues of non-Gaussianity for uncertainty quantification due to the Central Limit Theorem.

* This is also often the case for short-tailed disributions.

* For skewed distributions, a nonlinear tranformation of the response may alleviate the issuse, 
  
  * e.g., log transformation or square-root.

* For long-tailed distributions, we may need to use robust regression, which we will not cover in this class but is discussed in the course reference books.
