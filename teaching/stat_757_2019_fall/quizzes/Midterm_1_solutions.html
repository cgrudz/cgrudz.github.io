<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Midterm_1_solutions</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style type="text/css">
  
  /*
   * Copyright 2019 Colin Grudzien <cgrudzien@unr.edu>
   *
   * Based on CSS developed orignally by 
   *
   * Christophe-Marie Duquesne <chmd@chmd.fr>
   *
   * CSS for making functional teaching documents with accessible mathematics
   * with Markdown/ pandoc. Inspired by moderncv.
   * 
   * This CSS document is delivered to you under the CC BY-SA 3.0 License.
   * https://creativecommons.org/licenses/by-sa/3.0/deed.en_US
   */
  
  /* Whole document - standard notebook size and margins */
  body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      max-width: 8.5in;
      margin: auto;
      background: #FFFFFF;
      padding: 1in 1in 1in 1in;
  }
  
  /* Title of the document */
  h1 {
      font-size: 50px;
      color: #000000;
      text-align:center;
      margin-bottom:15px;
  }
  
  h1+p {
      /*subtitle*/
      color: #000000;
      font-size: 20px;
      line-height: 2em;
      margin-top: 0.5em;
      margin-bottom: 0em !important;
      text-align: center;
      font-weight: bold;
  }
  
  /* Subheadings */
  h2 {
      color: #000000;
  }
  
  /* Definitions */
  dt {
      float: left;
      clear: left;
      width: 17%;
      /*font-weight: bold;*/
  }
  dd {
      margin-left: 17%;
  }
  p {
      margin-top:0;
      margin-bottom:7px;
  }
  
  /* Blockquotes */
  blockquote {
      text-align: left; 
  }
  
  /* Links */
  a {
      text-decoration: none;
      color: #2980d6;
  }
  a:hover, a:active {
      background-color: #2980d6;
      color: #FFFFFF;
      text-decoration: none;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Horizontal separators */
  hr {
      color: #A6A6A6;
  }
  
  table {
      width: 100%;
  	border-collapse:collapse;
  	border-spacing:0;
  	display:table
  }
  
  table.minimalistBlack {
    border: 3px solid #000000;
    width: 100%;
    text-align: left;
    border-collapse: collapse;
  }
  table.minimalistBlack td, table.minimalistBlack th {
    padding: 5px 4px;
  }
  table.minimalistBlack tbody td {
    font-size: 15px;
  }
  table.minimalistBlack thead {
    background: #CFCFCF;
    background: -moz-linear-gradient(top, #dbdbdb 0%, #d3d3d3 66%, #CFCFCF 100%);
    background: -webkit-linear-gradient(top, #dbdbdb 0%, #d3d3d3 66%, #CFCFCF 100%);
    background: linear-gradient(to bottom, #dbdbdb 0%, #d3d3d3 66%, #CFCFCF 100%);
    border-bottom: 3px solid #000000;
  }
  table.minimalistBlack thead th {
    font-size: 15px;
    font-weight: bold;
    color: #000000;
    text-align: left;
  }
  table.minimalistBlack tfoot {
    font-size: 14px;
    font-weight: bold;
    color: #000000;
    border-top: 3px solid #000000;
  }
  table.minimalistBlack tfoot td {
    font-size: 14px;
  }
  
  /* Make div for blank space for answers in between questions 
   * Include your own necessary amount of blank space in the 
   * height variable.*/
  .answer_div {
      width:100%;
      height:2in;
      float:left;
  }
  
  .long_answer_div{
      width:100%;
      height:3in;
      float:left;
  }
  
  /* Define page breaks in document for printing purposes
   * Include the following line
   * <div class="pagebreak"> </div>
   * in order to initiate page breaks for document printing */
  @media print {
      .pagebreak { 
      page-break-before: always; 
      padding-top:1in;
  } 
  }
  
  div.solutions * {
      color:#000099;
  }
  
  </style>
</head>
<body>
<h1 id="midterm-1">Midterm 1</h1>
<h2 id="instructions">Instructions</h2>
<blockquote>
<p>You are allowed <b>a single <span class="math inline">\(8.5 \times 11\)</span> inch page of handwritten notes</b>.<br />
You are not to communicate with any other student, use any other reference or use any electronic devices in this exam. A violation of these instructions will be considered cheating.</p>
</blockquote>
<h2 id="problems">Problems:</h2>
<h3 id="problem-1">Problem 1:</h3>
<p>State the form of the standard linear regression model in a scalar equation. State the assumptions of the Gauss-Markov theorem. Identify which parts of the equation are known or unkown, and which are deterministic or random in the standard model. In what sense are the estimated parameters by least squares optimal?</p>
<div class="solutions">
<h4 id="solution">Solution:</h4>
The standard model in a scalar equation takes the form, <span class="math display">\[\begin{align}
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i,
\end{align}\]</span> where it is assumed that:
<ol>
<li>
for each <span class="math inline">\(i\)</span>, <span class="math display">\[\mathbb{E}\left[ \epsilon_i \right]= 0 \]</span> and <span class="math display">\[\mathbb{E}\left[\epsilon_i^2\right] = \sigma^2;\]</span>
</li>
<li>
and for each <span class="math inline">\(i \neq j\)</span>, <span class="math display">\[\mathbb{E}\left[\epsilon_i \epsilon_j\right]= 0.\]</span>
</li>
</ol>
<p>In the above model, <span class="math inline">\(X_i,Y_i\)</span> are known values representing observed cases, while <span class="math inline">\(\beta_0,\beta_1,\epsilon_i\)</span> are all unkown. The values <span class="math inline">\(Y_i,\epsilon_i\)</span> are assumed to be random variables, while <span class="math inline">\(X_i, \beta_0,\beta_1\)</span> are fixed, deterministic values. In the above setting, the parameters estimated by least squares are the best-linear-unbiased-estimator, which are optimal in the sense that they have minimum variance among all linear-unbiased-estimators.</p>
</div>
<h3 id="problem-2">Problem 2:</h3>
<p>Use the definition of the <b>unbiased, sample-based variance estimate</b> to recover the <b>regression estimate</b> for <span class="math inline">\(\sigma^2\)</span>. Recall, the denominator must divide by the degrees of freedom remaining after the estimation of the mean. Explain what each term corresponds to in the equation and where degrees of freedom are lost.</p>
<div class="solutions">
<h4 id="solution-1">Solution:</h4>
<p>The unbiased, sample-based estimate of the variance is given by the sum of square deviations of samples from the estimated mean, divided by the number of degrees of freedom. In particular, our estimated mean for each sample <span class="math inline">\(Y_i\)</span> is given by the fitted value <span class="math inline">\(\hat{Y}_i\)</span>. Thus we find, <span class="math display">\[\begin{align}
\hat{\sigma}^2 = \frac{\sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2}{n-p}
\end{align}\]</span> where <span class="math inline">\(n\)</span> is the number of cases and <span class="math inline">\(p\)</span> is the number of parameters used in the regression model to estimate the mean function. In simple regression, <span class="math inline">\(p=2\)</span>.</p>
</div>
<h3 id="problem-3">Problem 3:</h3>
Suppose we are given <span class="math inline">\(n=2\)</span> cases of the data <span class="math inline">\(\left\{\left(X_i,Y_i\right)\right\}_{i=1}^2\)</span>. Construct the design matrix for the simple regression problem. Do you anticipate issues with performing regression analysis in this data set? Explain why or why not.
<div class="solutions">
<h4 id="solution-2">Solution:</h4>
<p>If we are given two cases of data, our design matrix is given as <span class="math display">\[\begin{align}
\mathbf{X} = 
\begin{pmatrix}
1 &amp; X_1 \\
1 &amp; X_2
\end{pmatrix}
\end{align}\]</span> where the first column corresponds to the intercept term. This situation is troubling because we have <span class="math inline">\(n=2\)</span> cases of data and <span class="math inline">\(p=2\)</span> parameters to estimate. This means that we will not have an estimate for the variance <span class="math inline">\(\sigma^2\)</span> and we are running a risk of overfitting the data.</p>
</div>
<h3 id="problem-4">Problem 4:</h3>
Recall the definitions of the point estimators, <span class="math display">\[\begin{align}
\hat{\beta}_1 \triangleq \frac{ \sum_{i=1}^n \left( X_i - \overline{X}\right)\left( Y_i - \overline{Y}\right)}{\sum_{i=1}^n \left(X_i - \overline{X}\right)^2} &amp; &amp; 
\hat{\beta}_0 \triangleq \frac{1}{n} \left( \sum_{i=1}^n Y_i - \hat{\beta}_1 X_i \right)
\end{align}\]</span> Suppose that the linear model satisfies, <span class="math inline">\(Y_i = \beta_0 + \epsilon_i\)</span>, but that the other assumptions of the standard model hold. Derive the values of <span class="math inline">\(\mathbb{E}\left[ \hat{\beta}_1\right]\)</span> and <span class="math inline">\(\mathbb{E}\left[ \hat{\beta}_0 \right]\)</span>. What does this imply about the value of <span class="math inline">\(\beta_0\)</span>?
<div class="solutions">
<h4 id="solution-3">Solution:</h4>
<p>We note, if <span class="math inline">\(Y_i = \beta_0 + \epsilon_i\)</span>, then,</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_1&amp;= \frac{ \sum_{i=1}^n \left( X_i - \overline{X}\right)\left( Y_i - \overline{Y}\right)}{\sum_{i=1}^n \left(X_i - \overline{X}\right)^2} \\
&amp;= \frac{\sum_{i=1}^n \left(X_i - \overline{X}\right) \left( \beta_0 + \epsilon_i  - \beta_0 - \overline{\epsilon}\right)}{\sum_{i=1}^n \left(X_i - \overline{X}\right)^2}\\
&amp;= \frac{\sum_{i=1}^n \left(X_i - \overline{X}\right) \left( \epsilon_i - \overline{\epsilon}\right)}{\sum_{i=1}^n \left(X_i - \overline{X}\right)^2}
\end{align}\]</span> where <span class="math inline">\(\overline{\epsilon}= \frac{1}{n}\sum_{i=1}^n \epsilon_i\)</span>. We treat all <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\overline{X}\)</span> above as deterministic constants, such that the above is a linear combination of mean zero random variables. By distributing the expectation over the linear combination, we find that the above is mean zero.</p>
<p>From the above, <span class="math display">\[\begin{align}
\hat{\beta}_0 &amp;= \frac{1}{n} \left( \sum_{i=1}^n Y_i - \hat{\beta}_1 X_i \right)\\
&amp; = \beta_0 + \overline{\epsilon}
\end{align}\]</span> as <span class="math inline">\(\beta_1=0\)</span>. The random term <span class="math inline">\(\overline{\epsilon}\)</span> once again vanishes in expectation, such that <span class="math inline">\(\mathbb{E}\left[\hat{\beta}_0\right]= \beta_0\)</span>.</p>
<p>We now note that, <span class="math display">\[\begin{align}
\hat{\beta}_0 &amp;= \frac{1}{n} \left( \sum_{i=1}^n Y_i - \hat{\beta}_1 X_i \right) \\
&amp;= \frac{1}{n}\left(\sum_{i=1}^n Y_i \right) \\
&amp;= \overline{Y}
\end{align}\]</span> so that <span class="math inline">\(\hat{\beta}_0\)</span> is always the sample-based mean of the random variable <span class="math inline">\(Y\)</span>, meaning that <span class="math inline">\(\mathbb{E}\left[\overline{Y}\right]=\beta_0\)</span> is the true mean which the <span class="math inline">\(Y_i\)</span> are distributed about.</p>
</div>
<h3 id="problem-5">Problem 5:</h3>
Assume the conditions of problem 4 above hold in this problem. Recall the definitions, <span class="math display">\[\begin{align}
RSS = \sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2 &amp; &amp;TSS = \sum_{i=1}^n \left(Y_i - \overline{Y}\right)^2
\end{align}\]</span> Derive the values of <span class="math inline">\(\mathbb{E}\left[RSS\right]\)</span> and <span class="math inline">\(\mathbb{E}\left[TSS\right]\)</span>. What is the implication for <span class="math inline">\(R^2\)</span>?
<div class="solutions">
<h4 id="solution-4">Solution:</h4>
<p>We remark here, under the condition that <span class="math inline">\(\beta_1=0\)</span>, we see that, <span class="math display">\[\begin{align}
TSS &amp;= \sum_{i=1}^n \left(Y_i - \overline{Y}\right)^2 \\
&amp;= \sum_{i=1}^n \left( \beta_0 + \epsilon_i - \beta_0 -  \overline{\epsilon}\right)^2\\
&amp;= \sum_{i=1}^n \left(\epsilon_i - \overline{\epsilon}\right)^2,
\end{align}\]</span> which is the sum of square deviations of the samples from the sample-based mean. Thus in expectation, we have <span class="math display">\[\mathbb{E}\left[TSS\right] = (n-1)\sigma^2,\]</span> as this is the un-normalized, unbiased estimate for the true <span class="math inline">\(\sigma^2\)</span>. For the <span class="math inline">\(RSS\)</span> on the other hand, we see <span class="math display">\[\begin{align}
RSS &amp;= \sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2\\
&amp;= \sum_{i=1}^n \left(Y_i - \overline{Y}\right)^2,
\end{align}\]</span> because the fitted value for every <span class="math inline">\(i\)</span> will be given by, <span class="math display">\[\begin{align}
\hat{Y}_i = \hat{\beta}_0 = \overline{Y}
\end{align}\]</span> when <span class="math inline">\(\beta_1=0\)</span>. Thus the <span class="math inline">\(RSS= TSS\)</span>, meaning that <span class="math display">\[\begin{align} 
R^2 = 1 - \frac{RSS}{TSS} =  0. 
\end{align}\]</span></p>
</div>
<h3 id="problem-6">Problem 6:</h3>
Suppose that <span class="math inline">\(X\)</span> is the number of past years of work experience and <span class="math inline">\(Y\)</span> is weekly average income in dollars for full-time, regular workers in the United States in 2019 in a simple regression model. Suppose that in addition, <span class="math inline">\(X=0\)</span> is within the scope of the model. Is <span class="math inline">\(\beta_0=0\)</span> a reasonable assumption for the form of the signal? Justify why or why not with the implication for the regression model and, in particular, for the mean response.
<div class="solutions">
<h4 id="solution-5">Solution:</h4>
<p>If <span class="math inline">\(\beta_0=0\)</span> this means that our estimated mean function will pass through the point <span class="math inline">\((0,0)\)</span>. In particular, this would imply that we expect a regular, full-time worker in 2019 with zero years of past work experience earns zero USD per week. In this case, assuming that <span class="math inline">\(\beta_0\)</span> is not well justified because we do not expect that a regular, full-time worker will be working without payment.</p>
</div>
<h3 id="problem-7">Problem 7:</h3>
Use the standard model in matrix form and the definition of the least squares estimated parameters, <span class="math display">\[\begin{align}
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon} &amp; &amp;
\hat{\boldsymbol{\beta}} \triangleq \left(\mathbf{X}^\mathrm{T} \mathbf{X} \right)^{-1} \mathbf{X}^\mathrm{T}\mathbf{Y},
\end{align}\]</span> to prove that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is an unbiased estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span>. Furthermore, use the definition of the covariance of a random vector, <span class="math display">\[\begin{align}
cov(\mathbf{Y})= \mathbb{E}\left\{\left(\mathbf{Y} - \mathbb{E}\left[\mathbf{Y}\right]\right)\left(\mathbf{Y} - \mathbb{E}\left[\mathbf{Y}\right]\right)^\mathrm{T} \right\},
\end{align}\]</span> to derive the covariance of this estimate.
<div class="solutions">
<h4 id="solution-6">Solution:</h4>
<p>We note that by definition, <span class="math display">\[\begin{align}
\mathbb{E}\left[ \hat{\boldsymbol{\beta}}\right] &amp;= \mathbb{E}\left[\left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T} \mathbf{X} \boldsymbol{\beta} +\left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}\boldsymbol{\epsilon}\right] \\
&amp; = \mathbb{E}\left[ \mathbf{I} \boldsymbol{\beta} \right] + \mathbb{E}\left[ \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}\boldsymbol{\epsilon}\right] \\
&amp;= \boldsymbol{\beta}
\end{align}\]</span> as <span class="math inline">\(\boldsymbol{\epsilon}\)</span> is mean zero.</p>
<p>Using the above fact, we can write, <span class="math display">\[\begin{align}
cov\left(\hat{\boldsymbol{\beta}}\right) &amp;= \mathbb{E}\left[ \left( \hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)\left( \hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^\mathrm{T}\right] \\
&amp;= \mathbb{E}\left[\left\{ \boldsymbol{\beta} - \boldsymbol{\beta} + \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}\boldsymbol{\epsilon} \right\}
\left\{ \boldsymbol{\beta} - \boldsymbol{\beta} + \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}\boldsymbol{\epsilon} \right\}^\mathrm{T}\right] \\
&amp;= \mathbb{E}\left[\left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}\boldsymbol{\epsilon}\boldsymbol{\epsilon}^\mathrm{T} \mathbf{X} \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\right]\\
&amp;= \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}\left(\sigma^2\mathbf{I}\right)\mathbf{X} \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1} \\
&amp;= \sigma^2 \left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}
\end{align}\]</span> by cancelling terms in the final line, and passing the scalar in front of the equation.</p>
</div>
<h3 id="problem-8">Problem 8:</h3>
<p>Recall the definition, <span class="math display">\[\begin{align}
ESS \triangleq \sum_{i=1}^n \left( \hat{Y}_i - \overline{Y}\right)^2.
\end{align}\]</span> Compute <span class="math inline">\(\mathbb{E}\left[ESS\right]\)</span>. <b>Hint:</b> you can assume the following facts, <span class="math display">\[\begin{align}\sum_{i=1}^n  \hat{\epsilon}_i = 0 &amp; &amp;
var\left(\hat{\beta}_1\right) = \frac{\sigma^2}{\sum_{i=1}^n \left(X_i - \overline{X}\right)^2}.
\end{align}\]</span></p>
<div class="solutions">
<h4 id="solution-7">Solution:</h4>
<p>We recall, because the sum of residuals is equal to zero, we know <span class="math display">\[\begin{align}
\overline{Y} = \frac{1}{n}\sum_{i=1}^n Y_i = \frac{1}{n}\sum_{i=1}^n\left( \hat{Y}_i +\hat{\epsilon}_i\right) = \frac{1}{n} \sum_{i=1}^n \hat{Y}_i .
\end{align}\]</span> Therefore, we will write <span class="math display">\[\begin{align}
ESS &amp;= \sum_{i=1}^n \left(\hat{Y}_i - \frac{1}{n}\left\{\sum_{j=1}^n \hat{Y}_j  \right\}\right)^2 \\
&amp;= \sum_{i=1}^n \left(\hat{\beta}_0 + \hat{\beta}_1 X_i - \hat{\beta}_0 - \hat{\beta}_1 \overline{X}\right)^2 \\
&amp; = \hat{\beta}_1^2 \sum_{i=1}^n \left(X_i - \overline{X}\right)^2 
\end{align}\]</span></p>
<p>Using the definition of the variance we have that, <span class="math display">\[\begin{align}
\mathbb{E} \left[ \hat{\beta}_1^2 \right] &amp;= \mathbb{E}\left[ \left(\hat{\beta}_1 - \beta_1 + \beta_1\right)^2 \right] \\
&amp; = \mathbb{E} \left[ \left(\hat{\beta}_1 - \beta_1\right)^2 + 2 \hat{\beta}_1 \beta_1 - \beta_1^2 \right] \\
&amp;= var\left(\hat{\beta}_1\right) + 2\beta_1^2  - \beta_1^2 \\
&amp;= var\left(\hat{\beta}_1 \right) + \beta_1^2.
\end{align}\]</span></p>
<p>Therefore, <span class="math display">\[\begin{align}
\mathbb{E}\left[ESS\right] &amp;= \left\{ var\left(\hat{\beta}_1\right) + \beta_1^2 \right\}\sum_{i=1}^n \left(X_i - \overline{X}\right)^2 \\
&amp;= \sigma^2 + \beta_1^2 \sum_{i=1}^n \left(X_i - \overline{X}\right)^2
\end{align}\]</span></p>
</div>
</body>
</html>
